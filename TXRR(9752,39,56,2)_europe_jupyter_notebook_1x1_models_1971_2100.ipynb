{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B82ZCJsL6HDE"
      },
      "source": [
        "# Anomaly Detection using Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP0CMbSa6HDJ"
      },
      "source": [
        "Date: 07.06.2025<br>\n",
        "Author: Hiroyuki Murakami (<Hiroyuki.Murakami@noaa.gov>), Sophia Zhou (sz3962@princeton.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rODL4R1z6HDK"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9bsWyo36HDL"
      },
      "source": [
        "This project is for introducing the anomaly detection method using deep learning, so called \"autoencoder\", for daily precipitation and daily maximum temperature. <br>\n",
        "\n",
        "Before reading this jupyter notebook, following web sites should be read to understand the basic consept of anomaly detection and autoencoder.<br>\n",
        " - [Intro to anomaly detection with OpenCV, Computer Vision, and scikit-learn](https://pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/)\n",
        " - [Anomaly detection with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/)\n",
        " - [Autoencoders with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/)\n",
        "\n",
        "Please also read following paper for the details of methodology intoduced in this jupyter notebook.<br>\n",
        " - Murakami, H., T. L. Delworth, W. F. Cooke, S. B. Kapnick, and P. -C. Hsu, 2022: Increasing frequency of anomalous precipitation events in Japan detected by a deep learning autoencoder. Earth’s Future, 10, e2021EF002481. [Link](http://dx.doi.org/10.1029/2021EF002481)\n",
        "\n",
        "The methodology and codes were based on the codes in [Autoencoders with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKkeq_Uu6HDM"
      },
      "source": [
        "## Required Installed Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_9axWai6HDN"
      },
      "source": [
        "Following Python packages are required for this Jupyter-Notebook. <br>\n",
        "  > tensorflow, scikit-learn, pillow, h5py, keras, netCDF4, pandas, matplotlib, cartopy, numpy, pickle, jupyter, jupyter_contrib_nbextensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:29.908570Z",
          "start_time": "2024-07-16T13:04:29.906675Z"
        },
        "id": "05sLSO8Y6vAx"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:29.914094Z",
          "start_time": "2024-07-16T13:04:29.909388Z"
        },
        "id": "PdEK8wcG6MCZ"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#sys.path.append('/content/drive/MyDrive/Autoencoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:29.918168Z",
          "start_time": "2024-07-16T13:04:29.914890Z"
        },
        "id": "NELVxlEmSg_Z"
      },
      "outputs": [],
      "source": [
        "#!yes | pip uninstall shapely\n",
        "#!pip install shapely --no-binary shapely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:29.929454Z",
          "start_time": "2024-07-16T13:04:29.919323Z"
        },
        "id": "YOE2LAr46sKi"
      },
      "outputs": [],
      "source": [
        "#!pip install cartopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:29.933458Z",
          "start_time": "2024-07-16T13:04:29.930217Z"
        },
        "id": "3md8lSe2PAVl"
      },
      "outputs": [],
      "source": [
        "#!pip install netcdf4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.505478Z",
          "start_time": "2024-07-16T13:04:29.934240Z"
        },
        "id": "4gOS1acW6HDO"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LatitudeFormatter,LongitudeFormatter\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "from netCDF4 import num2date, Dataset\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from pyimagesearch.convautoencoder import ConvAutoencoder\n",
        "\n",
        "from mylib.save_load_file import pickle_dump, pickle_load\n",
        "from mylib.mystat import MYSTAT\n",
        "\n",
        "#from cdo import Cdo\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.508690Z",
          "start_time": "2024-07-16T13:04:34.506893Z"
        },
        "id": "Shc8F-ee6HDQ"
      },
      "outputs": [],
      "source": [
        "mystat = MYSTAT() # my own statistical package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBZXs1n6HDR"
      },
      "source": [
        "If you don't see any errr messages, all of the Python packages have been normaly installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EtvB4M26HDR"
      },
      "source": [
        "# Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.525391Z",
          "start_time": "2024-07-16T13:04:34.510007Z"
        },
        "id": "rA53yZVN6HDS"
      },
      "outputs": [],
      "source": [
        "#--domain\n",
        "domain=\"EU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGN65aFb6HDS"
      },
      "source": [
        "Domain of interest. Chose USA or JP (Japan) or EU (Europe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.556582Z",
          "start_time": "2024-07-16T13:04:34.528317Z"
        },
        "id": "aiwhueSY6HDT"
      },
      "outputs": [],
      "source": [
        "#--Season\n",
        "if domain==\"JP\":\n",
        "    model=\"APHRODITE\" # http://aphrodite.st.hirosaki-u.ac.jp/\n",
        "    tsyear=1977\n",
        "    teyear=2015\n",
        "    season=\"MJJASO\" # May-Octber\n",
        "elif domain==\"USA\":\n",
        "    model=\"PRISMG\" # https://prism.oregonstate.edu/\n",
        "    tsyear=1981\n",
        "    teyear=2021\n",
        "    season=\"JAS\" # July-September\n",
        "elif domain==\"EU\":\n",
        "  model = \"E-OBS\"  # https://cds.climate.copernicus.eu/datasets/insitu-gridded-observations-europe?tab=overview\n",
        "  tsyear = 1971\n",
        "  teyear = 2023\n",
        "  season = \"MJJASO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.575754Z",
          "start_time": "2024-07-16T13:04:34.560027Z"
        },
        "id": "YEW3BhErZsfr"
      },
      "outputs": [],
      "source": [
        "#--Test Model\n",
        "testmodel=\"SPEAR-MED-ALLSSP585\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.580190Z",
          "start_time": "2024-07-16T13:04:34.577776Z"
        },
        "id": "t5yHMDxkZsfr"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\":\n",
        "    emax=30 # ensembe member for tested data\n",
        "    emax2=emax # emax for test\n",
        "    testmodel2=testmodel.split(\"-\")[-1]\n",
        "    tsyear2=1971\n",
        "    teyear2=2100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKSUeZHH6HDT"
      },
      "source": [
        "If USA is chosen, the [PRISM](https://prism.oregonstate.edu/) precipitation data is used and the analysis period is 1981-2021 for July-September.<br>\n",
        "If JP is chosen, the [APHRODITE](http://aphrodite.st.hirosaki-u.ac.jp/) precipitation data is used and the analysis period is 1977-2015 for May-October.<br>\n",
        "If EU is chosen, the [E-OBS](https://cds.climate.copernicus.eu/datasets/insitu-gridded-observations-europe?tab=overview) precipitation data is used and the analysis period is 1977-2023 for May-October.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.584954Z",
          "start_time": "2024-07-16T13:04:34.581165Z"
        },
        "id": "6K8ZyTqi6HDT"
      },
      "outputs": [],
      "source": [
        "#--Grid type\n",
        "if domain==\"JP\":\n",
        "    grid=\"HGRID\" # 25-km grid\n",
        "elif domain==\"USA\":\n",
        "    grid=\"SGRID\" # 50-km grid\n",
        "elif domain==\"EU\":\n",
        "    grid=\"SGRID\" # 50-km grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPo2dINH6HDU"
      },
      "source": [
        "Precipitation GPV is on the SGRID (50-km grid) for USA<br>\n",
        "Precipitation GPV is on the HGRID (25-km grid) for JP<br>\n",
        "Precipitation GPV is on the SGRID (50-km grid) for EU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.598183Z",
          "start_time": "2024-07-16T13:04:34.585970Z"
        },
        "id": "kkmoFecg6HDU"
      },
      "outputs": [],
      "source": [
        "#--Element\n",
        "#elem=\"PRECIP\"\n",
        "elem = \"T_REF_MAX\"\n",
        "#--Climatology\n",
        "runclim=\"RCLIM\" #running mean climate\n",
        "\n",
        "#--days of running mean\n",
        "runday = 5\n",
        "\n",
        "#--Units of input data\n",
        "#vunits = 'mm/day'\n",
        "vunits= 'K/day'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXbYDBev6HDU"
      },
      "source": [
        "- The precipitation data used here are the 5-day running mean precipitation anomalies.\n",
        "- The anomalies of daily mean precipitation were obtained by subtracting the moving 20-yr climatological daily mean (derived from the preceding 20-yr data) from the raw data. Then, 5-day running-mean anomalies were computed by averaging the anomalies using the previous 5 days to remove the short-term weather events (e.g., squall lines). Therefore, the target is large-scale anomalous precipitation that lasts for a few days rather than short-term daily extreme precipitation events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.608790Z",
          "start_time": "2024-07-16T13:04:34.599173Z"
        },
        "id": "CPB53y9f6HDU"
      },
      "outputs": [],
      "source": [
        "#--parameters for season\n",
        "if season==\"DJF\":\n",
        "    selmonths=\"1,2,12\"\n",
        "elif season==\"JAS\":\n",
        "    selmonths=\"7,8,9\"\n",
        "elif season==\"MJJASO\":\n",
        "    selmonths=\"5,6,7,8,9,10\"\n",
        "elif season==\"AMJJAS\":\n",
        "    selmonths=\"4,5,6,7,8,9\"\n",
        "elif season==\"JJAS\":\n",
        "    selmonths=\"6,7,8,9\"\n",
        "print (\"selmonths=\",selmonths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOTypfTZ6HDV"
      },
      "source": [
        "\"selmonths\" is used for reading input data through the python-cdo later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.638340Z",
          "start_time": "2024-07-16T13:04:34.609910Z"
        },
        "id": "Rho7h8Ic6HDV"
      },
      "outputs": [],
      "source": [
        "#--parameters for plotting\n",
        "if domain==\"JP\":\n",
        "    ddomain = [123,146,24,46]\n",
        "    figsize = (20,12)\n",
        "    legend_posi = [0.2, 0.1, 0.6, 0.03]\n",
        "    contours=np.arange(-15.0,15.0,0.5) # set contours to plot later\n",
        "    cmap=cm.bwr_r # set colormap\n",
        "\n",
        "elif domain==\"USA\":\n",
        "    #ddomain = [232,295,22,51] # draw domain\n",
        "    ddomain = [232,295,23,46] # draw domain\n",
        "\n",
        "    figsize = (20,10)\n",
        "    legend_posi = [0.2, 0.2, 0.6, 0.03]\n",
        "    contours=np.arange(-10.0,10.0,0.5) # set contours to plot later\n",
        "    cmap=cm.bwr_r # set colormap\n",
        "elif domain==\"EU\":\n",
        "    #ddomain = [232,295,22,51] # draw domain\n",
        "    ddomain = [-24.5,45,26,71] # what is domain? ******* NEEDS CHANGE -> This is the domain of plot: [west, east, south, north] given in longigudes and latitudes\n",
        "\n",
        "    figsize = (20,15)\n",
        "    legend_posi = [0.2, 0.2, 0.6, 0.01]\n",
        "    contours=np.arange(-10.0,10.0,0.5) # set contours to plot later\n",
        "    #cmap=\"seismic\" # set colormap\n",
        "    cmap=cm.bwr_r\n",
        "\n",
        "    dlon0,dlat0=5,5\n",
        "    xticks0=np.arange(-20,45,dlon0)\n",
        "    yticks0=np.arange(30,70,dlat0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poE0k5oLZsfr"
      },
      "source": [
        "## Observed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.642719Z",
          "start_time": "2024-07-16T13:04:34.639585Z"
        },
        "id": "9EQmDDoNwFw9"
      },
      "outputs": [],
      "source": [
        "if domain==\"EU\":\n",
        "    #datadir = \"/content/drive/MyDrive/Precip_Europe\"\n",
        "    #ifile = \"%s/merge2_SPEAR_MED_EUROPE_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    datadir = \"/archive/hnm/for_someone/Sophia/TX\"\n",
        "    ifile = \"%s/merge2_SPEAR_1x1_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    regfile = \"/archive/hnm/Comm/grid_1x1_EUROPE3_europeregion.nc\"\n",
        "\n",
        "#cdo.infov(input=ifile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx75mFcF6HDW"
      },
      "source": [
        "These parameters are used for drawing maps using Basemap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.656668Z",
          "start_time": "2024-07-16T13:04:34.643646Z"
        },
        "id": "lWCL5__TSOJz"
      },
      "outputs": [],
      "source": [
        "#--Output Data Directory\n",
        "datatype=\"TXRR_model_combinedautoencoder_1971-2100\"\n",
        "\n",
        "print (\"Input Directory=\",datadir)\n",
        "print (\"Output Directory=\",datatype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.660367Z",
          "start_time": "2024-07-16T13:04:34.657691Z"
        },
        "id": "Xzl2FJD3xBk0"
      },
      "outputs": [],
      "source": [
        "domain = \"EU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wlGyr-t6HDW"
      },
      "source": [
        "Define input and output directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.679668Z",
          "start_time": "2024-07-16T13:04:34.661404Z"
        },
        "id": "PgkKW_0D6HDW"
      },
      "outputs": [],
      "source": [
        "odirs2 = [\"output_ver\",\"anomaly\",\"output_model\",\"anomaly_model\",\"output_test\",\"anomaly_test\",\"restored\",\"restored_model\",\"test\",\"restored_test\",\"output_nat\"]\n",
        "\n",
        "for odir2 in odirs2:\n",
        "    if not os.path.exists(\"%s/%s\" % (datatype,odir2)):\n",
        "        os.makedirs(\"%s/%s\" % (datatype,odir2))\n",
        "    print  (datatype+\"/\"+odir2)\n",
        "    print(datatype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0GwfldT6HDX"
      },
      "source": [
        "Creating output directory. Data structure is as follows.\n",
        "\n",
        "<pre>\n",
        "   |-- Input_data\n",
        "   |     |--AphroJP_V1207_DPREC.1900-2010_V1207R3_DPREC.2011-2015_HiFLOR_5-day_anom_cprev20yrs.nc (APRODITE Data)\n",
        "   |     |\n",
        "   |     |--PRISM_ppt_stable_4kmM3_1981-2021_GSPEAR_5-day_anom_cprev20yrs.nc (PRISM Data)\n",
        "   |\n",
        "   |-- Output_JP (output for anmaly detection for the Japan domain)\n",
        "   |     |--anomaly (output directory for picutres of anomalous events)\n",
        "   |     |    |--cluster6 (output for K-mean Cluster)\n",
        "   |     |\n",
        "   |     |--output (output directory for MSE and autoencoder model)\n",
        "   |\n",
        "   |-- Output_USA (output for anmaly detection for the USA domain)\n",
        "   |     |--anomaly (output directory for picutres of anomalous events)\n",
        "   |     |    |--cluster6 (output for K-mean Cluster)\n",
        "   |     |\n",
        "   |     |--output (output directory for MSE and autoencoder model)\n",
        "   |\n",
        "   |-- mylib (includes statistical codes)\n",
        "   |\n",
        "   |-- pyimagesearch (includes tensorflow codes)\n",
        "   |\n",
        "   |-- pics (some reference pictures)\n",
        "  \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.684802Z",
          "start_time": "2024-07-16T13:04:34.682374Z"
        },
        "id": "cV-xWyxx6HDX"
      },
      "outputs": [],
      "source": [
        "#parameters for normalizing\n",
        "if elem==\"PRECIP\":\n",
        "    if domain==\"JP\":\n",
        "        maxvalue = 150\n",
        "        minvalue = -40\n",
        "    elif domain==\"USA\":\n",
        "        maxvalue = 100\n",
        "        minvalue = -40\n",
        "    elif domain==\"EU\":\n",
        "        maxvalue = 200\n",
        "        minvalue = -50\n",
        "else:\n",
        "    maxvalue = 20\n",
        "    minvalue = -20\n",
        "    maxvalueRR = 55\n",
        "    minvalueRR = -15\n",
        "\n",
        "# NEEDS CHANGE\n",
        "print (\"normalized = (rawdata - %i)/(%i - %i)\" % (minvalue,maxvalue,minvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr0ZEecY6HDX"
      },
      "source": [
        "This determins the parameter for normalization.<br>\n",
        "Precipitation data (mm/day) is normalized resulting in normalized data that approximately range 0-1.<br>\n",
        "> normalized = (rawdata - minvalue)/(maxvalue - minvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ont2RQWP6HDY"
      },
      "source": [
        "# Subroutines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUNaHVmu6HDY"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.694677Z",
          "start_time": "2024-07-16T13:04:34.685762Z"
        },
        "id": "IE060Wfl6HDY"
      },
      "outputs": [],
      "source": [
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        top=4\n",
        "        bot=4\n",
        "        left=2\n",
        "        right=2\n",
        "\n",
        "elif domain==\"USA\":\n",
        "    if grid==\"SGRID\":\n",
        "        top=2\n",
        "        bot=1\n",
        "        left=1\n",
        "        right=1\n",
        "elif domain==\"EU\": # NEEDS CHANGING\n",
        "    if grid==\"SGRID\":\n",
        "        top=0\n",
        "        bot=1\n",
        "        left=4\n",
        "        right=4\n",
        "\n",
        "pad_width1=((0,0),(bot,top),(left,right))\n",
        "pad_width2=((0,0),(1,0),(4,4),(0,0))\n",
        "\n",
        "print (\"pad=\",pad_width1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzd8SEtTZsfv"
      },
      "outputs": [],
      "source": [
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCn9UtcC6HDY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.704402Z",
          "start_time": "2024-07-16T13:04:34.695776Z"
        },
        "id": "64jJwoyVZsfv"
      },
      "outputs": [],
      "source": [
        "def pad_remover(rdata,domain,grid, og = True, multidata=False):\n",
        "    ndim = rdata.ndim\n",
        "    if domain==\"JP\":\n",
        "        if grid==\"HGRID\":\n",
        "            if ndim==4:\n",
        "                if multidata:\n",
        "                    recon = rdata[:,4:-4,2:-2,0]\n",
        "                else:\n",
        "                    recon = rdata[0,4:-4,2:-2,0]\n",
        "            elif ndim==3:\n",
        "                recon = rdata[4:-4,2:-2,0]\n",
        "    elif domain==\"USA\":\n",
        "        if grid==\"SGRID\":\n",
        "            if ndim==4:\n",
        "                if multidata:\n",
        "                    recon = rdata[:,1:-2,1:-1,0]\n",
        "                else:\n",
        "                    recon = rdata[0,1:-2,1:-1,0]\n",
        "            elif ndim==3:\n",
        "                recon = rdata[1:-2,1:-1,0]\n",
        "    elif domain==\"EU\":\n",
        "        if grid==\"SGRID\":\n",
        "            if ndim==4:\n",
        "                if multidata:\n",
        "                    print(\"multidata?\")\n",
        "                    if og:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[:,bot::,left:-right,0]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[:,bot:-top,left::,0]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[:,bot::,left::,0]\n",
        "                        else:\n",
        "                            recon = rdata[:,bot:-top,left:-right,0]\n",
        "                    else:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[0,bot::,left:-right,:]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[0,bot:-top,left::,:]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[0,bot::,left::,:]\n",
        "                        else:\n",
        "                            recon = rdata[0,bot:-top,left:-right,:]\n",
        "                else:\n",
        "                    if top==0 and right!=0:\n",
        "                        recon = rdata[0,bot::,left:-right,0]\n",
        "                    elif top!=0 and right==0:\n",
        "                        recon = rdata[0,bot:-top,left::,0]\n",
        "                    elif top==0 and right==0:\n",
        "                        recon = rdata[0,bot::,left::,0]\n",
        "                    else:\n",
        "                        recon = rdata[0,bot:-top,left:-right,0]\n",
        "\n",
        "            elif ndim==3:\n",
        "                if multidata:\n",
        "                    print(\"multidata?\")\n",
        "                    if og:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[:,bot::,left:-right]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[:,bot:-top,left::]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[:,bot::,left::]\n",
        "                        else:\n",
        "                            recon = rdata[:,bot:-top,left:-right]\n",
        "                    else:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[bot::,left:-right,:]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[bot:-top,left::,:]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[bot::,left::,:]\n",
        "                        else:\n",
        "                            recon = rdata[bot:-top,left:-right,:]\n",
        "                else:\n",
        "                    if og:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[bot::,left:-right,0]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[bot:-top,left::,0]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[bot::,left::,0]\n",
        "                        else:\n",
        "                            recon = rdata[bot:-top,left:-right,0]\n",
        "                    else:\n",
        "                        if top==0 and right!=0:\n",
        "                            recon = rdata[0,bot::,left:-right]\n",
        "                        elif top!=0 and right==0:\n",
        "                            recon = rdata[0,bot:-top,left::]\n",
        "                        elif top==0 and right==0:\n",
        "                            recon = rdata[0,bot::,left::]\n",
        "                        else:\n",
        "                            recon = rdata[0,bot:-top,left:-right]\n",
        "            else:\n",
        "                 if og:\n",
        "                    if top==0 and right!=0:\n",
        "                        recon = rdata[bot::,left:-right]\n",
        "                    elif top!=0 and right==0:\n",
        "                        recon = rdata[bot:-top,left::]\n",
        "                    elif top==0 and right==0:\n",
        "                        recon = rdata[bot::,left::]\n",
        "                    else:\n",
        "                        recon = rdata[bot:-top,left:-right]\n",
        "                 else:\n",
        "                    if top==0 and right!=0:\n",
        "                        recon = rdata[bot::,left:-right]\n",
        "                    elif top!=0 and right==0:\n",
        "                        recon = rdata[bot:-top,left::]\n",
        "                    elif top==0 and right==0:\n",
        "                        recon = rdata[bot::,left::]\n",
        "                    else:\n",
        "                        recon = rdata[bot:-top,left:-right]\n",
        "\n",
        "\n",
        "    return recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTVIdZ836HDZ"
      },
      "source": [
        "This function is used for removing padded data to restore the data on original grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYJ9En9Q6HDZ"
      },
      "source": [
        "## Subroutine for Computing Linear Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.718533Z",
          "start_time": "2024-07-16T13:04:34.705404Z"
        },
        "id": "V5A0sw6f6HDZ"
      },
      "outputs": [],
      "source": [
        "def linreg(X,Y):\n",
        "    \"\"\"\n",
        "     return a,b in solution to y = ax + b such that root mean square distance\n",
        "     between trend line and original points is minimized\n",
        "    \"\"\"\n",
        "    N = len(X)\n",
        "    Sx = Sy = Sxx = Syy = Sxy = 0.0\n",
        "\n",
        "    for x, y in zip(X, Y):\n",
        "        Sx = Sx + x\n",
        "        Sy = Sy + y\n",
        "        Sxx = Sxx + x*x\n",
        "        Syy = Syy + y*y\n",
        "        Sxy = Sxy + x*y\n",
        "    det = Sxx * N - Sx * Sx\n",
        "    aa = (Sxy * N - Sy * Sx)/det\n",
        "    bb = (Sxx * Sy - Sx * Sxy)/det\n",
        "    YY2 = aa * X + bb\n",
        "    return X, YY2, aa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz-Ee6C46HDZ"
      },
      "source": [
        "This function is used for computing linear trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB-TzNda6HDa"
      },
      "source": [
        "## Subroutine for Mann-Kendall Trend Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.723973Z",
          "start_time": "2024-07-16T13:04:34.719657Z"
        },
        "id": "FhgsxN_l6HDa"
      },
      "outputs": [],
      "source": [
        "def mk_test(x, alpha = 0.05):\n",
        "    from scipy.stats import norm\n",
        "    \"\"\"\n",
        "    this perform the MK (Mann-Kendall) test to check if the trend is present in\n",
        "    data or not\n",
        "\n",
        "    Input:\n",
        "        x:   a vector of data\n",
        "        alpha: significance level\n",
        "\n",
        "    Output:\n",
        "        h: True (if trend is present) or False (if trend is absence)\n",
        "        p: p value of the sifnificance test\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "      >>> x = np.random.rand(100)\n",
        "      >>> h,p = mk_test(x,0.05)  # meteo.dat comma delimited\n",
        "    \"\"\"\n",
        "    n = len(x)\n",
        "\n",
        "    # calculate S\n",
        "    s = 0\n",
        "    for k in range(n-1):\n",
        "        for j in range(k+1,n):\n",
        "            s += np.sign(x[j] - x[k])\n",
        "\n",
        "    # calculate the unique data\n",
        "    unique_x = np.unique(x)\n",
        "    g = len(unique_x)\n",
        "\n",
        "    # calculate the var(s)\n",
        "    if n == g: # there is no tie\n",
        "        var_s = (n*(n-1)*(2*n+5))/18\n",
        "    else: # there are some ties in data\n",
        "        tp = np.zeros(unique_x.shape)\n",
        "        for i in range(len(unique_x)):\n",
        "            tp[i] = sum(unique_x[i] == x)\n",
        "        var_s = (n*(n-1)*(2*n+5) + np.sum(tp*(tp-1)*(2*tp+5)))/18\n",
        "\n",
        "    if s>0:\n",
        "        z = (s - 1)/np.sqrt(var_s)\n",
        "    elif s == 0:\n",
        "        z = 0\n",
        "    elif s<0:\n",
        "        z = (s + 1)/np.sqrt(var_s)\n",
        "\n",
        "    # calculate the p_value\n",
        "    p = 2*(1-norm.cdf(abs(z))) # two tail test\n",
        "    h = abs(z) > norm.ppf(1-alpha/2)\n",
        "\n",
        "    return h, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LWDfP57Zsfv"
      },
      "source": [
        "## Subroutine for Regional statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.729985Z",
          "start_time": "2024-07-16T13:04:34.725067Z"
        },
        "id": "MI9njZLHZsfv"
      },
      "outputs": [],
      "source": [
        "def regional_stats(uids,regids,rdata,kind=\"mean\"):\n",
        "\n",
        "     values_ = np.zeros(len(uids))\n",
        "\n",
        "     for jj,uid in enumerate(uids):\n",
        "         igrid_ = np.where(regids==uid)\n",
        "         if kind==\"mean\":\n",
        "             values_[jj] = rdata[igrid_].mean()\n",
        "         elif kind==\"max\":\n",
        "             values_[jj] = rdata[igrid_].max()\n",
        "         elif kind==\"min\":\n",
        "             values_[jj] = rdata[igrid_].min()\n",
        "\n",
        "     if kind==\"mean\":\n",
        "         ireg = np.argmax(values_)\n",
        "     elif kind==\"max\":\n",
        "         ireg = np.argmax(values_)\n",
        "     elif kind==\"min\":\n",
        "         ireg = np.argmin(values_)\n",
        "\n",
        "     return uids[ireg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bklLpXxu6HDa"
      },
      "source": [
        "This function is used for computing statistical significance in trend ([Mann-Kendall test](https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvjzX6-B6HDa"
      },
      "source": [
        "# Reading Input Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPAMd9GM6HDb"
      },
      "source": [
        "## Reading regional mask data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:34.770746Z",
          "start_time": "2024-07-16T13:04:34.731028Z"
        },
        "id": "5jcKPusXZsfv"
      },
      "outputs": [],
      "source": [
        "\n",
        "fXX = Dataset(regfile,\"r\")\n",
        "\n",
        "jreg1 = fXX.variables['jreg1'][0]\n",
        "jreg2 = fXX.variables['jreg2'][0]\n",
        "\n",
        "jreg1_uids = np.unique(jreg1)\n",
        "jreg2_uids = np.unique(jreg2)\n",
        "\n",
        "#--- use jreg2\n",
        "jreg = jreg2\n",
        "jreg_uids = jreg2_uids\n",
        "jreg_uids= [x for x in jreg2_uids if x != 0]\n",
        "\n",
        "\n",
        "nreg = len(jreg_uids)\n",
        "print (nreg)\n",
        "sregnames={\n",
        "    1:\"Eastern Europe\",\n",
        "    2:\"Northern Europe\",\n",
        "    3:\"Central Europe\",\n",
        "    4:\"Southeastern Europe\",\n",
        "    5:\"Western Europe\",\n",
        "    6:\"Southern Europe\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khlXIX4ZZsfv"
      },
      "source": [
        "## Reading Observed Precipitation Dataset (PRISM or APHRODITE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:04:35.229506Z",
          "start_time": "2024-07-16T13:04:34.771810Z"
        },
        "id": "q5x7Q48w6HDb"
      },
      "outputs": [],
      "source": [
        "if model==\"APHRODITE\":\n",
        "    ifile1 = \"%s/AphroJP_V1207_DPREC.1900-2010_V1207R3_DPREC.2011-2015_HiFLOR_5-day_anom_cprev20yrs.nc\"  % (datadir)\n",
        "    ielem = elem.lower()\n",
        "elif model==\"PRISMG\":\n",
        "    ifile1 = \"%s/PRISM_ppt_stable_4kmM3_1981-2021_GSPEAR_5-day_anom_cprev20yrs.nc\" % (datadir)\n",
        "    ielem= \"data\"\n",
        "elif model==\"E-OBS\":\n",
        "    ifile1 = \"%s/merge_RR_SPEAR_MED_EUROPE3_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    #ifile1 = \"%s/merge_RR_1x1_EUROPE3_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "\n",
        "    ielem= \"rr\" # made successful change here\n",
        "if elem == \"T_REF_MAX\":\n",
        "    if model==\"E-OBS\":\n",
        "           # ifile1 = \"%s/1950-2023_TX_1x1_EUROPE3_5-day_mean.nc\" % (datadir)\n",
        "        ifile1 = \"%s/merge_TX_1x1_EUROPE3_TX_5-day_anom_1950-2023_c1951-1970.nc\" % (datadir)\n",
        "        ielem= \"tx\" # made successful change here\n",
        "\n",
        "print (\"input file=\",ifile1)\n",
        "f11 = Dataset(ifile1,\"r\")\n",
        "\n",
        "times_orig = f11.variables['time']\n",
        "dates_orig = num2date(times_orig[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "dates_pd = pd.to_datetime(dates_orig)\n",
        "periods = dates_pd.to_period(freq='D')\n",
        "\n",
        "smon = int(selmonths.split(\",\")[0])\n",
        "emon = int(selmonths.split(\",\")[-1])\n",
        "mask_dates = (periods.year>=tsyear)&(periods.year<=teyear)&(periods.month>=smon)&(periods.month<=emon)\n",
        "#print (dates_pd[mask_dates])\n",
        "\n",
        "data = f11.variables[ielem][mask_dates]\n",
        "data = data.filled(fill_value=0)\n",
        "\n",
        "\n",
        "lons = f11.variables['lon']\n",
        "lats = f11.variables['lat']\n",
        "#times = f11.variables['time']\n",
        "times = times_orig[mask_dates]\n",
        "dates = num2date(times[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "print  (\"dates in the input data=\",dates)\n",
        "print (\"data dimension=(time,latitude,longitude)=\",np.shape(data[:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4RSaP3zZsfv"
      },
      "outputs": [],
      "source": [
        "if model==\"APHRODITE\":\n",
        "    ifile1 = \"%s/AphroJP_V1207_DPREC.1900-2010_V1207R3_DPREC.2011-2015_HiFLOR_5-day_anom_cprev20yrs.nc\"  % (datadir)\n",
        "    ielem = elem.lower()\n",
        "elif model==\"PRISMG\":\n",
        "    ifile1 = \"%s/PRISM_ppt_stable_4kmM3_1981-2021_GSPEAR_5-day_anom_cprev20yrs.nc\" % (datadir)\n",
        "    ielem= \"data\"\n",
        "elif model==\"E-OBS\":\n",
        "    ifile1 = \"%s/merge_RR_SPEAR_MED_EUROPE3_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    ifile1= \"/archive/hnm/for_someone/Sophia/RR/merge_RR_1x1_EUROPE3_5-day_anom_1950-2023_c1951-1970.nc\"\n",
        "    #ifile1 = \"%s/merge_RR_1x1_EUROPE3_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "\n",
        "    ielem= \"rr\" # made successful change here\n",
        "\n",
        "print (\"input file=\",ifile1)\n",
        "f11 = Dataset(ifile1,\"r\")\n",
        "\n",
        "times_orig = f11.variables['time']\n",
        "dates_orig = num2date(times_orig[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "dates_pd = pd.to_datetime(dates_orig)\n",
        "periods = dates_pd.to_period(freq='D')\n",
        "\n",
        "smon = int(selmonths.split(\",\")[0])\n",
        "emon = int(selmonths.split(\",\")[-1])\n",
        "mask_dates = (periods.year>=tsyear)&(periods.year<=teyear)&(periods.month>=smon)&(periods.month<=emon)\n",
        "#print (dates_pd[mask_dates])\n",
        "\n",
        "data_rr= f11.variables[ielem][mask_dates]\n",
        "data_rr = data_rr.filled(fill_value=0)\n",
        "\n",
        "\n",
        "lons = f11.variables['lon']\n",
        "lats = f11.variables['lat']\n",
        "#times = f11.variables['time']\n",
        "times = times_orig[mask_dates]\n",
        "dates = num2date(times[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "print  (\"dates in the input data=\",dates)\n",
        "print (\"data dimension=(time,latitude,longitude)=\",np.shape(data_rr[:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXGNzPKtZsfw"
      },
      "source": [
        "## Read Model Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SNW6OJdZsfw"
      },
      "source": [
        "### SPEAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:06:26.656282Z",
          "start_time": "2024-07-16T13:04:35.230715Z"
        },
        "id": "EHr-VyqHZsfw"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " data2=[]\n",
        " ename2=\"t_ref_max\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            #grid2=\"europe2_spear_med\"\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        " for ee in range(1,emax+1,1):\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f12 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig2 = f12.variables['time']\n",
        "   dates_orig2 = num2date(times_orig2[:],times_orig2.units ,calendar=times_orig2.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd2 = pd.to_datetime(dates_orig2)\n",
        "   periods2 = dates_pd2.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates2 = (periods2.year>=tsyear)&(periods2.year<=teyear)&(periods2.month>=smon)&(periods2.month<=emon)\n",
        "\n",
        "   data2_temp = f12.variables[ename2][mask_dates2]\n",
        "   data2_temp = data2_temp.filled(fill_value=0)\n",
        "\n",
        "   data2.append(data2_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons2 = f12.variables['lon']\n",
        "     lats2 = f12.variables['lat']\n",
        "#     times2 = f12.variables['time']\n",
        "\n",
        "#     print  (times2)\n",
        "     dates2 = dates_orig2[mask_dates2]\n",
        "\n",
        " data2=np.array(data2, dtype=np.float16)\n",
        "\n",
        " data3=[]\n",
        " for ee in range(1,emax2+1,1):\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data3.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data3=np.array(data3, dtype=np.float16)\n",
        " print (np.shape(data2))\n",
        " print (np.shape(data3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bvGU-GWZsfw"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " data_rr2=[]\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            #grid2=\"europe2_spear_med\"\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        " for ee in range(1,emax+1,1):\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f12 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig2 = f12.variables['time']\n",
        "   dates_orig2 = num2date(times_orig2[:],times_orig2.units ,calendar=times_orig2.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd2 = pd.to_datetime(dates_orig2)\n",
        "   periods2 = dates_pd2.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates2 = (periods2.year>=tsyear)&(periods2.year<=teyear)&(periods2.month>=smon)&(periods2.month<=emon)\n",
        "\n",
        "   data2_temp = f12.variables[ename2][mask_dates2]\n",
        "   data2_temp = data2_temp.filled(fill_value=0)\n",
        "\n",
        "   data_rr2.append(data2_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons2 = f12.variables['lon']\n",
        "     lats2 = f12.variables['lat']\n",
        "#     times2 = f12.variables['time']\n",
        "\n",
        "#     print  (times2)\n",
        "     dates2 = dates_orig2[mask_dates2]\n",
        "\n",
        " data_rr2=np.array(data_rr2, dtype=np.float16)\n",
        "\n",
        " data_rr3=[]\n",
        " for ee in range(1,emax2+1,1):\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data_rr3.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data_rr3=np.array(data_rr3, dtype=np.float16)\n",
        " print (np.shape(data_rr2))\n",
        " print (np.shape(data_rr3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZLlfu5XZsfw"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"t_ref_max\"\n",
        " print(\"TEST\")\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "     grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " data4=[]\n",
        " testmodel2=\"NATURAL\"\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       #print(\"here\")\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data4.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data4=np.array(data4, dtype=np.float16)\n",
        " print (np.shape(data4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZptCqlu1Zsfw"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " print(\"TEST\")\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        grid2=\"europe3_1x1\"\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " data_rr4=[]\n",
        " testmodel2=\"NATURAL\"\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       #print(\"here\")\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data_rr4.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data_rr4=np.array(data_rr4, dtype=np.float16)\n",
        " print (np.shape(data_rr4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbdE9SXx6HDb"
      },
      "source": [
        "The input netcdf data are already converted from the original precipitation data to 5-day mean anomaly.<br>\n",
        "Input data is red through python-cdo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAy8pvE56HDb"
      },
      "source": [
        "## Normalizing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEHpbidNZsfw"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:06:26.738085Z",
          "start_time": "2024-07-16T13:06:26.657942Z"
        },
        "id": "kyhtVvKX6HDb"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data.min()=\",data[:].min())\n",
        "print (\"data.max()=\",data[:].max())\n",
        "\n",
        "data_norm = (data[:].squeeze() - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm.min()=\",data_norm.min())\n",
        "print (\"data_norm.max()=\",data_norm.max())\n",
        "print (\"input data dimension (time,lat,lon)=\",np.shape(data_norm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qqDg2W5Zsfw"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data.min()=\",data_rr[:].min())\n",
        "print (\"data.max()=\",data_rr[:].max())\n",
        "\n",
        "data_norm_rr = (data_rr[:].squeeze() - minvalueRR)/(maxvalueRR-minvalueRR)\n",
        "print (\"data_norm.min()=\",data_norm_rr.min())\n",
        "print (\"data_norm.max()=\",data_norm_rr.max())\n",
        "print (\"input data dimension (time,lat,lon)=\",np.shape(data_norm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhgOaS0nZsfw"
      },
      "outputs": [],
      "source": [
        "data_ver = np.stack([data_norm,data_norm_rr],axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAVY3qPiZsfw"
      },
      "outputs": [],
      "source": [
        "np.shape(data_ver)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXh02kYEZsfw"
      },
      "source": [
        "### Model training period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:06:59.587181Z",
          "start_time": "2024-07-16T13:06:26.739162Z"
        },
        "id": "i9-3qPouZsfw"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data2.min()=\",data2[:].min())\n",
        "print (\"data2.max()=\",data2[:].max())\n",
        "\n",
        "data_norm2 = (data2[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm2.min()=\",data_norm2.min())\n",
        "print (\"data_norm2.max()=\",data_norm2.max())\n",
        "print (\"input data2 dimension (ens,time,lat,lon)=\",np.shape(data_norm2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zANuOXSfZsfw"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data2.min()=\",data_rr2[:].min())\n",
        "print (\"data2.max()=\",data_rr2[:].max())\n",
        "\n",
        "minvalue2 = -10\n",
        "maxvalue2 = 100\n",
        "\n",
        "\n",
        "data_norm_rr2 = (data_rr2[:] - minvalue2)/(maxvalue2-minvalue2)\n",
        "print (\"data_norm2.min()=\",data_norm2.min())\n",
        "print (\"data_norm2.max()=\",data_norm2.max())\n",
        "print (\"input data2 dimension (ens,time,lat,lon)=\",np.shape(data_norm2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ce7OLp5Zsfw"
      },
      "outputs": [],
      "source": [
        "data_ver2 = np.stack([data_norm2,data_norm_rr2],axis=4)\n",
        "np.shape(data_ver2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujb-7XYUZsfw"
      },
      "source": [
        "### Model test period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:21.341343Z",
          "start_time": "2024-07-16T13:06:59.588574Z"
        },
        "id": "cj3MsrwGZsfw"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data3.min()=\",data3[:].min())\n",
        "print (\"data3.max()=\",data3[:].max())\n",
        "\n",
        "data_norm3 = (data3[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm3.min()=\",data_norm3.min())\n",
        "print (\"data_norm3.max()=\",data_norm3.max())\n",
        "print (\"input data3 dimension (ens,time,lat,lon)=\",np.shape(data_norm3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQvfImHeZsfw"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data3.min()=\",data_rr3[:].min())\n",
        "print (\"data3.max()=\",data_rr3[:].max())\n",
        "\n",
        "data_norm_rr3 = (data_rr3[:] - minvalue2)/(maxvalue2-minvalue2)\n",
        "print (\"data_norm3.min()=\",data_norm_rr3.min())\n",
        "print (\"data_norm3.max()=\",data_norm_rr3.max())\n",
        "print (\"input data3 dimension (ens,time,lat,lon)=\",np.shape(data_norm_rr3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGAvNV5YZsfx"
      },
      "outputs": [],
      "source": [
        "data_ver3 = np.stack([data_norm3,data_norm_rr3],axis=4)\n",
        "np.shape(data_ver3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUv1vex-Zsfx"
      },
      "source": [
        "### Natural Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:21.341343Z",
          "start_time": "2024-07-16T13:06:59.588574Z"
        },
        "id": "3AtbNagsZsfx"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data3.min()=\",data4[:].min())\n",
        "print (\"data3.max()=\",data4[:].max())\n",
        "\n",
        "minvalue_ = -21.3\n",
        "maxvalue_ = 20\n",
        "\n",
        "data_norm4 = (data4[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm3.min()=\",data_norm4.min())\n",
        "print (\"data_norm3.max()=\",data_norm4.max())\n",
        "print (\"input data3 dimension (ens,time,lat,lon)=\",np.shape(data_norm4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf3-v4C8Zsfx"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data3.min()=\",data_rr4[:].min())\n",
        "print (\"data3.max()=\",data_rr4[:].max())\n",
        "\n",
        "\n",
        "data_norm_rr4 = (data_rr4[:] - minvalue2)/(maxvalue2-minvalue2)\n",
        "print (\"data_norm3.min()=\",data_norm_rr4.min())\n",
        "print (\"data_norm3.max()=\",data_norm_rr4.max())\n",
        "print (\"input data3 dimension (ens,time,lat,lon)=\",np.shape(data_norm_rr4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9BcrwZWZsfx"
      },
      "outputs": [],
      "source": [
        "data_ver4 = np.stack([data_norm4,data_norm_rr4],axis=4)\n",
        "np.shape(data_ver4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7n8gDv6HDc"
      },
      "source": [
        "## Editing Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:21.378500Z",
          "start_time": "2024-07-16T13:08:21.343783Z"
        },
        "id": "zewhQ3kh6HDc"
      },
      "outputs": [],
      "source": [
        "# Date\n",
        "yyyymmdds=[]\n",
        "for date in dates[:]:\n",
        "    yyyymmdds.append(int(\"%4.4i%2.2i%2.2i\" % (date.year,date.month,date.day)))\n",
        "for date in dates[:]:\n",
        "    yyyymmdds.append(int(\"%4.4i%2.2i%2.2i\" % (date.year,date.month,date.day)))\n",
        "\n",
        "ds = pd.Series(yyyymmdds)\n",
        "dates = pd.to_datetime(ds, format='%Y%m%d')\n",
        "print (dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_HvfhqeZsfx"
      },
      "outputs": [],
      "source": [
        "# Get date for training\n",
        "yyyymmdds2=[]\n",
        "for date2 in dates2[:]:\n",
        "    yyyymmdds2.append(int(\"%4.4i%2.2i%2.2i\" % (date2.year,date2.month,date2.day)))\n",
        "\n",
        "ds2 = pd.Series(yyyymmdds2)\n",
        "dates2 = pd.to_datetime(ds2, format='%Y%m%d')\n",
        "print (dates2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBF6v41gZsfx"
      },
      "outputs": [],
      "source": [
        "yyyymmdds3=[]\n",
        "for date3 in dates3[:]:\n",
        "    yyyymmdds3.append(int(\"%4.4i%2.2i%2.2i\" % (date3.year,date3.month,date3.day)))\n",
        "\n",
        "ds3 = pd.Series(yyyymmdds3)\n",
        "dates3 = pd.to_datetime(ds3, format='%Y%m%d')\n",
        "print (dates3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqnIc24Z6HDc"
      },
      "source": [
        "## Padding the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:22.225400Z",
          "start_time": "2024-07-16T13:08:21.533337Z"
        },
        "id": "Jh213qWvZsfx"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list = data_ver\n",
        "print (np.shape(temp_img_array_list))\n",
        "pad_width2=((0,0),(1,0),(4,4),(0,0))\n",
        "temp_img_array_list = np.pad(temp_img_array_list, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "#temp_img_array_list2 =  np.expand_dims(temp_img_array_list2, axis=-1)\n",
        "\n",
        "emax2,tmax2,jmax2,imax2= np.shape(temp_img_array_list)\n",
        "print (np.shape(temp_img_array_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_sTX7lVZsfx"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list2 = data_ver2\n",
        "print (np.shape(temp_img_array_list2))\n",
        "pad_width2=((0,0),(0,0),(1,0),(4,4),(0,0))\n",
        "temp_img_array_list2 = np.pad(temp_img_array_list2, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "#temp_img_array_list2 =  np.expand_dims(temp_img_array_list2, axis=-1)\n",
        "\n",
        "emax2,tmax2,jmax2,imax2,zmax2= np.shape(temp_img_array_list2)\n",
        "print (np.shape(temp_img_array_list2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ctBVpCzZsfx"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3 = data_ver3\n",
        "print (np.shape(temp_img_array_list3))\n",
        "pad_width2=((0,0),(0,0),(1,0),(4,4),(0,0))\n",
        "temp_img_array_list3 = np.pad(temp_img_array_list3, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "#temp_img_array_list2 =  np.expand_dims(temp_img_array_list2, axis=-1)\n",
        "\n",
        "emax3,tmax3,jmax3,imax3,zmax3= np.shape(temp_img_array_list3)\n",
        "print (np.shape(temp_img_array_list3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztyAdy_FZsfx"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list4 = data_ver4\n",
        "print (np.shape(temp_img_array_list4))\n",
        "pad_width2=((0,0),(0,0),(1,0),(4,4),(0,0))\n",
        "temp_img_array_list4 = np.pad(temp_img_array_list4, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "#temp_img_array_list2 =  np.expand_dims(temp_img_array_list2, axis=-1)\n",
        "\n",
        "emax3,tmax3,jmax3,imax3,zmax3= np.shape(temp_img_array_list4)\n",
        "print (np.shape(temp_img_array_list4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7-T2g326HDd"
      },
      "source": [
        "Line 6: Zero values are padded to the edges of longitude and latitude.<br>\n",
        "Line 8: Reshape the dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjGfKM0AZsfx"
      },
      "outputs": [],
      "source": [
        "emax12, tmax12,jmax12,imax12,kmax12 = np.shape(temp_img_array_list2)\n",
        "temp_img_array_list2_ = temp_img_array_list2.reshape((emax12*tmax12,jmax12,imax12,kmax12))\n",
        "print (np.shape(temp_img_array_list2_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnrB4f0h6HDd"
      },
      "source": [
        "## Dividing the input data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:23.451875Z",
          "start_time": "2024-07-16T13:08:23.410491Z"
        },
        "id": "0OGa8lXU6HDe"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(temp_img_array_list, test_size=0.2, random_state=1)\n",
        "\n",
        "print (\"dimension for training data=\",np.shape(train_data))\n",
        "print (\"dimension for validation data=\",np.shape(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:23.980387Z",
          "start_time": "2024-07-16T13:08:23.452979Z"
        },
        "id": "qt_VC4nhZsfx"
      },
      "outputs": [],
      "source": [
        "train_data2, test_data2 = train_test_split(temp_img_array_list2_, test_size=0.2, random_state=1)\n",
        "\n",
        "print (\"dimension for training data=\",np.shape(train_data2))\n",
        "print (\"dimension for validation data=\",np.shape(test_data2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUBMoC16HDe"
      },
      "source": [
        "Split all the data into training data and test data. 20% of the data is assigned to validation data.<br>\n",
        "The training data is used for developing autoencoder, whreas the test data is used for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XONJCuQJ6HDe"
      },
      "source": [
        "# Compiling Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K4MAZtH6HDe"
      },
      "source": [
        "This part is the main core for the autoencoder configuration.<br>\n",
        "There are some layers to build autoencoder.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:23.983239Z",
          "start_time": "2024-07-16T13:08:23.981470Z"
        },
        "id": "ljiB2LCX6HDf"
      },
      "outputs": [],
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 40 # changed from 40 because the data is so large\n",
        "INIT_LR = 1e-3\n",
        "#BS = 8\n",
        "BS = 16\n",
        "\n",
        "# reason for choosing these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgCL-35Z6HDf"
      },
      "source": [
        "EPOCHS: Iteration number to train<br>\n",
        "INIT_LR: Used for decay parameter for Adam (optimizer)<br>\n",
        "BS: Batch Size for training<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH4ajwk16HDf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Configure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:23.994427Z",
          "start_time": "2024-07-16T13:08:23.984194Z"
        },
        "id": "ZHzwDe8V6HDf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "class ConvAutoencoder:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, filters=(32, 64), latentDim=16, suffix=\"0\"):\n",
        "        # initialize the input shape to be \"channels last\" along with\n",
        "        # the channels dimension itself\n",
        "        # channels dimension itself\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # define the input to the encoder\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = inputs\n",
        "\n",
        "        # loop over the number of filters\n",
        "        for f in filters:\n",
        "            # apply a CONV => RELU => BN operation\n",
        "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "           #x = Activation('tanh')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # flatten the network and then construct our latent vector\n",
        "        volumeSize = K.int_shape(x)\n",
        "        x = Flatten()(x)\n",
        "        latent = Dense(latentDim)(x)\n",
        "\n",
        "        # build the encoder model\n",
        "        encoder = Model(inputs, latent, name=\"encoder_%s\" % (suffix))\n",
        "\n",
        "        # start building the decoder model which will accept the\n",
        "        # output of the encoder as its inputs\n",
        "        latentInputs = Input(shape=(latentDim,))\n",
        "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\n",
        "        # loop over our number of filters again, but this time in\n",
        "        # reverse order\n",
        "        for f in filters[::-1]:\n",
        "            # apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "            x = Conv2DTranspose(f, (3, 3), strides=2,\n",
        "                padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            #x = Activation('tanh')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # apply a single CONV_TRANSPOSE layer used to recover the\n",
        "        # original depth of the image\n",
        "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "        outputs = Activation(\"sigmoid\")(x)\n",
        "        #outputs = Activation(\"tanh\")(x)\n",
        "\n",
        "        # build the decoder model\n",
        "        decoder = Model(latentInputs, outputs, name=\"decoder_%s\" % (suffix))\n",
        "\n",
        "        # our autoencoder is the encoder + decoder\n",
        "        autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
        "            name=\"autoencoder_%s\" % (suffix))\n",
        "\n",
        "        # return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "        return (encoder, decoder, autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabTBL5b6HDg"
      },
      "source": [
        "L18-43: Developing encoder <br>\n",
        "L31: Conv2D(f, (3,3)...) is the 2D convolution layer. f(= [32, 64]) is filter and (3,3) is kernel.<br>\n",
        "L40: Dense(16) is the dense layer to make a lateral data<br>\n",
        "L43: Define encoder<br>\n",
        "\n",
        "L44-L69: Developing decoder<br>\n",
        "L55, L66: Transposed convolution layer<br>\n",
        "\n",
        "L72: Defining autoencoder (= decoder(encoder(inputs)))<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0wodThZsfy"
      },
      "source": [
        "### AutoencoderEnsemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:24.005809Z",
          "start_time": "2024-07-16T13:08:23.995413Z"
        },
        "id": "rJZMPlYaZsfy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "class AutoencoderEnsemble:\n",
        "    def __init__(self, encoders, decoders, input_shape, optimizer_class, optimizer_kwargs):\n",
        "        self.encoders = encoders\n",
        "        self.decoders = decoders\n",
        "        self.input_shape = input_shape\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "        self.models = []\n",
        "        self.histories = []\n",
        "        self._build_models()\n",
        "\n",
        "    def _build_models(self):\n",
        "        for encoder, decoder in zip(self.encoders, self.decoders):\n",
        "            common_input = Input(shape=self.input_shape)\n",
        "            output = decoder(encoder(common_input))\n",
        "            autoencoder = Model(common_input, output)\n",
        "            optimizer = self.optimizer_class(**self.optimizer_kwargs)\n",
        "            autoencoder.compile(loss=\"mse\", optimizer=optimizer)\n",
        "            self.models.append(autoencoder)\n",
        "\n",
        "    def train_models(self, train_data, train_labels, epochs, batch_size, validation_data=None,verbose=1):\n",
        "        self.histories = []\n",
        "        for model in self.models:\n",
        "            history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=validation_data,verbose=verbose)\n",
        "            self.histories.append(history.history)\n",
        "\n",
        "    def predict(self, test_data, model_to_predict=0, verbose=1):\n",
        "        if model_to_predict==0:\n",
        "            predictions = [model.predict(test_data, verbose=verbose) for model in self.models]\n",
        "            averaged_output = np.mean(predictions, axis=0)\n",
        "        else:\n",
        "            averaged_output = self.models[model_to_predict-1].predict(test_data)\n",
        "        return averaged_output\n",
        "\n",
        "    def summary(self):\n",
        "        for model in self.models:\n",
        "             print (model.summary())\n",
        "\n",
        "    def plot_learning_curves(self, datatype,fileheader):\n",
        "        plt.style.use(\"ggplot\")\n",
        "        plt.figure()\n",
        "\n",
        "        for i, history in enumerate(self.histories):\n",
        "            epochs = range(len(history['loss']))\n",
        "            plt.plot(epochs, history[\"loss\"], label=f\"train_loss_model_{i+1}\")\n",
        "            if 'val_loss' in history:\n",
        "                plt.plot(epochs, history[\"val_loss\"], label=f\"val_loss_model_{i+1}\")\n",
        "\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Epoch #\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend(loc=\"lower left\")\n",
        "        plt.savefig(f\"{datatype}/{fileheader}.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def save_models(self, directory, fileheader):\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        for i, model in enumerate(self.models):\n",
        "            model_path = os.path.join(directory, f\"{fileheader}_{i+1}.h5\")\n",
        "            model.save(model_path)\n",
        "            print(f\"[INFO] saved model {i+1} to {model_path}\")\n",
        "\n",
        "    def load_models(self, directory, fileheader, num_models, custom_objects=None):\n",
        "        self.models = []\n",
        "        for i in range(1, num_models+1):  #  number of models\n",
        "            model_path = os.path.join(directory, f\"{fileheader}_{i}.h5\")\n",
        "            print(f\"[INFO] loading model {i} from {model_path}\")\n",
        "            model = load_model(model_path, custom_objects=custom_objects)\n",
        "            self.models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SreQx9k6HDg",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Compile Autoencoder for Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:08:24.324347Z",
          "start_time": "2024-07-16T13:08:24.006736Z"
        },
        "id": "6_N-24C_6HDh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "tmax,jmax,imax,zmax = np.shape(temp_img_array_list)\n",
        "print (tmax,jmax,imax,zmax)\n",
        "\n",
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax,jmax,zmax, filters=(32, 64, 128), latentDim=64)\n",
        "elif domain==\"USA\":\n",
        "    (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64), latentDim=32)\n",
        "elif domain==\"EU\":\n",
        "   # (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64,128), latentDim=32)\n",
        "   # (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32), latentDim=64)\n",
        "    (encoder_1, decoder_1, autoencoder_1) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32), latentDim=32, suffix=\"1\")\n",
        "    (encoder_2, decoder_2, autoencoder_2) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64), latentDim=32, suffix=\"2\")\n",
        "   #(encoder_3, decoder_3, autoencoder_3) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64,128), latentDim=64, suffix=\"3\")\n",
        "    (encoder_3, decoder_3, autoencoder_3) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32,64), latentDim=128, suffix=\"3\")\n",
        "\n",
        "encoders = [encoder_1,encoder_2,encoder_3]\n",
        "decoders = [decoder_1,decoder_2,decoder_3]\n",
        "input_shape = (jmax,imax,zmax)\n",
        "\n",
        "# opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "#opt = Adam(learning_rate = INIT_LR,weight_decay = INIT_LR/EPOCHS) #updated version\n",
        "#opt = Adam(learning_rate = INIT_LR,weight_decay = INIT_LR/EPOCHS) #updated version\n",
        "optimizer_class = Adam\n",
        "optimizer_kwargs = {\"learning_rate\": INIT_LR, \"weight_decay\": INIT_LR / EPOCHS}\n",
        "\n",
        "\n",
        "# combined autoencoders\n",
        "autoencoder = AutoencoderEnsemble(encoders, decoders,input_shape,optimizer_class, optimizer_kwargs)\n",
        "print(autoencoder.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19IerV5S6HDh"
      },
      "source": [
        "For JP, (latitude, longitude, buffer) = (96,96,1) data is converted to (64), then restored to (96,96,1). <br>\n",
        "For USA, (latitude, longitude, buffer) = (32,52,1) data is converted to (32), then restored to (32,52,1)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09X9Ov196HDh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Training Autoencoder (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.058736Z",
          "start_time": "2024-07-16T13:08:24.325375Z"
        },
        "id": "aCbhnuOjZsfy"
      },
      "outputs": [],
      "source": [
        "autoencoder.train_models(train_data, train_data, EPOCHS,  BS, validation_data=(test_data,test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.269291Z",
          "start_time": "2024-07-16T13:19:32.061184Z"
        },
        "id": "iaz6Zo1aZsfy"
      },
      "outputs": [],
      "source": [
        "fileheader=\"plot_mse\"\n",
        "autoencoder.plot_learning_curves(datatype,fileheader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "he8pupSBZsfy"
      },
      "source": [
        "## Compile Autoencoder for Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.773859Z",
          "start_time": "2024-07-16T13:19:32.270441Z"
        },
        "id": "cUiy1qq-Zsfy"
      },
      "outputs": [],
      "source": [
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "\n",
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax,jmax,zmax, filters=(32, 64, 128), latentDim=64)\n",
        "elif domain==\"USA\":\n",
        "    (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32)\n",
        "elif domain==\"EU\":\n",
        "   # (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64,128), latentDim=32) # may need changes here// ?\n",
        "   # (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32), latentDim=64) # may need changes here// ?\n",
        "   #(encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32) # may need changes here// ?\n",
        "    (encoder2_1, decoder2_1, autoencoder2_1) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32), latentDim=32, suffix=\"1\")\n",
        "    (encoder2_2, decoder2_2, autoencoder2_2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32, suffix=\"2\")\n",
        "   #(encoder2_3, decoder2_3, autoencoder2_3) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64,128), latentDim=64, suffix=\"3\")\n",
        "    (encoder2_3, decoder2_3, autoencoder2_3) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32,64), latentDim=64, suffix=\"3\")\n",
        "\n",
        "encoders2 = [encoder2_1,encoder2_2,encoder2_3]\n",
        "decoders2 = [decoder2_1,decoder2_2,decoder2_3]\n",
        "input_shape2 = (jmax2,imax2,kmax12)\n",
        "\n",
        "optimizer_class2 = Adam\n",
        "optimizer_kwargs2 = {\"learning_rate\": INIT_LR, \"weight_decay\": INIT_LR / EPOCHS}\n",
        "\n",
        "autoencoder2 = AutoencoderEnsemble(encoders2, decoders2, input_shape2, optimizer_class2, optimizer_kwargs2)\n",
        "print(autoencoder2.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuNGk-UeZsfy"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list2)\n",
        "np.shape(train_data2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzUWquCkZsfy"
      },
      "source": [
        "## Training Autoencoder (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.775793Z",
          "start_time": "2024-07-16T13:19:32.775783Z"
        },
        "id": "UqVKZKJjZsfy"
      },
      "outputs": [],
      "source": [
        "autoencoder2.train_models(train_data2, train_data2, EPOCHS, BS, validation_data=(test_data2,test_data2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.776795Z",
          "start_time": "2024-07-16T13:19:32.776787Z"
        },
        "id": "Pe3oUVofZsfy"
      },
      "outputs": [],
      "source": [
        "fileheader=\"plot_model_mse\"\n",
        "autoencoder2.plot_learning_curves(datatype,fileheader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdyETCC26HDi"
      },
      "source": [
        "”Loss function”is a fancy mathematical term for an object that measures how often a model makes an incorrect prediction. In the context of classification, they measure how often a model misclassifies members of different groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoUc3iOW6HDi",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Save Temporal Data (Autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.777323Z",
          "start_time": "2024-07-16T13:19:32.777316Z"
        },
        "id": "SdFDJEJ56HDi"
      },
      "outputs": [],
      "source": [
        "# serialize the autoencoder model to disk\n",
        "print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder.save_models(\"%s/output_ver\" % (datatype), \"autoencoder.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.777976Z",
          "start_time": "2024-07-16T13:19:32.777969Z"
        },
        "id": "tYD2mgv3Zsfy"
      },
      "outputs": [],
      "source": [
        "# serialize the autoencoder model to disk\n",
        "print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder2.save_models(\"%s/output_model\" % (datatype), \"autoencoder_model.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3cTh8n56HDi"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.778440Z",
          "start_time": "2024-07-16T13:19:32.778433Z"
        },
        "id": "S5qGGjzZZsfy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "custom_objects = {'mse': MeanSquaredError()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.779119Z",
          "start_time": "2024-07-16T13:19:32.779112Z"
        },
        "id": "Zf9TVbXf6HDi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load the model and image data from disk\n",
        "print(\"[INFO] loading autoencoder and image data...\")\n",
        "num_models=len(encoders)\n",
        "autoencoder.load_models(\"%s/output_ver\" % (datatype), \"autoencoder.model\", num_models, custom_objects=custom_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.779693Z",
          "start_time": "2024-07-16T13:19:32.779685Z"
        },
        "id": "UbjuT-GnZsfz"
      },
      "outputs": [],
      "source": [
        "# load the model and image data from disk\n",
        "print(\"[INFO] loading autoencoder and image data...\")\n",
        "num_models2=len(encoders2)\n",
        "autoencoder2.load_models(\"%s/output_model\" % (datatype), \"autoencoder_model.model\", num_models2, custom_objects=custom_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "zCPIr0hxZsfz"
      },
      "source": [
        "# Check autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVz0OcZbZsfz"
      },
      "source": [
        "np.shape(dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StUokI4ZZsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icZ2lMQdZsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHIvlzDSZsfz"
      },
      "source": [
        "## Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJFIOFh7Zsfz"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2 = np.random.randint(0,len(temp_img_array_list), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idx2:\n",
        "\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list[i,:,:,0]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "   #decoded = autoencoder_2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,0],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    print(np.shape(recon))\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.780198Z",
          "start_time": "2024-07-16T13:19:32.780190Z"
        },
        "id": "6IWjt4-bZsfz"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2 = np.random.randint(0,len(temp_img_array_list), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idx2:\n",
        "\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list[i,:,:,1]*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "   #decoded = autoencoder_2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,1],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "    print(np.shape(recon))\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5nNeVWdZsfz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNUOZmQvZsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2L_FBQ3Zsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6IADi3iZsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(original2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.780854Z",
          "start_time": "2024-07-16T13:19:32.780847Z"
        },
        "id": "Jt6dc8o4Zsfz"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list[0,:]), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates2[i])\n",
        "    date = dates2[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ee,ii,:,:,0]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,0],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_model/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates2[ii].year,dates2[ii].month,dates2[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGMi4d_zZsfz"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bAQHpvUZsfz"
      },
      "outputs": [],
      "source": [
        "len(temp_img_array_list2[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I28lxQZpZsfz"
      },
      "outputs": [],
      "source": [
        "errors.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SThV1Da3Zsfz"
      },
      "outputs": [],
      "source": [
        "original.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPD7dX8nZsfz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX9KtmwdZsfz"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list2[0,:]), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates2[i])\n",
        "    date = dates2[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ee,ii,:,:,1]*(maxvalue2-minvalue2)+minvalue2\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,1],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalue2-minvalue2)+minvalue2\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_model/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates2[ii].year,dates2[ii].month,dates2[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibAdtKDw6HDi"
      },
      "source": [
        "# Compute Mean Square Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "TG6KUyDhZsfz"
      },
      "source": [
        "## Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.781398Z",
          "start_time": "2024-07-16T13:19:32.781390Z"
        },
        "id": "NYdPTt-46HDj"
      },
      "outputs": [],
      "source": [
        "def comp_error(images, autoencoder, verbose=1):\n",
        "    tmax12_,jmax12_,imax12_,kmax12_ = np.shape(images)\n",
        "    print(np.shape(images))\n",
        "    errors_ = np.zeros(tmax12_)\n",
        "   #recons_ = autoencoder.predict(images, verbose=verbose)\n",
        "    recons_ = autoencoder.predict(images)\n",
        "    for tt in range(tmax12_):\n",
        "        errors_[tt] = np.mean((images[tt] - recons_[tt])**2)\n",
        "        if tt%1000==0:\n",
        "            print (\"%i/%i %f\" % (tt+1,tmax12_,errors_[tt]))\n",
        "    del(recons_)\n",
        "    return errors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRKaZqMpZsf0"
      },
      "outputs": [],
      "source": [
        "def comp_error1(images, autoencoder,model, verbose=1):\n",
        "    tmax12_,jmax12_,imax12_,kmax12_ = np.shape(images)\n",
        "    print(np.shape(images))\n",
        "    errors_ = np.zeros(tmax12_)\n",
        "   #recons_ = autoencoder.predict(images, verbose=verbose)\n",
        "    recons_ = autoencoder.predict(images,model_to_predict=model)\n",
        "    for tt in range(tmax12_):\n",
        "        errors_[tt] = np.mean((images[tt] - recons_[tt])**2)\n",
        "        if tt%1000==0:\n",
        "            print (\"%i/%i %f\" % (tt+1,tmax12_,errors_[tt]))\n",
        "    del(recons_)\n",
        "    return errors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqLB7QbGZsf0"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5LzP6f_Zsf0"
      },
      "outputs": [],
      "source": [
        "errors= comp_error(temp_img_array_list, autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbnqYxDyZsf0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ3oY1hZsf0"
      },
      "outputs": [],
      "source": [
        "np.shape(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.782693Z",
          "start_time": "2024-07-16T13:19:32.782685Z"
        },
        "id": "hi4D_2mI6HDj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# saving error to disk\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors, \"%s/output_ver/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.783329Z",
          "start_time": "2024-07-16T13:19:32.783322Z"
        },
        "id": "uRKyq3vG6HDj"
      },
      "outputs": [],
      "source": [
        "# load error from disk\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors = pickle_load(\"%s/output_ver/error.pickle\" % (datatype))\n",
        "errors = np.array(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.783857Z",
          "start_time": "2024-07-16T13:19:32.783850Z"
        },
        "id": "czouYY6x6HDj"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh = np.quantile(errors, 0.95)\n",
        "idxs = np.where(np.array(errors) >= thresh)[0]\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdTKrzPYZsf0"
      },
      "outputs": [],
      "source": [
        "np.shape(idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsyG0jynZsf0"
      },
      "outputs": [],
      "source": [
        "np.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvlT58VAZsf0"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.784425Z",
          "start_time": "2024-07-16T13:19:32.784418Z"
        },
        "id": "PSnnLo04Zsf0"
      },
      "outputs": [],
      "source": [
        "emax12_,tmax12_,jmax12_,imax12_,kmax12_ = np.shape(temp_img_array_list2)\n",
        "errors2_ = comp_error(temp_img_array_list2_, autoencoder2)\n",
        "errors2 = errors2_.reshape((emax12_, tmax12_,))\n",
        "np.shape(errors2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI_hSviVZsf0"
      },
      "outputs": [],
      "source": [
        "errors2 = errors2_.reshape((emax12_, tmax12_,))\n",
        "np.shape(errors2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7bDjR03Zsf0"
      },
      "outputs": [],
      "source": [
        "emax12_,tmax12_,jmax12_,imax12_,kmax12_ = np.shape(temp_img_array_list2)\n",
        "errors3_ = comp_error(temp_img_array_list2_, autoencoder2)\n",
        "errors3 = errors3_.reshape((emax12_, tmax12_,))\n",
        "np.shape(errors3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.785018Z",
          "start_time": "2024-07-16T13:19:32.785011Z"
        },
        "id": "I87xAM7NZsf0"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors2, \"%s/output_model/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.785627Z",
          "start_time": "2024-07-16T13:19:32.785619Z"
        },
        "id": "TjqdNFSTZsf0"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors2 = pickle_load(\"%s/output_model/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.786140Z",
          "start_time": "2024-07-16T13:19:32.786133Z"
        },
        "id": "hlsCWm2YZsf0"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh2 = np.quantile(errors2.flatten(), 0.95)\n",
        "idxs_model= np.where(np.array(errors2) >= thresh2)\n",
        "idxs_model = np.array(idxs_model)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_model[0,:])))\n",
        "thresh2, np.shape(idxs_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F93MUEj6HDk"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W2rRQ5P6HDk",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Plot Each Anomalous Day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s70kS-A36HDk"
      },
      "source": [
        "Drawing anomalous eents (Left panel is the original data, right panel is the restored data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcd_kFsZsf0"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.786778Z",
          "start_time": "2024-07-16T13:19:32.786771Z"
        },
        "id": "-myOzQm56HDk"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idxs[0:3]:\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[i,:,:,0]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,0],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xHrpAuMZsf0"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idxs[0:3]:\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[i,:,:,1]*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded[:,:,:,1],domain,grid,og=False) # reconstructed\n",
        "    recon = recon*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZR88QXZsf0"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.787331Z",
          "start_time": "2024-07-16T13:19:32.787324Z"
        },
        "id": "9rEB7O3tZsf0"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "pmax, kmax = np.shape(idxs_model)\n",
        "idx2a = np.random.randint(0,kmax,10) # display 10 sample pnly\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idx2a:\n",
        "\n",
        "    ee = idxs_model[0,i]\n",
        "    tt = idxs_model[1,i]\n",
        "\n",
        "    print (ee,tt,dates2[tt])\n",
        "\n",
        "    date = dates2[tt]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ee,tt]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[ee,tt], axis=0)\n",
        "\n",
        "    decoded = autoencoder.predict(image2, verbose=0) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i ee%2.2i\" % (date.year,date.month,date.day,ee+1), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i ee%2.2i\" % (date.year,date.month,date.day,ee+1), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_model/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,date.year,date.month,date.day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upFfqs5H6HDl"
      },
      "source": [
        "Special events for USA<br>\n",
        "  - [List of Floods Events](https://en.wikipedia.org/wiki/Floods_in_the_United_States_(2000%E2%80%93present))<br>\n",
        "  - [List of Hurricane Landfalling Events](https://en.wikipedia.org/wiki/List_of_United_States_hurricanes)<br>\n",
        "    - 2016-08-21: Southern Louisiana tropical disturbance\n",
        "    - 2017-08-29: Hurricane Hervey (Texas)\n",
        "    - 2018-09-17: Hurricane Florence (North Calorina)\n",
        "    - 2021-08-20: Hurricane Ida (New Jersey - New York)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw9uw5YF6HDl"
      },
      "source": [
        "## Draw Composite of Anomalous Days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAsj6CDvZsf1"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6IR_g1BZsf1"
      },
      "outputs": [],
      "source": [
        "np.shape(original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.788147Z",
          "start_time": "2024-07-16T13:19:32.788140Z"
        },
        "id": "IhgHRDMg6HDl"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list[idxs[:],:,:,0]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "image2 =temp_img_array_list[idxs[:]]\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded[:,:,:,0],domain,grid,og=False,multidata=True)\n",
        "recon = recon*(maxvalue-minvalue)+minvalue\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs[:]))/float(len(dates[:]))*100.0\n",
        "ddays = len(idxs[:])\n",
        "\n",
        "plt.title(\"%.1f%%, TX Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5HhV1KpZsf1"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list[idxs[:],:,:,1]*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "\n",
        "image2 =temp_img_array_list[idxs[:]]\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded[:,:,:,1],domain,grid,og=False,multidata=True)\n",
        "recon = recon*(maxvalueRR-minvalueRR)+minvalueRR\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs[:]))/float(len(dates[:]))*100.0\n",
        "ddays = len(idxs[:])\n",
        "\n",
        "plt.title(\"%.1f%%, RR Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "elem2= \"PRECIP\"\n",
        "art.set_label('%s [%s]' % (elem2,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rRFkLJcZsf1"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.788745Z",
          "start_time": "2024-07-16T13:19:32.788738Z"
        },
        "id": "5eNWIGTdZsf1"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "ees = idxs_model[0,:]\n",
        "tts = idxs_model[1,:]\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list2[ees,tts,:,:,0]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "\n",
        "image2 =temp_img_array_list2[ees,tts]\n",
        "\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded[:,:,:,0],domain,grid,multidata=True)\n",
        "recon = recon*(maxvalue-minvalue)+minvalue\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=\"seismic\", extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs_model[1,:]))/float(len(dates2[:]))/float(emax)*100.0\n",
        "ddays = len(idxs_model[1,:])\n",
        "\n",
        "plt.title(\"%.1f%%, Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly_model\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HSnNLb1Zsf1"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "ees = idxs_model[0,:]\n",
        "tts = idxs_model[1,:]\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list2[ees,tts,:,:,1]*(maxvalue2-minvalue2)+minvalue2\n",
        "\n",
        "\n",
        "image2 =temp_img_array_list2[ees,tts]\n",
        "\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded[:,:,:,1],domain,grid,multidata=True)\n",
        "recon = recon*(maxvalue2-minvalue2)+minvalue2\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=\"seismic\", extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs_model[1,:]))/float(len(dates2[:]))/float(emax)*100.0\n",
        "ddays = len(idxs_model[1,:])\n",
        "\n",
        "plt.title(\"%.1f%%, Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly_model\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJI37STy6HDl"
      },
      "source": [
        "## Time Series of observed anomalous days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.789361Z",
          "start_time": "2024-07-16T13:19:32.789352Z"
        },
        "id": "K1IXzzS36HDl"
      },
      "outputs": [],
      "source": [
        "# Time series for anomalous days\n",
        "adates = dates[idxs]\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# derive (years,frequency of anomaly)\n",
        "alldata = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "alldata.to_json(\"%s/timeseries.json\" % (datatype))\n",
        "\n",
        "# Plot\n",
        "years = alldata.index\n",
        "alldata = alldata.to_numpy()\n",
        "plt.plot(years, alldata, \"o-\", color=\"r\")\n",
        "\n",
        "h, pval = mk_test(alldata, alpha=0.05)\n",
        "print (h,pval)\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,alldata)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days (Pvalue=%.2f)\" % (pval))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries.png\" % (datatype))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t4q5eM0Zsf1"
      },
      "outputs": [],
      "source": [
        "obs =np.array(aa)\n",
        "obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.789914Z",
          "start_time": "2024-07-16T13:19:32.789907Z"
        },
        "id": "Grf8B4OCZsf1"
      },
      "outputs": [],
      "source": [
        "# Observed Monthly plot\n",
        "\n",
        "adates = dates[idxs]\n",
        "\n",
        "#print (adates.dt.year.value_counts())\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "ax1 = plt.subplot(2,1,1)\n",
        "\n",
        "# derive (years,frequency of anomaly)\n",
        "alldata = adates.dt.month.value_counts().reindex(dates.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "years_ = sorted(dates.dt.year.unique())\n",
        "\n",
        "# Plot\n",
        "months_ = alldata.index\n",
        "alldata = alldata.to_numpy()/float(years_[-1]-years_[0]+1)\n",
        "plt.plot(months_, alldata, \"o-\", color=\"r\")\n",
        "\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Trend\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "\n",
        "counts_ = adates.groupby([adates.dt.year, adates.dt.month]).agg({'count'})\n",
        "dates_ = counts_.index\n",
        "vals_ = counts_.values.squeeze()\n",
        "\n",
        "data_ = np.ma.zeros((len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros(len(months_))\n",
        "pval_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for date_, count_ in zip(dates_,vals_):\n",
        "    year_ = date_[0]\n",
        "    month_ = date_[1]\n",
        "    tt = year_ - years_[0]\n",
        "    im = month_ - months_[0]\n",
        "    data_[tt,im]=count_\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,im]\n",
        "    xx_,yy_,trend_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "plt.plot(months_, trend_, \"-\", color=\"red\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_[im] <= 0.05:\n",
        "        plt.plot(months_[im], trend_[im], \"o\", color=\"red\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months_[im], trend_[im], \"o\", color=\"black\", ms=12)\n",
        "plt.axhline(y=0, ls=\"--\", color=\"black\")\n",
        "\n",
        "plt.title(\"Trend in Abnormal Days for Each Month [days per year]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxnZzLwCZsf1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvOEmAVfZsf1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.790496Z",
          "start_time": "2024-07-16T13:19:32.790489Z"
        },
        "id": "Io11x9vKZsf1"
      },
      "outputs": [],
      "source": [
        "# Model Time Series\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "#for ee in range(30):\n",
        "#for ee in range(30,61,1):\n",
        "  idx9 = np.where(idxs_model[0,:]==ee)\n",
        "  idx9 = np.array(idx9).flatten()\n",
        "  adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "  temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "  years = temp.index\n",
        "\n",
        "  temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], temps[:,:-1].min(axis=0), temps[:,:-1].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:-1], temps[:,:-1].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "else:\n",
        "    plt.fill_between(years[:], temps[:,:].min(axis=0), temps[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:], temps[:,:].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "# Save timeseries\n",
        "df_ = pd.DataFrame(data=np.swapaxes(temps,0,1), index=years, columns=np.arange(1,emax+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_model.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "h, pval = mk_test(temps[:,:].mean(axis=0), alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,temps[:,:].mean(axis=0))\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rhgpjhUZsf1"
      },
      "outputs": [],
      "source": [
        "# Model Time Series\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "#for ee in range(30):\n",
        "#for ee in range(30,61,1):\n",
        "  idx9 = np.where(idxs_model[0,:]==ee)\n",
        "  idx9 = np.array(idx9).flatten()\n",
        "  adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "  temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "  years = temp.index\n",
        "\n",
        "  temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], temps[:,:-1].min(axis=0), temps[:,:-1].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:-1], temps[:,:-1].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "else:\n",
        "    plt.fill_between(years[:], temps[:,:].min(axis=0), temps[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:], temps[:,:].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "# Save timeseries\n",
        "df_ = pd.DataFrame(data=np.swapaxes(temps,0,1), index=years, columns=np.arange(1,emax+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_model.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "h, pval = mk_test(temps[:,:].mean(axis=0), alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,temps[:,:].mean(axis=0))\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL6gNCnrZsf1"
      },
      "outputs": [],
      "source": [
        "trends_model = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,temps[e])\n",
        "    trends_model.append(aa)\n",
        "trends_model =np.array(trends_model)\n",
        "trends_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.791097Z",
          "start_time": "2024-07-16T13:19:32.791090Z"
        },
        "id": "DE6VQwp6Zsf1"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "    idx9 = np.where(idxs_model[0,:]==ee)\n",
        "    idx9 = np.array(idx9).flatten()\n",
        "    adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "    temp = adates.dt.month.value_counts().reindex(dates2.dt.month.unique(),fill_value=0).sort_index()\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "\n",
        "years_ = sorted(dates2.dt.year.unique())\n",
        "months_ = sorted(dates2.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax,len(months_)))\n",
        "pval_ = np.ma.zeros((emax,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax):\n",
        "    idx9 = np.where(idxs_model[0,:]==ee)\n",
        "    idx9 = np.array(idx9).flatten()\n",
        "    adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "    counts_ = adates.groupby([adates.dt.year, adates.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = year_ - years_[0]\n",
        "        im = month_ - months_[0]\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[im], trend_ensmean_[im], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[im], trend_ensmean_[im], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5CkJJW6HDm",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Regional Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.791620Z",
          "start_time": "2024-07-16T13:19:32.791613Z"
        },
        "id": "lN_9b3RlZsf1"
      },
      "outputs": [],
      "source": [
        "max_regs = np.zeros(len(idxs))\n",
        "\n",
        "for ii,i in enumerate(idxs):\n",
        "\n",
        "    original = temp_img_array_list[i]*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "   #max_regs[ii] = regional_stats(jreg_uids, jreg, original2, kind=\"mean\")\n",
        "    max_regs[ii] = regional_stats(jreg_uids, jreg, original2, kind=\"max\")\n",
        "   #print (ii,i,max_reg1[ii])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKtUQigZsf1"
      },
      "source": [
        "### Composite (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.792136Z",
          "start_time": "2024-07-16T13:19:32.792128Z"
        },
        "id": "SgAF8eV76HDm"
      },
      "outputs": [],
      "source": [
        "nregs = len(jreg_uids)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "fig, axs = plt.subplots(nrows=math.ceil(nregs/2), ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(20,15))\n",
        "pcol=0\n",
        "prow=0\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    idx = np.where(max_regs==cl)\n",
        "\n",
        "    original = temp_img_array_list[idxs[idx]]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 =temp_img_array_list[idxs[idx]]\n",
        "    decoded = autoencoder.predict(image2)\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].coastlines(resolution=\"50m\")\n",
        "    axs[prow,pcol].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[prow,pcol].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[prow,pcol].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[prow,pcol].contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "\n",
        "    frac = float(len(idxs[idx]))/float(len(idxs[:]))*100.0\n",
        "\n",
        "    ddays = len(idxs[idx])\n",
        "    axs[prow,pcol].set_title(\"%s (%.1f%%, Total %i days)\" % (sregnames[cl], frac, ddays), fontsize=20)\n",
        "\n",
        "    pcol=pcol+1\n",
        "    if pcol==2:\n",
        "        pcol=0\n",
        "        prow=prow+1\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.0, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_cluster_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHhDRp7Zsf1"
      },
      "source": [
        "### Time Series (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.792722Z",
          "start_time": "2024-07-16T13:19:32.792715Z"
        },
        "id": "9XdYbaDG6HDn"
      },
      "outputs": [],
      "source": [
        "#print (adates.dt.year.value_counts()) Obs\n",
        "\n",
        "plt.figure(0, figsize=(12,10))\n",
        "\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    plt.subplot(math.ceil(nregs/2), 2, cl)\n",
        "    idx2 = np.where(max_regs==cl)\n",
        "\n",
        "    adates = dates[idxs[idx2]]\n",
        "\n",
        "    temp = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "    data = np.array(temp.tolist())\n",
        "\n",
        "    plt.plot(years, data, \"o-\", color=\"red\", label=\"Cluster%i\" % (cl+1))\n",
        "\n",
        "   # Mann and Kendall singnificant test for trend\n",
        "    h, pval = mk_test(data, alpha=0.05)\n",
        "\n",
        "    if pval <=0.05:\n",
        "        xx, yy, aa = linreg(years,data)\n",
        "        plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "        plt.title(\"%s (Trend=%.5f, Trend P-Val=%.2f)\" % (sregnames[cl],aa, pval))\n",
        "    else:\n",
        "        plt.title(\"%s (Trend P-Val=%.2f)\" % (sregnames[cl], pval))\n",
        "\n",
        "outdir = \"%s/anomaly/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"%s/timeseries_regional%i_each.png\" % (outdir,nregs))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi5KhSDVZsf2"
      },
      "source": [
        "### Composite (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.793305Z",
          "start_time": "2024-07-16T13:19:32.793298Z"
        },
        "id": "_BOBQC40Zsf2"
      },
      "outputs": [],
      "source": [
        "pmax, kmax = np.shape(idxs_model)\n",
        "max_regs_model = np.zeros(kmax)\n",
        "\n",
        "for i in range(kmax):\n",
        "\n",
        "    ee = idxs_model[0,i]\n",
        "    tt = idxs_model[1,i]\n",
        "\n",
        "    original = temp_img_array_list2[ee,tt]*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "   #max_regs_model[i] = regional_stats(jreg_uids, jreg, original2, kind=\"mean\")\n",
        "    max_regs_model[i] = regional_stats(jreg_uids, jreg, original2, kind=\"max\")\n",
        "    if i%1000==0:\n",
        "        print (\"%i/%i max=%i\" % (i+1,kmax,max_regs_model[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.794173Z",
          "start_time": "2024-07-16T13:19:32.794166Z"
        },
        "id": "kY2KHQ8YZsf2"
      },
      "outputs": [],
      "source": [
        "# Show composite of each cluster pattern (Model)\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "#draw_tc=True\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "fig, axs = plt.subplots(nrows=math.ceil(nregs/2), ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(20,15))\n",
        "\n",
        "pcol=0\n",
        "prow=0\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    print (\"plotting region\",cl)\n",
        "\n",
        "    idx = np.where(max_regs_model==cl)\n",
        "\n",
        "    idx = np.array(idx).flatten()\n",
        "\n",
        "    if len(idx)==0:\n",
        "        continue\n",
        "\n",
        "    ees = idxs_model[0,idx]\n",
        "    tts = idxs_model[1,idx]\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ees,tts]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "#    temp_img_array_list = np.pad(temp_img_array_list, pad_width=((0,0),(1,2),(1,1)), mode='constant',constant_values=0)\n",
        "\n",
        "    image2 =temp_img_array_list2[ees,tts]\n",
        "    #image2 = image2[0,1:-2,1:-1,0] # pad -> original\n",
        "    decoded = autoencoder2.predict(image2, verbose=0)\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].coastlines(resolution=\"50m\")\n",
        "    axs[prow,pcol].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[prow,pcol].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[prow,pcol].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[prow,pcol].contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "\n",
        "    #m.readshapefile('st99_d00', name='states', drawbounds=True)\n",
        "    frac = float(len(idx))/float(len(max_regs_model[:]))*100.0\n",
        "    ddays = len(idx)\n",
        "\n",
        "    ddays = ddays/float(emax) # normalized by emax\n",
        "    axs[prow,pcol].set_title(\"%s (%.1f%%, Total %i days)\" % (sregnames[cl], frac, ddays), fontsize=20)\n",
        "\n",
        "    pcol=pcol+1\n",
        "    if pcol==2:\n",
        "        pcol=0\n",
        "        prow=prow+1\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.0, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"./%s/anomaly_model/regional%2.2i\" % (datatype, nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_regional_composite.png\" % (outdir))\n",
        "print (datatype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNeVN49xZsf2"
      },
      "source": [
        "### Time Series (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.794661Z",
          "start_time": "2024-07-16T13:19:32.794654Z"
        },
        "id": "JenJPXmuZsf2"
      },
      "outputs": [],
      "source": [
        "#print (adates.dt.year.value_counts()) Obs\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "plt.figure(0, figsize=(10,8))\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    plt.subplot(math.ceil(nregs/2), 2, cl)\n",
        "\n",
        "    cl2=cl\n",
        "    idx2a = np.where(max_regs_model==cl)\n",
        "    idx2b = idxs_model[1,idx2a].squeeze()\n",
        "    adates = dates2[idx2b]\n",
        "\n",
        "    temp = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "    if season==\"DJF\":\n",
        "        years = temp.index[:-1]\n",
        "        data = np.array(temp.tolist()[:-1])/float(emax)\n",
        "    else:\n",
        "        years = temp.index\n",
        "        data = np.array(temp.tolist())/float(emax)\n",
        "\n",
        "    allens=[]\n",
        "    for ee in range(emax):\n",
        "        idx2a = np.where((max_regs_model==cl)&(idxs_model[0,:]==ee))\n",
        "        idx2b = idxs_model[1,idx2a].squeeze()\n",
        "        adates = dates2[idx2b]\n",
        "\n",
        "        try:\n",
        "            temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "            years = temp.index\n",
        "            allens.append(temp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    allens=np.array(allens)\n",
        "    allens2_list = []\n",
        "\n",
        "    for ee in range(emax):\n",
        "        temp1_ = pd.DataFrame({'year':years,\"ens\":ee+1,\"cl\":cl, \"freq\":allens[ee,:]})\n",
        "        allens2_list.append(temp1_)\n",
        "    allens2 = pd.concat(allens2_list, ignore_index=True, sort=False)\n",
        "\n",
        "    if season==\"DJF\":\n",
        "        plt.fill_between(years[:-1], allens[:,:-1].min(axis=0), allens[:,:-1].max(axis=0), color=\"red\", alpha=0.4)\n",
        "    else:\n",
        "        plt.fill_between(years[:], allens[:,:].min(axis=0), allens[:,:].max(axis=0), color=\"red\", alpha=0.4)\n",
        "\n",
        "    plt.plot(years, data, \"o-\", color=\"red\", label=\"%s\" % (sregnames[cl]))\n",
        "\n",
        "   # Mann and Kendall singnificant test for trend\n",
        "    h, pval = mk_test(data, alpha=0.05)\n",
        "\n",
        "    if pval <=0.05:\n",
        "        xx, yy, aa = linreg(years,data)\n",
        "        plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "        plt.title(\"%s (Trend=%.5f, Trend P-Val=%.2f)\" % (sregnames[cl],aa, pval))\n",
        "    else:\n",
        "        plt.title(\"%s (Trend P-Val=%.2f)\" % (sregnames[cl], pval))\n",
        "\n",
        "outdir = \"./%s/anomaly_model/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"%s/timeseries_regional%i_each.png\" % (outdir,nregs))\n",
        "print (datatype)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Save file\n",
        "#alldata2.to_json(\"%s/timeseries_cluster%2.2i_each.json\" % (outdir,ncluster))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na0aeXFJZsf3"
      },
      "source": [
        "# Test (Future) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.805236Z",
          "start_time": "2024-07-16T13:19:32.805230Z"
        },
        "scrolled": true,
        "id": "39jt5qD1Zsf3"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list3)\n",
        "errors3_ = comp_error(temp_img_array_list3.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_test = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.805797Z",
          "start_time": "2024-07-16T13:19:32.805790Z"
        },
        "id": "HPIGxG6hZsf3"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_test/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.806338Z",
          "start_time": "2024-07-16T13:19:32.806331Z"
        },
        "id": "r6laX25VZsf3"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_test = pickle_load(\"%s/output_test/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.806914Z",
          "start_time": "2024-07-16T13:19:32.806907Z"
        },
        "id": "K0cwSmvcZsf3"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh2 = np.quantile(errors2.flatten(), 0.95)\n",
        "idxs_test = np.where(np.array(errors_test) >= thresh2)\n",
        "idxs_test = np.array(idxs_test)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_test[0,:])))\n",
        "thresh2, np.shape(idxs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.807472Z",
          "start_time": "2024-07-16T13:19:32.807465Z"
        },
        "id": "XHqL69MSZsf3"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list3[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.808133Z",
          "start_time": "2024-07-16T13:19:32.808126Z"
        },
        "id": "VC1y_AzxZsf3"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "kbclRU-QZsf3"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.808663Z",
          "start_time": "2024-07-16T13:19:32.808656Z"
        },
        "id": "oemHZVDfZsf3"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "U0VFOHWlZsf3"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.809247Z",
          "start_time": "2024-07-16T13:19:32.809240Z"
        },
        "id": "chp6d-3oZsf4"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfUH_N-eZsf4"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.809718Z",
          "start_time": "2024-07-16T13:19:32.809711Z"
        },
        "id": "c_KiN1PAZsf4"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "print(temp.sum())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_q7kibHZsf4"
      },
      "outputs": [],
      "source": [
        "trends_future = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,alldata2[e])\n",
        "    trends_future.append(aa)\n",
        "trends_future =np.array(trends_future)\n",
        "trends_future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "SOMr_W52Zsf4"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.810605Z",
          "start_time": "2024-07-16T13:19:32.810598Z"
        },
        "id": "U_WIi2XyZsf4"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUU7py3kZsf4"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.811095Z",
          "start_time": "2024-07-16T13:19:32.811088Z"
        },
        "id": "bYWdvtizZsf4"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6CGeUmeNZsf4"
      },
      "source": [
        "# Test (Future) Regional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.812075Z",
          "start_time": "2024-07-16T13:19:32.812067Z"
        },
        "id": "T5GSPjtlZsf4"
      },
      "outputs": [],
      "source": [
        "pmax, kmax = np.shape(idxs_test)\n",
        "\n",
        "pmax, kmax = np.shape(idxs_test)\n",
        "max_regs_test = np.zeros(kmax)\n",
        "\n",
        "for i in range(kmax):\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    tt = idxs_test[1,i]\n",
        "\n",
        "    original = temp_img_array_list3[ee,tt]*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "   #max_regs_model[i] = regional_stats(jreg_uids, jreg, original2, kind=\"mean\")\n",
        "    max_regs_test[i] = regional_stats(jreg_uids, jreg, original2, kind=\"max\")\n",
        "    if i%1000==0:\n",
        "        print (\"%i/%i max=%i\" % (i+1,kmax,max_regs_test[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayGZ1EXnZsf4"
      },
      "source": [
        "### Composite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.812779Z",
          "start_time": "2024-07-16T13:19:32.812771Z"
        },
        "id": "3llgvQ-EZsf4"
      },
      "outputs": [],
      "source": [
        "# Show composite of each cluster pattern (Model)\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "#draw_tc=True\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "fig, axs = plt.subplots(nrows=math.ceil(nregs/2), ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(20,15))\n",
        "\n",
        "pcol=0\n",
        "prow=0\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    print (\"plotting region\",cl)\n",
        "\n",
        "    idx = np.where(max_regs_test==cl)\n",
        "\n",
        "    idx = np.array(idx).flatten()\n",
        "\n",
        "    if len(idx)==0:\n",
        "        continue\n",
        "\n",
        "    ees = idxs_test[0,idx]\n",
        "    tts = idxs_test[1,idx]\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ees,tts]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "#    temp_img_array_list = np.pad(temp_img_array_list, pad_width=((0,0),(1,2),(1,1)), mode='constant',constant_values=0)\n",
        "\n",
        "    image3 =temp_img_array_list3[ees,tts]\n",
        "    #image2 = image2[0,1:-2,1:-1,0] # pad -> original\n",
        "    decoded = autoencoder2.predict(image3, verbose=0)\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].coastlines(resolution=\"50m\")\n",
        "    axs[prow,pcol].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[prow,pcol].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[prow,pcol].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[prow,pcol].contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "\n",
        "    #m.readshapefile('st99_d00', name='states', drawbounds=True)\n",
        "    frac = float(len(idx))/float(len(max_regs_test[:]))*100.0\n",
        "    ddays = len(idx)\n",
        "\n",
        "    ddays = ddays/float(emax) # normalized by emax\n",
        "    axs[prow,pcol].set_title(\"%s (%.1f%%, Total %i days)\" % (sregnames[cl], frac, ddays), fontsize=20)\n",
        "\n",
        "    pcol=pcol+1\n",
        "    if pcol==2:\n",
        "        pcol=0\n",
        "        prow=prow+1\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.0, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"./%s/anomaly_test/regional%2.2i\" % (datatype, nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_regional_composite.png\" % (outdir))\n",
        "print (datatype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfbmg-AKZsf4"
      },
      "source": [
        "### Time Series (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.813402Z",
          "start_time": "2024-07-16T13:19:32.813395Z"
        },
        "id": "jN_r1jwvZsf4"
      },
      "outputs": [],
      "source": [
        "#print (adates.dt.year.value_counts()) Obs\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "plt.figure(0, figsize=(10,8))\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    plt.subplot(math.ceil(nregs/2), 2, cl)\n",
        "\n",
        "    cl2=cl\n",
        "    idx2a = np.where(max_regs_test==cl)\n",
        "    idx2b = idxs_test[1,idx2a].squeeze()\n",
        "    adates = dates3[idx2b]\n",
        "\n",
        "    temp = adates.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    if season==\"DJF\":\n",
        "        years = temp.index[:-1]\n",
        "        data = np.array(temp.tolist()[:-1])/float(emax)\n",
        "    else:\n",
        "        years = temp.index\n",
        "        data = np.array(temp.tolist())/float(emax)\n",
        "\n",
        "    allens=[]\n",
        "    for ee in range(emax):\n",
        "        idx2a = np.where((max_regs_test==cl)&(idxs_test[0,:]==ee))\n",
        "        idx2b = idxs_test[1,idx2a].squeeze()\n",
        "        adates = dates3[idx2b]\n",
        "\n",
        "        try:\n",
        "            temp = adates.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "            years = temp.index\n",
        "            allens.append(temp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    allens=np.array(allens)\n",
        "    allens2_list = []\n",
        "\n",
        "    for ee in range(emax):\n",
        "        temp1_ = pd.DataFrame({'year':years,\"ens\":ee+1,\"cl\":cl, \"freq\":allens[ee,:]})\n",
        "        allens2_list.append(temp1_)\n",
        "    allens2 = pd.concat(allens2_list, ignore_index=True, sort=False)\n",
        "\n",
        "    if season==\"DJF\":\n",
        "        plt.fill_between(years[:-1], allens[:,:-1].min(axis=0), allens[:,:-1].max(axis=0), color=\"red\", alpha=0.4)\n",
        "    else:\n",
        "        plt.fill_between(years[:], allens[:,:].min(axis=0), allens[:,:].max(axis=0), color=\"red\", alpha=0.4)\n",
        "\n",
        "    plt.plot(years, data, \"o-\", color=\"red\", label=\"%s\" % (sregnames[cl]))\n",
        "\n",
        "   # Mann and Kendall singnificant test for trend\n",
        "    h, pval = mk_test(data, alpha=0.05)\n",
        "\n",
        "    if pval <=0.05:\n",
        "        xx, yy, aa = linreg(years,data)\n",
        "        plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "        plt.title(\"%s (Trend=%.5f, Trend P-Val=%.2f)\" % (sregnames[cl],aa, pval))\n",
        "    else:\n",
        "        plt.title(\"%s (Trend P-Val=%.2f)\" % (sregnames[cl], pval))\n",
        "\n",
        "outdir = \"./%s/anomaly_test/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"%s/timeseries_regional%i_each.png\" % (outdir,nregs))\n",
        "print (datatype)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Save file\n",
        "#alldata2.to_json(\"%s/timeseries_cluster%2.2i_each.json\" % (outdir,ncluster))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwf5hyUuZsf5"
      },
      "source": [
        "# Test (Future) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.805236Z",
          "start_time": "2024-07-16T13:19:32.805230Z"
        },
        "scrolled": true,
        "id": "4tjlm5W3Zsf5"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list4)\n",
        "errors4_ = comp_error(temp_img_array_list4.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_nat = errors4_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk_Zpkf7Zsf5"
      },
      "outputs": [],
      "source": [
        "np.shape(errors_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbz7nOWLZsf5"
      },
      "outputs": [],
      "source": [
        "np.shape(temp_img_array_list3.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.805797Z",
          "start_time": "2024-07-16T13:19:32.805790Z"
        },
        "id": "UCLhNcHcZsf5"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_nat/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.806338Z",
          "start_time": "2024-07-16T13:19:32.806331Z"
        },
        "id": "w5leUbr3Zsf5"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_test = pickle_load(\"%s/output_nat/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.806914Z",
          "start_time": "2024-07-16T13:19:32.806907Z"
        },
        "id": "CGlMM7UtZsf5"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh2 = np.quantile(errors_nat.flatten(), 0.95)\n",
        "idxs_test = np.where(np.array(errors_nat) >= thresh2)\n",
        "idxs_test = np.array(idxs_test)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_test[0,:])))\n",
        "thresh2, np.shape(idxs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.807472Z",
          "start_time": "2024-07-16T13:19:32.807465Z"
        },
        "id": "hb1P4D1VZsf5"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list3[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.808133Z",
          "start_time": "2024-07-16T13:19:32.808126Z"
        },
        "id": "5VKpgyTVZsf5"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "j_4UPIWSZsf5"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.808663Z",
          "start_time": "2024-07-16T13:19:32.808656Z"
        },
        "id": "IG4EhBqnZsf5"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "sS_deL09Zsf5"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.809247Z",
          "start_time": "2024-07-16T13:19:32.809240Z"
        },
        "id": "DAR9-RRIZsf5"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxw1juhiZsf5"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-16T13:19:32.809718Z",
          "start_time": "2024-07-16T13:19:32.809711Z"
        },
        "id": "6RDBi-9IZsf5"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS9hJeXvZsf5"
      },
      "outputs": [],
      "source": [
        "trends_nat = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,alldata2[e])\n",
        "    trends_nat.append(aa)\n",
        "trends_nat =np.array(trends_nat)\n",
        "trends_nat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ect0opowZsf5"
      },
      "source": [
        "# Gaussian Distribution and Box Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLpAgLJoZsf5"
      },
      "outputs": [],
      "source": [
        "data= [obs,trends_model,trends_future,trends_nat]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-wgit_-Zsf5"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.yticks([])\n",
        "plt.xticks([])\n",
        "\n",
        "SMALL_SIZE = 20\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 10\n",
        "\n",
        "labels=[\"Observations(1971-2023)\",\"AllForc(1971-2023)\",\"AllForc(1971-2100)\",\"NatForc(1971-2100)\"]\n",
        "\n",
        "plt.rc('font', size=15)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=15)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=10)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
        " # legend fontsize\n",
        "plt.title('Linear Trend in Anomalous Days')\n",
        "data= [obs,trends_model,trends_future,trends_nat]\n",
        "ax = fig.add_subplot()\n",
        "bp = ax.boxplot(data,vert=False,labels=labels,patch_artist=True, medianprops=dict(color=\"black\", alpha=0.7))\n",
        "ax.axvline(x = 0, color = 'black',linestyle='dashed')\n",
        "ax.set_xlabel('Days per year')\n",
        "\n",
        "\n",
        "colors = [\"mistyrose\",\"mistyrose\",\"mistyrose\",\"lightgray\"]\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "for i in range(1,4,1):\n",
        "    ax.plot(data[i],np.full((30),i+1),'o',markersize=3,color='black')\n",
        "for i in range(4):\n",
        "    ax.plot(np.average(data[i]),np.average(bp['medians'][i].get_ydata()),color='red', marker='s')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6_s4qJMZsf5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import statistics\n",
        "\n",
        "\n",
        "# Fit a normal distribution to the data:\n",
        "data= trends_model\n",
        "mu, std = norm.fit(data)\n",
        "\n",
        "# Plot the histogram.\n",
        "plt.hist(data, bins=10, density=True, alpha=0.3, color='red')\n",
        "\n",
        "# Plot the PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mu, std)\n",
        "plt.plot(x, p, 'k', linewidth=2,color='r',label = \"AllForc\")\n",
        "\n",
        "data= trends_nat\n",
        "mu, std = norm.fit(data)\n",
        "\n",
        "# Plot the histogram.\n",
        "plt.hist(data, bins=10, density=True, alpha=0.3, color='blue')\n",
        "\n",
        "# Plot the PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mu, std)\n",
        "plt.plot(x, p, 'k', linewidth=2,color='b',label = \"NatForc\")\n",
        "\n",
        "\n",
        "plt.axvline(x = obs, color = 'black', label = 'Observed')\n",
        "plt.title(\"Linear Trend in Anomalous Days\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kUNaHVmu6HDY",
        "QYJ9En9Q6HDZ",
        "fB-TzNda6HDa"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "313.188px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}