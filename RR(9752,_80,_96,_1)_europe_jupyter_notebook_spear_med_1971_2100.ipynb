{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B82ZCJsL6HDE"
      },
      "source": [
        "# Anomaly Detection using Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP0CMbSa6HDJ"
      },
      "source": [
        "Date: 07.06.2025<br>\n",
        "Author: Hiroyuki Murakami (<Hiroyuki.Murakami@noaa.gov>), Sophia Zhou (sz3962@princeton.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rODL4R1z6HDK"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9bsWyo36HDL"
      },
      "source": [
        "This project is for introducing the anomaly detection method using deep learning, so called \"autoencoder\", for daily precipitation. <br>\n",
        "\n",
        "Before reading this jupyter notebook, following web sites should be read to understand the basic consept of anomaly detection and autoencoder.<br>\n",
        " - [Intro to anomaly detection with OpenCV, Computer Vision, and scikit-learn](https://pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/)\n",
        " - [Anomaly detection with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/)\n",
        " - [Autoencoders with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/)\n",
        "\n",
        "Please also read following paper for the details of methodology intoduced in this jupyter notebook.<br>\n",
        " - Murakami, H., T. L. Delworth, W. F. Cooke, S. B. Kapnick, and P. -C. Hsu, 2022: Increasing frequency of anomalous precipitation events in Japan detected by a deep learning autoencoder. Earthâ€™s Future, 10, e2021EF002481. [Link](http://dx.doi.org/10.1029/2021EF002481)\n",
        "\n",
        "The methodology and codes were based on the codes in [Autoencoders with Keras, TensorFlow, and Deep Learning](https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKkeq_Uu6HDM"
      },
      "source": [
        "## Required Installed Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_9axWai6HDN"
      },
      "source": [
        "Following Python packages are required for this Jupyter-Notebook. <br>\n",
        "  > tensorflow, scikit-learn, pillow, h5py, keras, netCDF4, pandas, matplotlib, cartopy, numpy, pickle, jupyter, jupyter_contrib_nbextensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:45:47.863657Z",
          "start_time": "2024-07-17T10:45:47.861700Z"
        },
        "id": "05sLSO8Y6vAx"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:45:47.870395Z",
          "start_time": "2024-07-17T10:45:47.864662Z"
        },
        "id": "PdEK8wcG6MCZ"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#sys.path.append('/content/drive/MyDrive/Autoencoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:45:47.950888Z",
          "start_time": "2024-07-17T10:45:47.871177Z"
        },
        "id": "NELVxlEmSg_Z"
      },
      "outputs": [],
      "source": [
        "#!yes | pip uninstall shapely\n",
        "#!pip install shapely --no-binary shapely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:45:47.963387Z",
          "start_time": "2024-07-17T10:45:47.951666Z"
        },
        "id": "YOE2LAr46sKi"
      },
      "outputs": [],
      "source": [
        "#!pip install cartopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:45:47.973382Z",
          "start_time": "2024-07-17T10:45:47.964418Z"
        },
        "id": "3md8lSe2PAVl"
      },
      "outputs": [],
      "source": [
        "#!pip install netcdf4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.475510Z",
          "start_time": "2024-07-17T10:45:47.974123Z"
        },
        "id": "4gOS1acW6HDO"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LatitudeFormatter,LongitudeFormatter\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "from netCDF4 import num2date, Dataset\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from pyimagesearch.convautoencoder import ConvAutoencoder\n",
        "\n",
        "from mylib.save_load_file import pickle_dump, pickle_load\n",
        "from mylib.mystat import MYSTAT\n",
        "\n",
        "#from cdo import Cdo\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.479690Z",
          "start_time": "2024-07-17T10:47:28.477464Z"
        },
        "id": "Shc8F-ee6HDQ"
      },
      "outputs": [],
      "source": [
        "mystat = MYSTAT() # my own statistical package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBZXs1n6HDR"
      },
      "source": [
        "If you don't see any errr messages, all of the Python packages have been normaly installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EtvB4M26HDR"
      },
      "source": [
        "# Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.491079Z",
          "start_time": "2024-07-17T10:47:28.480560Z"
        },
        "id": "rA53yZVN6HDS"
      },
      "outputs": [],
      "source": [
        "#--domain\n",
        "domain=\"EU\"\n",
        "#domain=\"JP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGN65aFb6HDS"
      },
      "source": [
        "Domain of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.506278Z",
          "start_time": "2024-07-17T10:47:28.491904Z"
        },
        "id": "aiwhueSY6HDT"
      },
      "outputs": [],
      "source": [
        "#--Season\n",
        "if domain==\"JP\":\n",
        "    model=\"APHRODITE\" # http://aphrodite.st.hirosaki-u.ac.jp/\n",
        "    tsyear=1977\n",
        "    teyear=2015\n",
        "    season=\"MJJASO\" # May-Octber\n",
        "elif domain==\"USA\":\n",
        "    model=\"PRISMG\" # https://prism.oregonstate.edu/\n",
        "    tsyear=1981\n",
        "    teyear=2021\n",
        "    season=\"JAS\" # July-September\n",
        "elif domain==\"EU\":\n",
        "  model = \"E-OBS\"  # https://cds.climate.copernicus.eu/cdsapp#!/dataset/insitu-gridded-observations-europe?tab=overview\n",
        "  #tsyear = 1950\n",
        "  tsyear = 1971\n",
        "  #tsyear = 1981\n",
        "  teyear = 2023\n",
        "  season = \"MJJASO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.521383Z",
          "start_time": "2024-07-17T10:47:28.507222Z"
        },
        "id": "5KsPBJpKq-lo"
      },
      "outputs": [],
      "source": [
        "#--Test Model\n",
        "testmodel=\"SPEAR-MED-ALLSSP585\"\n",
        "#testmodel=\"SPEAR-MED-NATURAL\"\n",
        "#testmodel=\"SPEAR-MED-COMB-ALLSSP585\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.536013Z",
          "start_time": "2024-07-17T10:47:28.522394Z"
        },
        "id": "WmZB22u6q-lo"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\":\n",
        "    emax=30 # ensembe member for tested data\n",
        "    emax2=emax # emax for test\n",
        "    testmodel2=testmodel.split(\"-\")[-1]\n",
        "#    tsyear2=1971\n",
        "    tsyear2=1971\n",
        "    teyear2=2100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKSUeZHH6HDT"
      },
      "source": [
        "If USA is chosen, the [PRISM](https://prism.oregonstate.edu/) precipitation data is used and the analysis period is 1981-2021 for July-September.<br>\n",
        "If JP is chosen, the [APHRODITE](http://aphrodite.st.hirosaki-u.ac.jp/) precipitation data is used and the analysis period is 1977-2015 for May-October.<br>\n",
        "If EU is chosen, the [E-OBS](https://cds.climate.copernicus.eu/datasets/insitu-gridded-observations-europe?tab=overview) precipitation data is used and the analysis period is 1977-2023 for May-October.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.558723Z",
          "start_time": "2024-07-17T10:47:28.536844Z"
        },
        "id": "6K8ZyTqi6HDT"
      },
      "outputs": [],
      "source": [
        "#--Grid type\n",
        "if domain==\"JP\":\n",
        "    grid=\"HGRID\" # 25-km grid\n",
        "elif domain==\"USA\":\n",
        "    grid=\"SGRID\" # 50-km grid\n",
        "elif domain==\"EU\":\n",
        "   #grid=\"OGRID\" # 50-km grid not sure if this is right\n",
        "    grid=\"SGRID\" # 50-km grid not sure if this is right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPo2dINH6HDU"
      },
      "source": [
        "Precipitation GPV is on the SGRID (50-km grid) for USA<br>\n",
        "Precipitation GPV is on the HGRID (25-km grid) for JP<br>\n",
        "Precipitation GPV is on the SGRID (50-km grid) for EU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.568074Z",
          "start_time": "2024-07-17T10:47:28.559573Z"
        },
        "id": "kkmoFecg6HDU"
      },
      "outputs": [],
      "source": [
        "#--Element\n",
        "elem=\"PRECIP\"\n",
        "\n",
        "#--Climatology\n",
        "runclim=\"RCLIM\" #running mean climate\n",
        "\n",
        "#--days of running mean\n",
        "runday = 5\n",
        "\n",
        "#--Units of input data\n",
        "vunits = 'mm/day'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXbYDBev6HDU"
      },
      "source": [
        "- The precipitation data used here are the 5-day running mean precipitation anomalies.\n",
        "- The anomalies of daily mean precipitation were obtained by subtracting the moving 20-yr climatological daily mean (derived from the preceding 20-yr data) from the raw data. Then, 5-day running-mean anomalies were computed by averaging the anomalies using the previous 5 days to remove the short-term weather events (e.g., squall lines). Therefore, the target is large-scale anomalous precipitation that lasts for a few days rather than short-term daily extreme precipitation events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.577281Z",
          "start_time": "2024-07-17T10:47:28.568908Z"
        },
        "id": "CPB53y9f6HDU"
      },
      "outputs": [],
      "source": [
        "#--parameters for season\n",
        "if season==\"DJF\":\n",
        "    selmonths=\"1,2,12\"\n",
        "elif season==\"JAS\":\n",
        "    selmonths=\"7,8,9\"\n",
        "elif season==\"MJJASO\":\n",
        "    selmonths=\"5,6,7,8,9,10\"\n",
        "elif season==\"AMJJAS\":\n",
        "    selmonths=\"4,5,6,7,8,9\"\n",
        "elif season==\"JJAS\":\n",
        "    selmonths=\"6,7,8,9\"\n",
        "print (\"selmonths=\",selmonths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOTypfTZ6HDV"
      },
      "source": [
        "\"selmonths\" is used for reading input data through the python-cdo later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.594904Z",
          "start_time": "2024-07-17T10:47:28.578130Z"
        },
        "id": "Rho7h8Ic6HDV"
      },
      "outputs": [],
      "source": [
        "#--parameters for plotting\n",
        "if domain==\"JP\":\n",
        "    ddomain = [123,146,24,46]\n",
        "    figsize = (20,12)\n",
        "    legend_posi = [0.2, 0.1, 0.6, 0.03]\n",
        "    contours=np.arange(-15.0,15.0,0.5) # set contours to plot later\n",
        "    cmap=cm.bwr_r # set colormap\n",
        "\n",
        "elif domain==\"USA\":\n",
        "    #ddomain = [232,295,22,51] # draw domain\n",
        "    ddomain = [232,295,23,46] # draw domain\n",
        "\n",
        "    figsize = (20,10)\n",
        "    legend_posi = [0.2, 0.2, 0.6, 0.03]\n",
        "    contours=np.arange(-10.0,10.0,0.5) # set contours to plot later\n",
        "    cmap=cm.bwr_r # set colormap\n",
        "elif domain==\"EU\":\n",
        "    #ddomain = [232,295,22,51] # draw domain\n",
        "   #ddomain = [-24.5,45,26,71] # what is domain? ******* NEEDS CHANGE -> This is the domain of plot: [west, east, south, north] given in longigudes and latitudes\n",
        "    ddomain = [-11.5, 45.0,32.5,71.0]\n",
        "    figsize = (20,15)\n",
        "    legend_posi = [0.2, 0.2, 0.6, 0.01]\n",
        "    contours=np.arange(-10.0,10.0,0.5) # set contours to plot later\n",
        "    cmap=cm.bwr_r # set colormap\n",
        "\n",
        "    dlon0,dlat0=5,5\n",
        "    xticks0=np.arange(-20,45,dlon0)\n",
        "    yticks0=np.arange(30,70,dlat0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrlUSU4iq-lp"
      },
      "source": [
        "## Observed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.608905Z",
          "start_time": "2024-07-17T10:47:28.595764Z"
        },
        "id": "9EQmDDoNwFw9"
      },
      "outputs": [],
      "source": [
        "if domain==\"EU\":\n",
        "    #datadir = \"/content/drive/MyDrive/Precip_Europe\"\n",
        "    #ifile = \"%s/merge2_SPEAR_MED_EUROPE_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    datadir = \"/archive/hnm/for_someone/Sophia/RR\"\n",
        "    if grid==\"OGRID\":\n",
        "        ifile = \"%s/merge_SPEAR_1x1_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "        regfile = \"/archive/hnm/Comm/grid_1x1_EUROPE3_europeregion.nc\"\n",
        "    elif grid==\"SGRID\":\n",
        "        ifile = \"%s/merge_SPEAR_spear_m3d_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "        regfile = \"/archive/hnm/Comm/grid_SPEAR_MED_EUROPE3_europeregion.nc\"\n",
        "\n",
        "#cdo.infov(input=ifile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx75mFcF6HDW"
      },
      "source": [
        "These parameters are used for drawing maps using Basemap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.625115Z",
          "start_time": "2024-07-17T10:47:28.609802Z"
        },
        "id": "lWCL5__TSOJz"
      },
      "outputs": [],
      "source": [
        "#--Output Data Directory\n",
        "datatype=\"test7_spear_med_model_combinedautoencoder_1971-2100_copy2\"\n",
        "\n",
        "print (\"Input Directory=\",datadir)\n",
        "print (\"Output Directory=\",datatype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.629600Z",
          "start_time": "2024-07-17T10:47:28.625946Z"
        },
        "id": "Xzl2FJD3xBk0"
      },
      "outputs": [],
      "source": [
        "domain = \"EU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wlGyr-t6HDW"
      },
      "source": [
        "Define input and output directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.644019Z",
          "start_time": "2024-07-17T10:47:28.630435Z"
        },
        "id": "PgkKW_0D6HDW"
      },
      "outputs": [],
      "source": [
        "odirs2 = [\"output\",\"anomaly\",\"output_model\",\"anomaly_model\",\"output_test\",\"anomaly_test\",\"restored\",\"restored_model\",\"test\",\"restored_test\",\"output_119\",\"output_nat\",\"output_245\",\"output_370\",\"output_534\"]\n",
        "\n",
        "for odir2 in odirs2:\n",
        "    if not os.path.exists(\"%s/%s\" % (datatype,odir2)):\n",
        "        os.makedirs(\"%s/%s\" % (datatype,odir2))\n",
        "    print  (datatype+\"/\"+odir2)\n",
        "    print(datatype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0GwfldT6HDX"
      },
      "source": [
        "Creating output directory. Data structure is as follows.\n",
        "\n",
        "<pre>\n",
        "   |-- Input_data\n",
        "   |     |--AphroJP_V1207_DPREC.1900-2010_V1207R3_DPREC.2011-2015_HiFLOR_5-day_anom_cprev20yrs.nc (APRODITE Data)\n",
        "   |     |\n",
        "   |     |--PRISM_ppt_stable_4kmM3_1981-2021_GSPEAR_5-day_anom_cprev20yrs.nc (PRISM Data)\n",
        "   |\n",
        "   |-- Output_JP (output for anmaly detection for the Japan domain)\n",
        "   |     |--anomaly (output directory for picutres of anomalous events)\n",
        "   |     |    |--cluster6 (output for K-mean Cluster)\n",
        "   |     |\n",
        "   |     |--output (output directory for MSE and autoencoder model)\n",
        "   |\n",
        "   |-- Output_USA (output for anmaly detection for the USA domain)\n",
        "   |     |--anomaly (output directory for picutres of anomalous events)\n",
        "   |     |    |--cluster6 (output for K-mean Cluster)\n",
        "   |     |\n",
        "   |     |--output (output directory for MSE and autoencoder model)\n",
        "   |\n",
        "   |-- mylib (includes statistical codes)\n",
        "   |\n",
        "   |-- pyimagesearch (includes tensorflow codes)\n",
        "   |\n",
        "   |-- pics (some reference pictures)\n",
        "  \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.647356Z",
          "start_time": "2024-07-17T10:47:28.644873Z"
        },
        "id": "cV-xWyxx6HDX"
      },
      "outputs": [],
      "source": [
        "#parameters for normalizing\n",
        "if elem==\"PRECIP\":\n",
        "    if domain==\"JP\":\n",
        "        maxvalue = 150\n",
        "        minvalue = -40\n",
        "    elif domain==\"USA\":\n",
        "        maxvalue = 100\n",
        "        minvalue = -40\n",
        "    elif domain==\"EU\":\n",
        "        maxvalue = 200\n",
        "        minvalue = -50\n",
        "\n",
        "\n",
        "# NEEDS CHANGE\n",
        "print (\"normalized = (rawdata - %i)/(%i - %i)\" % (minvalue,maxvalue,minvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr0ZEecY6HDX"
      },
      "source": [
        "This determins the parameter for normalization.<br>\n",
        "Precipitation data (mm/day) is normalized resulting in normalized data that approximately range 0-1.<br>\n",
        "> normalized = (rawdata - minvalue)/(maxvalue - minvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ont2RQWP6HDY"
      },
      "source": [
        "# Subroutines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUNaHVmu6HDY"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.658367Z",
          "start_time": "2024-07-17T10:47:28.648189Z"
        },
        "id": "IE060Wfl6HDY"
      },
      "outputs": [],
      "source": [
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        top=4\n",
        "        bot=4\n",
        "        left=2\n",
        "        right=2\n",
        "\n",
        "elif domain==\"USA\":\n",
        "    if grid==\"SGRID\":\n",
        "        top=2\n",
        "        bot=1\n",
        "        left=1\n",
        "        right=1\n",
        "elif domain==\"EU\": # NEEDS CHANGING\n",
        "    if grid==\"SGRID\":\n",
        "        top=1\n",
        "        bot=0\n",
        "        left=2\n",
        "        right=2\n",
        "    elif grid==\"OGRID\":\n",
        "        top=1\n",
        "        bot=0\n",
        "        left=0\n",
        "        right=0\n",
        "\n",
        "pad_width1=((0,0),(bot,top),(left,right))\n",
        "pad_width2=((0,0),(0,0),(bot,top),(left,right))\n",
        "\n",
        "print (\"pad=\",pad_width1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCn9UtcC6HDY"
      },
      "source": [
        "Because a specific number of grid sizes that is computationally efficient, zero values were further padded to the edges of each longitude and latitude axis after the normalization; namely, four grids on both sides of longitudinal edges and two grids on both sides of latitudinal edges were padded with zero values for HGRID.<br>\n",
        "\n",
        "![padding.png](data:image/png;base64,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)\n",
        "\n",
        "For USA (PRISM data), the original dimension (Time, Latitude, Longitude) = (3772, 29, 50) is expanded to (3772, 32, 52).<br>\n",
        "For JP (APHRODITE data), the original dimension (Time, Latitude, Longitude) = (7176, 88, 92) is expanded to (7176, 96, 96)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.665760Z",
          "start_time": "2024-07-17T10:47:28.659423Z"
        },
        "id": "JW7Q5D2gq-lt"
      },
      "outputs": [],
      "source": [
        "def pad_remover(rdata,domain,grid, multidata=False):\n",
        "    ndim = rdata.ndim\n",
        "    if domain==\"JP\":\n",
        "        if grid==\"HGRID\":\n",
        "            if ndim==4:\n",
        "                if multidata:\n",
        "                    recon = rdata[:,4:-4,2:-2,0]\n",
        "                else:\n",
        "                    recon = rdata[0,4:-4,2:-2,0]\n",
        "            elif ndim==3:\n",
        "                recon = rdata[4:-4,2:-2,0]\n",
        "    elif domain==\"USA\":\n",
        "        if grid==\"SGRID\":\n",
        "            if ndim==4:\n",
        "                if multidata:\n",
        "                    recon = rdata[:,1:-2,1:-1,0]\n",
        "                else:\n",
        "                    recon = rdata[0,1:-2,1:-1,0]\n",
        "            elif ndim==3:\n",
        "                recon = rdata[1:-2,1:-1,0]\n",
        "    elif domain==\"EU\":\n",
        "        if ndim==4:\n",
        "            if multidata:\n",
        "                if top==0 and right!=0:\n",
        "                    recon = rdata[:,bot::,left:-right,0]\n",
        "                elif top!=0 and right==0:\n",
        "                    recon = rdata[:,bot:-top,left::,0]\n",
        "                elif top==0 and right==0:\n",
        "                    recon = rdata[:,bot::,left::,0]\n",
        "                else:\n",
        "                    recon = rdata[:,bot:-top,left:-right,0]\n",
        "            else:\n",
        "                if top==0 and right!=0:\n",
        "                    recon = rdata[0,bot::,left:-right,0]\n",
        "                elif top!=0 and right==0:\n",
        "                    recon = rdata[0,bot:-top,left::,0]\n",
        "                elif top==0 and right==0:\n",
        "                    recon = rdata[0,bot::,left::,0]\n",
        "                else:\n",
        "                    recon = rdata[0,bot:-top,left:-right,0]\n",
        "\n",
        "        elif ndim==3:\n",
        "            if top==0 and right!=0:\n",
        "                recon = rdata[bot::,left:-right,0]\n",
        "            elif top!=0 and right==0:\n",
        "                recon = rdata[bot:-top,left::,0]\n",
        "            elif top==0 and right==0:\n",
        "                recon = rdata[bot::,left::,0]\n",
        "            else:\n",
        "                recon = rdata[bot:-top,left:-right,0]\n",
        "\n",
        "    return recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTVIdZ836HDZ"
      },
      "source": [
        "This function is used for removing padded data to restore the data on original grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYJ9En9Q6HDZ"
      },
      "source": [
        "## Subroutine for Computing Linear Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.674859Z",
          "start_time": "2024-07-17T10:47:28.666734Z"
        },
        "id": "V5A0sw6f6HDZ"
      },
      "outputs": [],
      "source": [
        "def linreg(X,Y):\n",
        "    \"\"\"\n",
        "     return a,b in solution to y = ax + b such that root mean square distance\n",
        "     between trend line and original points is minimized\n",
        "    \"\"\"\n",
        "    N = len(X)\n",
        "    Sx = Sy = Sxx = Syy = Sxy = 0.0\n",
        "\n",
        "    for x, y in zip(X, Y):\n",
        "        Sx = Sx + x\n",
        "        Sy = Sy + y\n",
        "        Sxx = Sxx + x*x\n",
        "        Syy = Syy + y*y\n",
        "        Sxy = Sxy + x*y\n",
        "    det = Sxx * N - Sx * Sx\n",
        "    aa = (Sxy * N - Sy * Sx)/det\n",
        "    bb = (Sxx * Sy - Sx * Sxy)/det\n",
        "    YY2 = aa * X + bb\n",
        "    return X, YY2, aa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz-Ee6C46HDZ"
      },
      "source": [
        "This function is used for computing linear trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB-TzNda6HDa"
      },
      "source": [
        "## Subroutine for Mann-Kendall Trend Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.691397Z",
          "start_time": "2024-07-17T10:47:28.675697Z"
        },
        "id": "FhgsxN_l6HDa"
      },
      "outputs": [],
      "source": [
        "def mk_test(x, alpha = 0.05):\n",
        "    from scipy.stats import norm\n",
        "    \"\"\"\n",
        "    this perform the MK (Mann-Kendall) test to check if the trend is present in\n",
        "    data or not\n",
        "\n",
        "    Input:\n",
        "        x:   a vector of data\n",
        "        alpha: significance level\n",
        "\n",
        "    Output:\n",
        "        h: True (if trend is present) or False (if trend is absence)\n",
        "        p: p value of the sifnificance test\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "      >>> x = np.random.rand(100)\n",
        "      >>> h,p = mk_test(x,0.05)  # meteo.dat comma delimited\n",
        "    \"\"\"\n",
        "    n = len(x)\n",
        "\n",
        "    # calculate S\n",
        "    s = 0\n",
        "    for k in range(n-1):\n",
        "        for j in range(k+1,n):\n",
        "            s += np.sign(x[j] - x[k])\n",
        "\n",
        "    # calculate the unique data\n",
        "    unique_x = np.unique(x)\n",
        "    g = len(unique_x)\n",
        "\n",
        "    # calculate the var(s)\n",
        "    if n == g: # there is no tie\n",
        "        var_s = (n*(n-1)*(2*n+5))/18\n",
        "    else: # there are some ties in data\n",
        "        tp = np.zeros(unique_x.shape)\n",
        "        for i in range(len(unique_x)):\n",
        "            tp[i] = sum(unique_x[i] == x)\n",
        "        var_s = (n*(n-1)*(2*n+5) + np.sum(tp*(tp-1)*(2*tp+5)))/18\n",
        "\n",
        "    if s>0:\n",
        "        z = (s - 1)/np.sqrt(var_s)\n",
        "    elif s == 0:\n",
        "        z = 0\n",
        "    elif s<0:\n",
        "        z = (s + 1)/np.sqrt(var_s)\n",
        "\n",
        "    # calculate the p_value\n",
        "    p = 2*(1-norm.cdf(abs(z))) # two tail test\n",
        "    h = abs(z) > norm.ppf(1-alpha/2)\n",
        "\n",
        "    return h, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOtHRmU2q-lt"
      },
      "source": [
        "## Subroutine for Regional statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.704815Z",
          "start_time": "2024-07-17T10:47:28.692411Z"
        },
        "id": "7AhTsTmlq-lt"
      },
      "outputs": [],
      "source": [
        "def regional_stats(uids,regids,rdata,kind=\"mean\"):\n",
        "\n",
        "     values_ = np.zeros(len(uids))\n",
        "\n",
        "     for jj,uid in enumerate(uids):\n",
        "         igrid_ = np.where(regids==uid)\n",
        "         if kind==\"mean\":\n",
        "             values_[jj] = rdata[igrid_].mean()\n",
        "         elif kind==\"max\":\n",
        "             values_[jj] = rdata[igrid_].max()\n",
        "         elif kind==\"min\":\n",
        "             values_[jj] = rdata[igrid_].min()\n",
        "\n",
        "     if kind==\"mean\":\n",
        "         ireg = np.argmax(values_)\n",
        "     elif kind==\"max\":\n",
        "         ireg = np.argmax(values_)\n",
        "     elif kind==\"min\":\n",
        "         ireg = np.argmin(values_)\n",
        "\n",
        "     return uids[ireg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bklLpXxu6HDa"
      },
      "source": [
        "This function is used for computing statistical significance in trend ([Mann-Kendall test](https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvjzX6-B6HDa"
      },
      "source": [
        "# Reading Input Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPAMd9GM6HDb"
      },
      "source": [
        "## Reading regional mask data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:28.801010Z",
          "start_time": "2024-07-17T10:47:28.705644Z"
        },
        "id": "oGWrNDmmq-lu"
      },
      "outputs": [],
      "source": [
        "fXX = Dataset(regfile,\"r\")\n",
        "\n",
        "jreg1 = fXX.variables['jreg1'][0]\n",
        "jreg2 = fXX.variables['jreg2'][0]\n",
        "\n",
        "jreg1_uids = np.unique(jreg1)\n",
        "jreg2_uids = np.unique(jreg2)\n",
        "\n",
        "#--- use jreg2\n",
        "jreg = jreg2\n",
        "jreg_uids = jreg2_uids\n",
        "jreg_uids= [x for x in jreg2_uids if x != 0]\n",
        "\n",
        "\n",
        "nreg = len(jreg_uids)\n",
        "print (nreg)\n",
        "sregnames={\n",
        "    1:\"Eastern Europe\",\n",
        "    2:\"Northern Europe\",\n",
        "    3:\"Central Europe\",\n",
        "    4:\"Southeastern Europe\",\n",
        "    5:\"Western Europe\",\n",
        "    6:\"Southern Europe\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzzPkY5Gq-lu"
      },
      "source": [
        "## Reading Observed Precipitation Dataset (PRISM or APHRODITE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T10:47:30.854452Z",
          "start_time": "2024-07-17T10:47:28.801868Z"
        },
        "id": "q5x7Q48w6HDb"
      },
      "outputs": [],
      "source": [
        "if model==\"APHRODITE\":\n",
        "    ifile1 = \"%s/AphroJP_V1207_DPREC.1900-2010_V1207R3_DPREC.2011-2015_HiFLOR_5-day_anom_cprev20yrs.nc\"  % (datadir)\n",
        "    ielem = elem.lower()\n",
        "elif model==\"PRISMG\":\n",
        "    ifile1 = \"%s/PRISM_ppt_stable_4kmM3_1981-2021_GSPEAR_5-day_anom_cprev20yrs.nc\" % (datadir)\n",
        "    ielem= \"data\"\n",
        "elif model==\"E-OBS\":\n",
        "    if grid==\"SGRID\":\n",
        "        ifile1 = \"%s/merge_RR_SPEAR_MED_EUROPE3_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "    elif grid==\"OGRID\":\n",
        "        ifile1 = \"%s/merge_RR_1x1_EUROPE3_5-day_anom_1950-2023_cprev20yrs.nc\" % (datadir)\n",
        "\n",
        "    ielem= \"rr\" # made successful change here\n",
        "\n",
        "print (\"input file=\",ifile1)\n",
        "f11 = Dataset(ifile1,\"r\")\n",
        "\n",
        "times_orig = f11.variables['time']\n",
        "dates_orig = num2date(times_orig[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "dates_pd = pd.to_datetime(dates_orig)\n",
        "periods = dates_pd.to_period(freq='D')\n",
        "\n",
        "smon = int(selmonths.split(\",\")[0])\n",
        "emon = int(selmonths.split(\",\")[-1])\n",
        "mask_dates = (periods.year>=tsyear)&(periods.year<=teyear)&(periods.month>=smon)&(periods.month<=emon)\n",
        "#print (dates_pd[mask_dates])\n",
        "\n",
        "data = f11.variables[ielem][mask_dates]\n",
        "data = data.filled(fill_value=0)\n",
        "\n",
        "\n",
        "lons = f11.variables['lon']\n",
        "lats = f11.variables['lat']\n",
        "#times = f11.variables['time']\n",
        "times = times_orig[mask_dates]\n",
        "dates = num2date(times[:],times_orig.units ,calendar=times_orig.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "print  (\"dates in the input data=\",dates)\n",
        "print (\"data dimension=(time,latitude,longitude)=\",np.shape(data[:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7q_gASAq-lu"
      },
      "source": [
        "## Read Model Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTIIbRVq-lu"
      },
      "source": [
        "### SPEAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:15:21.533618Z",
          "start_time": "2024-07-17T10:47:30.855615Z"
        },
        "id": "oisnlozPq-lu"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " data2=[]\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        " #for ee in range(1,emax+1,1):\n",
        " for ee in range(1,1+1,1):\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "      ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f12 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig2 = f12.variables['time']\n",
        "   dates_orig2 = num2date(times_orig2[:],times_orig2.units ,calendar=times_orig2.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd2 = pd.to_datetime(dates_orig2)\n",
        "   periods2 = dates_pd2.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates2 = (periods2.year>=tsyear)&(periods2.year<=teyear)&(periods2.month>=smon)&(periods2.month<=emon)\n",
        "\n",
        "   data2_temp = f12.variables[ename2][mask_dates2]\n",
        "   data2_temp = data2_temp.filled(fill_value=0)\n",
        "\n",
        "   data2.append(data2_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons2 = f12.variables['lon']\n",
        "     lats2 = f12.variables['lat']\n",
        "#     times2 = f12.variables['time']\n",
        "\n",
        "#     print  (times2)\n",
        "     dates2 = dates_orig2[mask_dates2]\n",
        "\n",
        " data2=np.array(data2, dtype=np.float16)\n",
        "\n",
        " data3=[]\n",
        " #for ee in range(1,emax2+1,1):\n",
        " for ee in range(1,1+1,1):\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data3.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data3=np.array(data3, dtype=np.float16)\n",
        " print (np.shape(data2))\n",
        " print (np.shape(data3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQwwtzz1q-lu"
      },
      "outputs": [],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c_b9KL8q-lu"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " print(\"TEST\")\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " data5=[]\n",
        " testmodel2=\"NATURAL\"\n",
        " for ee in range(1,12+1,1):\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_C192_o1_Hist_NoAnthroAerosols_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "       print(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data5.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data5=np.array(data5, dtype=np.float16)\n",
        " print (np.shape(data5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbKDnECJq-lu"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " print(\"TEST\")\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " data4=[]\n",
        " testmodel2=\"NATURAL\"\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  if testmodel2==\"ALLSSP585\":\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP505_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "  elif testmodel2==\"NATURAL\":\n",
        "       #print(\"here\")\n",
        "       input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_NATURAL_IC1921_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data4.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "#     times3 = f13.variables['time']\n",
        "\n",
        "#     print  (times3)\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data4=np.array(data4, dtype=np.float16)\n",
        " print (np.shape(data4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5Jw828Nnq-lv"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " data119=[]\n",
        " data245=[]\n",
        " data370=[]\n",
        " data534=[]\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP119_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data119.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data119=np.array(data119, dtype=np.float16)\n",
        " print (np.shape(data119))\n",
        " data119"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufQB3zjmq-lv"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP245_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data245.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data245=np.array(data245, dtype=np.float16)\n",
        " print (np.shape(data245))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvdQ-etZq-lv"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        "\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        "\n",
        " for ee in range(1,emax2+1,1):\n",
        "  testmodel2=\"NATURAL\"\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP370_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    #print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   print(np.shape(data3_temp))\n",
        "   data370.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data370=np.array(data370, dtype=np.float16)\n",
        " print (np.shape(data370))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8fn05XAq-lv"
      },
      "outputs": [],
      "source": [
        "if testmodel[0:9]==\"SPEAR-MED\" and testmodel[0:14]!=\"SPEAR-MED-COMB\":\n",
        " ename2=\"precip\"\n",
        " if domain==\"JP\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"jp_spear_med\"\n",
        "        elif grid == \"SGRID2\":\n",
        "            grid2=\"jp_spear_med2\"\n",
        "        elif grid==\"HGRID\":\n",
        "            grid2=\"jp_hiflor\"\n",
        " elif domain==\"CN\":\n",
        "        if grid==\"CN025\":\n",
        "            grid2=\"cn_CN025\"\n",
        "        elif grid==\"CN050\":\n",
        "            grid2=\"cn_CN050\"\n",
        " elif domain==\"EU\":\n",
        "        if grid == \"SGRID\":\n",
        "            grid2=\"europe3_spear_med\"\n",
        "        elif grid==\"OGRID\":\n",
        "            grid2=\"europe3_1x1\"\n",
        "\n",
        " if runclim==\"RCLIM\":\n",
        "     key=\"cprev20yrs\"\n",
        " elif runclim==\"FCLIM\":\n",
        "     key=\"c1981-2000\"\n",
        "\n",
        " #for ee in range(1,emax2+1,1):\n",
        " for ee in range(1,1+1,1):\n",
        "  input_dir = \"/archive/hnm/SPEAR_anal/SPEAR_c192_o1_Hist_AllForc_IC1921_SSP534OS_IC2011_K50/pp_ens_%2.2i/atmos_daily/ts/daily/10yr_%s/%s\" % (ee,grid2,ename2)\n",
        "\n",
        "  if ename2==\"precip\":\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s_mmday.nc' % (ename2,runday,key))\n",
        "  else:\n",
        "       ifiles = glob.glob(input_dir + '/merge_atmos_daily.*%s*%i-day_anom_%s.nc' % (ename2,runday,key))\n",
        "\n",
        "\n",
        "  if len(ifiles) ==0:\n",
        "    print(\"here\")\n",
        "    pass\n",
        "  else:\n",
        "   ifile1 = ifiles[0]\n",
        "   print (\"reading=\",ifile1)\n",
        "\n",
        "   f13 = Dataset(ifile1,\"r\")\n",
        "\n",
        "   times_orig3 = f13.variables['time']\n",
        "   dates_orig3 = num2date(times_orig3[:],times_orig3.units ,calendar=times_orig3.calendar, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
        "\n",
        "   dates_pd3 = pd.to_datetime(dates_orig3)\n",
        "   periods3 = dates_pd3.to_period(freq='D')\n",
        "\n",
        "   smon = int(selmonths.split(\",\")[0])\n",
        "   emon = int(selmonths.split(\",\")[-1])\n",
        "   mask_dates3 = (periods3.year>=tsyear2)&(periods3.year<=teyear2)&(periods3.month>=smon)&(periods3.month<=emon)\n",
        "\n",
        "   data3_temp = f13.variables[ename2][mask_dates3]\n",
        "   data3_temp = data3_temp.filled(fill_value=0)\n",
        "\n",
        "   data534.append(data3_temp[:])\n",
        "\n",
        "   if ee==1:\n",
        "     lons3 = f13.variables['lon']\n",
        "     lats3 = f13.variables['lat']\n",
        "\n",
        "     dates3 = dates_orig3[mask_dates3]\n",
        "\n",
        " data534=np.array(data534, dtype=np.float16)\n",
        " print (np.shape(data534))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbdE9SXx6HDb"
      },
      "source": [
        "The input netcdf data are already converted from the original precipitation data to 5-day mean anomaly.<br>\n",
        "Input data is red through python-cdo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAy8pvE56HDb"
      },
      "source": [
        "## Normalizing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYItuIyLq-lv"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:15:21.774145Z",
          "start_time": "2024-07-17T11:15:21.535716Z"
        },
        "id": "kyhtVvKX6HDb"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data.min()=\",data[:].min())\n",
        "print (\"data.max()=\",data[:].max())\n",
        "\n",
        "data_norm = (data[:].squeeze() - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm.min()=\",data_norm.min())\n",
        "print (\"data_norm.max()=\",data_norm.max())\n",
        "print (\"input data dimension (time,lat,lon)=\",np.shape(data_norm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDdl08-Zq-lx"
      },
      "source": [
        "### Model training period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:17:08.959389Z",
          "start_time": "2024-07-17T11:15:21.775143Z"
        },
        "id": "xQZW65Rcq-lx"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data2.min()=\",data2[:].min())\n",
        "print (\"data2.max()=\",data2[:].max())\n",
        "\n",
        "data_norm2 = (data2[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm2.min()=\",data_norm2.min())\n",
        "print (\"data_norm2.max()=\",data_norm2.max())\n",
        "print (\"input data2 dimension (ens,time,lat,lon)=\",np.shape(data_norm2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUuIjyrDq-lx"
      },
      "source": [
        "### Model test period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:44.577194Z",
          "start_time": "2024-07-17T11:17:08.960407Z"
        },
        "id": "1J2YwHD8q-lx"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data3.min()=\",data3[:].min())\n",
        "print (\"data3.max()=\",data3[:].max())\n",
        "\n",
        "data_norm3 = (data3[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm3.min()=\",data_norm3.min())\n",
        "print (\"data_norm3.max()=\",data_norm3.max())\n",
        "print (\"input data3 dimension (ens,time,lat,lon)=\",np.shape(data_norm3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQFxxvEBq-lx"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data4.min()=\",data4[:].min())\n",
        "print (\"data4.max()=\",data4[:].max())\n",
        "\n",
        "data_norm4 = (data4[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm4.min()=\",data_norm4.min())\n",
        "print (\"data_norm4.max()=\",data_norm4.max())\n",
        "print (\"input data4 dimension (ens,time,lat,lon)=\",np.shape(data_norm4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKJQUMpXq-ly"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data4.min()=\",data119[:].min())\n",
        "print (\"data4.max()=\",data119[:].max())\n",
        "\n",
        "data_norm119 = (data119[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm4.min()=\",data_norm119.min())\n",
        "print (\"data_norm4.max()=\",data_norm119.max())\n",
        "print (\"input data4 dimension (ens,time,lat,lon)=\",np.shape(data_norm119))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqNOrajxq-ly"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data4.min()=\",data245[:].min())\n",
        "print (\"data4.max()=\",data245[:].max())\n",
        "\n",
        "data_norm245= (data245[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm4.min()=\",data_norm245.min())\n",
        "print (\"data_norm4.max()=\",data_norm245.max())\n",
        "print (\"input data4 dimension (ens,time,lat,lon)=\",np.shape(data_norm245))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFG2CgSnq-ly"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data4.min()=\",data370[:].min())\n",
        "print (\"data4.max()=\",data370[:].max())\n",
        "\n",
        "data_norm370 = (data370[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm4.min()=\",data_norm370.min())\n",
        "print (\"data_norm4.max()=\",data_norm370.max())\n",
        "print (\"input data4 dimension (ens,time,lat,lon)=\",np.shape(data_norm370))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTpKGEuBq-ly"
      },
      "outputs": [],
      "source": [
        "### Normalize\n",
        "print (\"data4.min()=\",data534[:].min())\n",
        "print (\"data4.max()=\",data534[:].max())\n",
        "\n",
        "data_norm534 = (data534[:] - minvalue)/(maxvalue-minvalue)\n",
        "print (\"data_norm4.min()=\",data_norm534.min())\n",
        "print (\"data_norm4.max()=\",data_norm534.max())\n",
        "print (\"input data4 dimension (ens,time,lat,lon)=\",np.shape(data_norm534))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7n8gDv6HDc"
      },
      "source": [
        "## Editing Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:44.616437Z",
          "start_time": "2024-07-17T11:21:44.580138Z"
        },
        "id": "zewhQ3kh6HDc"
      },
      "outputs": [],
      "source": [
        "# Date\n",
        "yyyymmdds=[]\n",
        "for date in dates[:]:\n",
        "    yyyymmdds.append(int(\"%4.4i%2.2i%2.2i\" % (date.year,date.month,date.day)))\n",
        "\n",
        "ds = pd.Series(yyyymmdds)\n",
        "dates = pd.to_datetime(ds, format='%Y%m%d')\n",
        "print (dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:44.651009Z",
          "start_time": "2024-07-17T11:21:44.617758Z"
        },
        "id": "TiMJEJ2sq-ly"
      },
      "outputs": [],
      "source": [
        "# Get date for training\n",
        "yyyymmdds2=[]\n",
        "for date2 in dates2[:]:\n",
        "    yyyymmdds2.append(int(\"%4.4i%2.2i%2.2i\" % (date2.year,date2.month,date2.day)))\n",
        "\n",
        "ds2 = pd.Series(yyyymmdds2)\n",
        "dates2 = pd.to_datetime(ds2, format='%Y%m%d')\n",
        "print (dates2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:44.725839Z",
          "start_time": "2024-07-17T11:21:44.652169Z"
        },
        "id": "1zhV4RdMq-ly"
      },
      "outputs": [],
      "source": [
        "yyyymmdds3=[]\n",
        "for date3 in dates3[:]:\n",
        "    yyyymmdds3.append(int(\"%4.4i%2.2i%2.2i\" % (date3.year,date3.month,date3.day)))\n",
        "\n",
        "ds3 = pd.Series(yyyymmdds3)\n",
        "dates3 = pd.to_datetime(ds3, format='%Y%m%d')\n",
        "print (dates3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqnIc24Z6HDc"
      },
      "source": [
        "## Padding the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:45.039156Z",
          "start_time": "2024-07-17T11:21:44.727085Z"
        },
        "id": "ugRH8_Ed6HDd"
      },
      "outputs": [],
      "source": [
        "# Input for training\n",
        "temp_img_array_list = []\n",
        "\n",
        "temp_img_array_list = data_norm\n",
        "print (\"original dimension=\",np.shape(temp_img_array_list))\n",
        "temp_img_array_list = np.pad(temp_img_array_list, pad_width=pad_width1, mode='constant',constant_values=0)\n",
        "\n",
        "temp_img_array_list =  np.expand_dims(temp_img_array_list, axis=-1)\n",
        "\n",
        "tmax,jmax,imax,zmax = np.shape(temp_img_array_list)\n",
        "print (\"dimension after padding=\",np.shape(temp_img_array_list), domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:50.005506Z",
          "start_time": "2024-07-17T11:21:45.040565Z"
        },
        "id": "1HblKYDnq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list2 = data_norm2\n",
        "print (np.shape(temp_img_array_list2))\n",
        "\n",
        "temp_img_array_list2 = np.pad(temp_img_array_list2, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list2 =  np.expand_dims(temp_img_array_list2, axis=-1)\n",
        "\n",
        "emax2,tmax2,jmax2,imax2,zmax2 = np.shape(temp_img_array_list2)\n",
        "print (np.shape(temp_img_array_list2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:56.761168Z",
          "start_time": "2024-07-17T11:21:50.006808Z"
        },
        "id": "pqMMnyrtq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3 = data_norm3\n",
        "print (np.shape(temp_img_array_list3))\n",
        "temp_img_array_list3 = np.pad(temp_img_array_list3, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list3 =  np.expand_dims(temp_img_array_list3, axis=-1)\n",
        "emax3,tmax3,jmax3,imax3,zmax3 = np.shape(temp_img_array_list3)\n",
        "\n",
        "print (np.shape(temp_img_array_list3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9D3cekq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list4 = data_norm4 # natural\n",
        "print (np.shape(temp_img_array_list4))\n",
        "temp_img_array_list4 = np.pad(temp_img_array_list4, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list4 =  np.expand_dims(temp_img_array_list4, axis=-1)\n",
        "emax4,tmax4,jmax4,imax4,zmax4 = np.shape(temp_img_array_list4)\n",
        "\n",
        "print (np.shape(temp_img_array_list4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr7zJPnbq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list119 = data_norm119\n",
        "print (np.shape(temp_img_array_list119))\n",
        "temp_img_array_list119 = np.pad(temp_img_array_list119, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list119 =  np.expand_dims(temp_img_array_list119, axis=-1)\n",
        "emax119,tmax119,jmax119,imax119,zmax119 = np.shape(temp_img_array_list119)\n",
        "\n",
        "print (np.shape(temp_img_array_list119))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgypOgtJq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list245 = data_norm245\n",
        "print (np.shape(temp_img_array_list245))\n",
        "temp_img_array_list245 = np.pad(temp_img_array_list245, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list245 =  np.expand_dims(temp_img_array_list245, axis=-1)\n",
        "emax245,tmax245,jmax245,imax245,zmax245 = np.shape(temp_img_array_list245)\n",
        "\n",
        "print (np.shape(temp_img_array_list245))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAghgP7Qq-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list370 = data_norm370\n",
        "print (np.shape(temp_img_array_list370))\n",
        "temp_img_array_list370 = np.pad(temp_img_array_list370, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list370 =  np.expand_dims(temp_img_array_list370, axis=-1)\n",
        "emax370,tmax370,jmax370,imax370,zmax370 = np.shape(temp_img_array_list370)\n",
        "\n",
        "print (np.shape(temp_img_array_list370))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKeYDIn2q-ly"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list534 = data_norm534\n",
        "print (np.shape(temp_img_array_list534))\n",
        "temp_img_array_list534 = np.pad(temp_img_array_list534, pad_width=pad_width2, mode='constant',constant_values=0)\n",
        "temp_img_array_list534 =  np.expand_dims(temp_img_array_list534, axis=-1)\n",
        "emax534,tmax534,jmax534,imax534,zmax534 = np.shape(temp_img_array_list534)\n",
        "\n",
        "print (np.shape(temp_img_array_list534))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOLIiw3xq-ly"
      },
      "source": [
        "### Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:56.764900Z",
          "start_time": "2024-07-17T11:21:56.762650Z"
        },
        "id": "T-5Jxn0Qq-ly"
      },
      "outputs": [],
      "source": [
        "emax12, tmax12,jmax12,imax12,kmax12 = np.shape(temp_img_array_list2)\n",
        "temp_img_array_list2_ = temp_img_array_list2.reshape((emax12*tmax12,jmax12,imax12,kmax12))\n",
        "print (np.shape(temp_img_array_list2_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7-T2g326HDd"
      },
      "source": [
        "Line 6: Zero values are padded to the edges of longitude and latitude.<br>\n",
        "Line 8: Reshape the dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnrB4f0h6HDd"
      },
      "source": [
        "## Dividing the input data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:56.870835Z",
          "start_time": "2024-07-17T11:21:56.765950Z"
        },
        "id": "0OGa8lXU6HDe"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(temp_img_array_list, test_size=0.2, random_state=1)\n",
        "\n",
        "print (\"dimension for training data=\",np.shape(train_data))\n",
        "print (\"dimension for validation data=\",np.shape(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:58.989854Z",
          "start_time": "2024-07-17T11:21:56.872120Z"
        },
        "id": "NefZB8liq-ly"
      },
      "outputs": [],
      "source": [
        "train_data2, test_data2 = train_test_split(temp_img_array_list2_, test_size=0.2, random_state=1)\n",
        "\n",
        "print (\"dimension for training data=\",np.shape(train_data2))\n",
        "print (\"dimension for validation data=\",np.shape(test_data2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUBMoC16HDe"
      },
      "source": [
        "Split all the data into training data and test data. 20% of the data is assigned to validation data.<br>\n",
        "The training data is used for developing autoencoder, whreas the test data is used for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XONJCuQJ6HDe"
      },
      "source": [
        "# Compiling Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K4MAZtH6HDe"
      },
      "source": [
        "This part is the main core for the autoencoder configuration.<br>\n",
        "There are some layers to build autoencoder.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:58.992929Z",
          "start_time": "2024-07-17T11:21:58.991036Z"
        },
        "id": "ljiB2LCX6HDf"
      },
      "outputs": [],
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 40 # changed from 40 because the data is so large\n",
        "INIT_LR = 1e-3\n",
        "#BS = 8\n",
        "BS = 16\n",
        "\n",
        "# reason for choosing these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgCL-35Z6HDf"
      },
      "source": [
        "EPOCHS: Iteration number to train<br>\n",
        "INIT_LR: Used for decay parameter for Adam (optimizer)<br>\n",
        "BS: Batch Size for training<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH4ajwk16HDf"
      },
      "source": [
        "## Configure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:45:03.090383Z",
          "start_time": "2024-07-17T13:45:03.082733Z"
        },
        "id": "ZHzwDe8V6HDf"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "class ConvAutoencoder:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, filters=(32, 64), latentDim=16, suffix=\"0\"):\n",
        "        # initialize the input shape to be \"channels last\" along with\n",
        "        # the channels dimension itself\n",
        "        # channels dimension itself\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # define the input to the encoder\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = inputs\n",
        "\n",
        "        # loop over the number of filters\n",
        "        for f in filters:\n",
        "            # apply a CONV => RELU => BN operation\n",
        "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "           #x = Activation('tanh')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # flatten the network and then construct our latent vector\n",
        "        volumeSize = K.int_shape(x)\n",
        "        x = Flatten()(x)\n",
        "        latent = Dense(latentDim)(x)\n",
        "\n",
        "        # build the encoder model\n",
        "        encoder = Model(inputs, latent, name=\"encoder_%s\" % (suffix))\n",
        "\n",
        "        # start building the decoder model which will accept the\n",
        "        # output of the encoder as its inputs\n",
        "        latentInputs = Input(shape=(latentDim,))\n",
        "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\n",
        "        # loop over our number of filters again, but this time in\n",
        "        # reverse order\n",
        "        for f in filters[::-1]:\n",
        "            # apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "            x = Conv2DTranspose(f, (3, 3), strides=2,\n",
        "                padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            #x = Activation('tanh')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # apply a single CONV_TRANSPOSE layer used to recover the\n",
        "        # original depth of the image\n",
        "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "        outputs = Activation(\"sigmoid\")(x)\n",
        "        #outputs = Activation(\"tanh\")(x)\n",
        "\n",
        "        # build the decoder model\n",
        "        decoder = Model(latentInputs, outputs, name=\"decoder_%s\" % (suffix))\n",
        "\n",
        "        # our autoencoder is the encoder + decoder\n",
        "        autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
        "            name=\"autoencoder_%s\" % (suffix))\n",
        "\n",
        "        # return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "        return (encoder, decoder, autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabTBL5b6HDg"
      },
      "source": [
        "L18-43: Developing encoder <br>\n",
        "L31: Conv2D(f, (3,3)...) is the 2D convolution layer. f(= [32, 64]) is filter and (3,3) is kernel.<br>\n",
        "L40: Dense(16) is the dense layer to make a lateral data<br>\n",
        "L43: Define encoder<br>\n",
        "\n",
        "L44-L69: Developing decoder<br>\n",
        "L55, L66: Transposed convolution layer<br>\n",
        "\n",
        "L72: Defining autoencoder (= decoder(encoder(inputs)))<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpVjIEAq-lz"
      },
      "source": [
        "### AutoencoderEnsemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:45:05.708566Z",
          "start_time": "2024-07-17T13:45:05.699047Z"
        },
        "id": "_aSVXRHrq-lz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "class AutoencoderEnsemble:\n",
        "    def __init__(self, encoders, decoders, input_shape, optimizer_class, optimizer_kwargs):\n",
        "        self.encoders = encoders\n",
        "        self.decoders = decoders\n",
        "        self.input_shape = input_shape\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "        self.models = []\n",
        "        self.histories = []\n",
        "        self._build_models()\n",
        "\n",
        "    def _build_models(self):\n",
        "        for encoder, decoder in zip(self.encoders, self.decoders):\n",
        "            common_input = Input(shape=self.input_shape)\n",
        "            output = decoder(encoder(common_input))\n",
        "            autoencoder = Model(common_input, output)\n",
        "            optimizer = self.optimizer_class(**self.optimizer_kwargs)\n",
        "            autoencoder.compile(loss=\"mse\", optimizer=optimizer)\n",
        "            self.models.append(autoencoder)\n",
        "\n",
        "    def train_models(self, train_data, train_labels, epochs, batch_size, validation_data=None,verbose=1):\n",
        "        self.histories = []\n",
        "        for model in self.models:\n",
        "            history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=validation_data,verbose=verbose)\n",
        "            self.histories.append(history.history)\n",
        "\n",
        "    def predict(self, test_data, model_to_predict=0, verbose=1):\n",
        "        if model_to_predict==0:\n",
        "            predictions = [model.predict(test_data, verbose=verbose) for model in self.models]\n",
        "            averaged_output = np.mean(predictions, axis=0)\n",
        "        else:\n",
        "            averaged_output = self.models[model_to_predict-1].predict(test_data)\n",
        "        return averaged_output\n",
        "\n",
        "    def summary(self):\n",
        "        for model in self.models:\n",
        "             print (model.summary())\n",
        "\n",
        "    def plot_learning_curves(self, datatype,fileheader):\n",
        "        plt.style.use(\"ggplot\")\n",
        "        plt.figure()\n",
        "\n",
        "        for i, history in enumerate(self.histories):\n",
        "            epochs = range(len(history['loss']))\n",
        "            plt.plot(epochs, history[\"loss\"], label=f\"train_loss_model_{i+1}\")\n",
        "            if 'val_loss' in history:\n",
        "                plt.plot(epochs, history[\"val_loss\"], label=f\"val_loss_model_{i+1}\")\n",
        "\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Epoch #\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend(loc=\"lower left\")\n",
        "        plt.savefig(f\"{datatype}/{fileheader}.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def save_models(self, directory, fileheader):\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        for i, model in enumerate(self.models):\n",
        "            model_path = os.path.join(directory, f\"{fileheader}_{i+1}.h5\")\n",
        "            model.save(model_path)\n",
        "            print(f\"[INFO] saved model {i+1} to {model_path}\")\n",
        "\n",
        "    def load_models(self, directory, fileheader, num_models, custom_objects=None):\n",
        "        self.models = []\n",
        "        for i in range(1, num_models+1):  #  number of models\n",
        "            model_path = os.path.join(directory, f\"{fileheader}_{i}.h5\")\n",
        "            print(f\"[INFO] loading model {i} from {model_path}\")\n",
        "            model = load_model(model_path, custom_objects=custom_objects)\n",
        "            self.models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SreQx9k6HDg"
      },
      "source": [
        "## Compile Autoencoder for Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:21:59.419909Z",
          "start_time": "2024-07-17T11:21:59.034735Z"
        },
        "id": "6_N-24C_6HDh"
      },
      "outputs": [],
      "source": [
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "#tmax,jmax,imax,kmax = np.shape(temp_img_array_list)\n",
        "tmax,jmax,imax,kmax = (9752, 80, 96, 1)\n",
        "zmax = 1\n",
        "print (tmax,jmax,imax,kmax)\n",
        "\n",
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax,jmax,zmax, filters=(32, 64, 128), latentDim=64)\n",
        "elif domain==\"USA\":\n",
        "    (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64), latentDim=32)\n",
        "elif domain==\"EU\":\n",
        "   # (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64,128), latentDim=32)\n",
        "   # (encoder, decoder, autoencoder) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32), latentDim=64)\n",
        "    (encoder_1, decoder_1, autoencoder_1) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32), latentDim=32, suffix=\"1\")\n",
        "    (encoder_2, decoder_2, autoencoder_2) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64), latentDim=32, suffix=\"2\")\n",
        "   #(encoder_3, decoder_3, autoencoder_3) = ConvAutoencoder.build(imax, jmax, zmax, filters=(32,64,128), latentDim=64, suffix=\"3\")\n",
        "    (encoder_3, decoder_3, autoencoder_3) = ConvAutoencoder.build(imax, jmax, zmax, filters=(16,32,64), latentDim=64, suffix=\"3\")\n",
        "\n",
        "encoders = [encoder_1,encoder_2,encoder_3]\n",
        "decoders = [decoder_1,decoder_2,decoder_3]\n",
        "input_shape = (jmax,imax,kmax)\n",
        "\n",
        "# opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "#opt = Adam(learning_rate = INIT_LR,weight_decay = INIT_LR/EPOCHS) #updated version\n",
        "#opt = Adam(learning_rate = INIT_LR,weight_decay = INIT_LR/EPOCHS) #updated version\n",
        "optimizer_class = Adam\n",
        "optimizer_kwargs = {\"learning_rate\": INIT_LR, \"weight_decay\": INIT_LR / EPOCHS}\n",
        "\n",
        "\n",
        "# combined autoencoders\n",
        "autoencoder = AutoencoderEnsemble(encoders, decoders,input_shape,optimizer_class, optimizer_kwargs)\n",
        "print(autoencoder.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19IerV5S6HDh"
      },
      "source": [
        "For JP, (latitude, longitude, buffer) = (96,96,1) data is converted to (64), then restored to (96,96,1). <br>\n",
        "For USA, (latitude, longitude, buffer) = (32,52,1) data is converted to (32), then restored to (32,52,1)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09X9Ov196HDh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Training Autoencoder (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:44:31.878719Z",
          "start_time": "2024-07-17T11:21:59.421154Z"
        },
        "id": "uzw3QnTNq-lz"
      },
      "outputs": [],
      "source": [
        "autoencoder.train_models(train_data, train_data, EPOCHS,  BS, validation_data=(test_data,test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T11:44:32.390260Z",
          "start_time": "2024-07-17T11:44:31.880590Z"
        },
        "id": "6PsIm-Myq-lz"
      },
      "outputs": [],
      "source": [
        "fileheader=\"plot_mse\"\n",
        "autoencoder.plot_learning_curves(datatype,fileheader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueRKWok1q-lz"
      },
      "source": [
        "## Compile Autoencoder for Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:45:13.779652Z",
          "start_time": "2024-07-17T13:45:13.511646Z"
        },
        "id": "KJzi-tEfq-lz"
      },
      "outputs": [],
      "source": [
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "#tmax2,jmax2,imax2,kmax2 = np.shape(temp_img_array_list2_)\n",
        "tmax2,jmax2,imax2,kmax2 = (292560,80,96,1)\n",
        "zmax2 = 1\n",
        "print (tmax2,jmax2,imax2,kmax2)\n",
        "\n",
        "if domain==\"JP\":\n",
        "    if grid==\"HGRID\":\n",
        "        (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2,jmax2,zmax2, filters=(32, 64, 128), latentDim=64)\n",
        "elif domain==\"USA\":\n",
        "    (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32)\n",
        "elif domain==\"EU\":\n",
        "   # (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64,128), latentDim=32) # may need changes here// ?\n",
        "   # (encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32), latentDim=64) # may need changes here// ?\n",
        "   #(encoder2, decoder2, autoencoder2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32) # may need changes here// ?\n",
        "    (encoder2_1, decoder2_1, autoencoder2_1) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32), latentDim=32, suffix=\"1\")\n",
        "    (encoder2_2, decoder2_2, autoencoder2_2) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64), latentDim=32, suffix=\"2\")\n",
        "   #(encoder2_3, decoder2_3, autoencoder2_3) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(32,64,128), latentDim=64, suffix=\"3\")\n",
        "    (encoder2_3, decoder2_3, autoencoder2_3) = ConvAutoencoder.build(imax2, jmax2, zmax2, filters=(16,32,64), latentDim=64, suffix=\"3\")\n",
        "\n",
        "encoders2 = [encoder2_1,encoder2_2,encoder2_3]\n",
        "decoders2 = [decoder2_1,decoder2_2,decoder2_3]\n",
        "input_shape2 = (jmax2,imax2,kmax2)\n",
        "\n",
        "optimizer_class2 = Adam\n",
        "optimizer_kwargs2 = {\"learning_rate\": INIT_LR, \"weight_decay\": INIT_LR / EPOCHS}\n",
        "\n",
        "autoencoder2 = AutoencoderEnsemble(encoders2, decoders2, input_shape2, optimizer_class2, optimizer_kwargs2)\n",
        "print(autoencoder2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ZvqjlAgmq-lz"
      },
      "source": [
        "## Training Autoencoder (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-18T04:05:37.310893Z",
          "start_time": "2024-07-17T13:45:16.003378Z"
        },
        "id": "23EjqtTvq-lz"
      },
      "outputs": [],
      "source": [
        "autoencoder2.train_models(train_data2, train_data2, EPOCHS,  BS, validation_data=(test_data2,test_data2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-18T04:05:38.562072Z",
          "start_time": "2024-07-18T04:05:37.471778Z"
        },
        "id": "HibNk8jJq-lz"
      },
      "outputs": [],
      "source": [
        "fileheader=\"plot_model_mse\"\n",
        "autoencoder2.plot_learning_curves(datatype,fileheader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdyETCC26HDi"
      },
      "source": [
        "â€Loss functionâ€is a fancy mathematical term for an object that measures how often a model makes an incorrect prediction. In the context of classification, they measure how often a model misclassifies members of different groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoUc3iOW6HDi",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Save Temporal Data (Autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:37.948806Z",
          "start_time": "2024-07-17T13:21:37.750948Z"
        },
        "id": "SdFDJEJ56HDi"
      },
      "outputs": [],
      "source": [
        "# serialize the autoencoder model to disk\n",
        "print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder.save_models(\"%s/output\" % (datatype), \"autoencoder.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-18T04:05:40.122746Z",
          "start_time": "2024-07-18T04:05:38.563170Z"
        },
        "id": "aUIn4o0lq-lz"
      },
      "outputs": [],
      "source": [
        "# serialize the autoencoder model to disk\n",
        "print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder2.save_models(\"%s/output_model\" % (datatype), \"autoencoder_model.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3cTh8n56HDi"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-18T04:05:40.126974Z",
          "start_time": "2024-07-18T04:05:40.124135Z"
        },
        "id": "Xk8PB7ORq-l0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "custom_objects = {'mse': MeanSquaredError()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:43.310585Z",
          "start_time": "2024-07-17T13:21:43.072224Z"
        },
        "id": "Zf9TVbXf6HDi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load the model and image data from disk\n",
        "print(\"[INFO] loading autoencoder and image data...\")\n",
        "num_models=len(encoders)\n",
        "autoencoder.load_models(\"%s/output\" % (datatype), \"autoencoder.model\", num_models, custom_objects=custom_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-18T04:05:40.999475Z",
          "start_time": "2024-07-18T04:05:40.128073Z"
        },
        "id": "Oue4N0Uhq-l0"
      },
      "outputs": [],
      "source": [
        "# load the model and image data from disk\n",
        "print(\"[INFO] loading autoencoder and image data...\")\n",
        "num_models2=len(encoders2)\n",
        "autoencoder2.load_models(\"%s/output_model\" % (datatype), \"autoencoder_model.model\", num_models2, custom_objects=custom_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "AE3_X1y4q-l0"
      },
      "source": [
        "# Check autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXSLgp0Eq-l0"
      },
      "source": [
        "## Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:24:36.405814Z",
          "start_time": "2024-07-17T13:21:47.243139Z"
        },
        "id": "2A7icjSlq-l0"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2 = np.random.randint(0,len(temp_img_array_list), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idx2:\n",
        "\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list[i]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "   #decoded = autoencoder_2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVV-hOMmq-l0"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.462272Z",
          "start_time": "2024-07-17T13:21:32.462265Z"
        },
        "id": "iDYpNZt3q-l0"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list[0,:]), 5)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates2[i])\n",
        "    date = dates2[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_model/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates2[ii].year,dates2[ii].month,dates2[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibAdtKDw6HDi",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Compute Mean Square Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl1oY7wbq-l0"
      },
      "source": [
        "## Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:25:52.729834Z",
          "start_time": "2024-07-17T13:25:52.726007Z"
        },
        "id": "NYdPTt-46HDj"
      },
      "outputs": [],
      "source": [
        "def comp_error(images, autoencoder, verbose=1):\n",
        "    tmax12_,jmax12_,imax12_,kmax12_ = np.shape(images)\n",
        "    errors_ = np.zeros(tmax12_)\n",
        "   #recons_ = autoencoder.predict(images, verbose=verbose)\n",
        "    recons_ = autoencoder.predict(images)\n",
        "    for tt in range(tmax12_):\n",
        "        errors_[tt] = np.mean((images[tt] - recons_[tt])**2)\n",
        "        if tt%1000==0:\n",
        "            print (\"%i/%i %f\" % (tt+1,tmax12_,errors_[tt]))\n",
        "    del(recons_)\n",
        "    return errors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:26:04.634542Z",
          "start_time": "2024-07-17T13:25:53.607730Z"
        },
        "id": "YCO9JcFu6HDj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "errors = comp_error(temp_img_array_list, autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:26:04.643545Z",
          "start_time": "2024-07-17T13:26:04.636905Z"
        },
        "id": "hi4D_2mI6HDj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# saving error to disk\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors, \"%s/output/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:26:04.650011Z",
          "start_time": "2024-07-17T13:26:04.644487Z"
        },
        "id": "uRKyq3vG6HDj"
      },
      "outputs": [],
      "source": [
        "# load error from disk\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors = pickle_load(\"%s/output/error.pickle\" % (datatype))\n",
        "errors = np.array(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:26:04.665286Z",
          "start_time": "2024-07-17T13:26:04.651119Z"
        },
        "id": "czouYY6x6HDj"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh = np.quantile(errors, 0.99)\n",
        "idxs = np.where(np.array(errors) >= thresh)[0]\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNJjgAyOq-l0"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.465968Z",
          "start_time": "2024-07-17T13:21:32.465961Z"
        },
        "id": "3Ov4Oe5hq-l0"
      },
      "outputs": [],
      "source": [
        "emax12_,tmax12_,jmax12_,imax12_,kmax12_ = np.shape(temp_img_array_list2)\n",
        "errors2_ = comp_error(temp_img_array_list2_, autoencoder2)\n",
        "errors_model = errors2_.reshape((emax12_,tmax12_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.466620Z",
          "start_time": "2024-07-17T13:21:32.466613Z"
        },
        "id": "lBz1D5buq-l0"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_model, \"%s/output_model/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.467100Z",
          "start_time": "2024-07-17T13:21:32.467093Z"
        },
        "id": "NqpYoiXbq-l0"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_model = pickle_load(\"%s/output_model/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.467703Z",
          "start_time": "2024-07-17T13:21:32.467696Z"
        },
        "id": "fdMX9Ez4q-l0"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "thresh2 = np.quantile(errors_model.flatten(), 0.99)\n",
        "idxs_model = np.where(np.array(errors_model) >= thresh2)\n",
        "idxs_model = np.array(idxs_model)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_model[0,:])))\n",
        "thresh2, np.shape(idxs_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F93MUEj6HDk"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W2rRQ5P6HDk",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Plot Each Anomalous Day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s70kS-A36HDk"
      },
      "source": [
        "Drawing anomalous eents (Left panel is the original data, right panel is the restored data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Ou4Y_7q-l0"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:28:55.982627Z",
          "start_time": "2024-07-17T13:26:12.444939Z"
        },
        "id": "-myOzQm56HDk"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idxs[0:5]:\n",
        "\n",
        "    print (dates[i])\n",
        "    date = dates[i]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list[i]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list[i], axis=0)\n",
        "    decoded = autoencoder.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly/fig_%4.4i_%2.2i_%2.2i.png\" % (datatype,dates[i].year,dates[i].month,dates[i].day))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NMwthJGq-l1"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.468945Z",
          "start_time": "2024-07-17T13:21:32.468937Z"
        },
        "id": "h0dzqNKCq-l1"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "pmax, kmax = np.shape(idxs_model)\n",
        "idx2a = np.random.randint(0,kmax,10) # display 10 sample pnly\n",
        "\n",
        "#for i in idxs:\n",
        "for i in idx2a:\n",
        "\n",
        "    ee = idxs_model[0,i]\n",
        "    tt = idxs_model[1,i]\n",
        "\n",
        "    print (ee,tt,dates2[tt])\n",
        "\n",
        "    date = dates2[tt]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ee,tt]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list2[ee,tt], axis=0)\n",
        "\n",
        "    decoded = autoencoder.predict(image2, verbose=0) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i ee%2.2i\" % (date.year,date.month,date.day,ee+1), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i ee%2.2i\" % (date.year,date.month,date.day,ee+1), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_model/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,date.year,date.month,date.day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upFfqs5H6HDl"
      },
      "source": [
        "Special events for USA<br>\n",
        "  - [List of Floods Events](https://en.wikipedia.org/wiki/Floods_in_the_United_States_(2000%E2%80%93present))<br>\n",
        "  - [List of Hurricane Landfalling Events](https://en.wikipedia.org/wiki/List_of_United_States_hurricanes)<br>\n",
        "    - 2016-08-21: Southern Louisiana tropical disturbance\n",
        "    - 2017-08-29: Hurricane Hervey (Texas)\n",
        "    - 2018-09-17: Hurricane Florence (North Calorina)\n",
        "    - 2021-08-20: Hurricane Ida (New Jersey - New York)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw9uw5YF6HDl",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Draw Composite of Anomalous Days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "bQtyNZjDq-l1"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:29:01.343087Z",
          "start_time": "2024-07-17T13:28:55.984465Z"
        },
        "id": "IhgHRDMg6HDl"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list[idxs[:]]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "image2 =temp_img_array_list[idxs[:]]\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "recon = recon*(maxvalue-minvalue)+minvalue\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "axs.set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs[:]))/float(len(dates[:]))*100.0\n",
        "ddays = len(idxs[:])\n",
        "\n",
        "plt.title(\"%.1f%%, Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OppA6QYtq-l1"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.470096Z",
          "start_time": "2024-07-17T13:21:32.470089Z"
        },
        "id": "fQk4T4Ocq-l1"
      },
      "outputs": [],
      "source": [
        "# Show composite of anomaly pattern (Obs)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "ees = idxs_model[0,:]\n",
        "tts = idxs_model[1,:]\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10)) # set figure environemnt\n",
        "\n",
        "# grab the original image and reconstructed image\n",
        "original = temp_img_array_list2[ees,tts]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "\n",
        "image2 =temp_img_array_list2[ees,tts]\n",
        "\n",
        "decoded = autoencoder.predict(image2, verbose=0)\n",
        "\n",
        "recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "recon = recon*(maxvalue-minvalue)+minvalue\n",
        "original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(10,10))\n",
        "\n",
        "axs.set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "axs.coastlines()\n",
        "axs.add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "axs.add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "axs.set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "axs.set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "latfmt=LatitudeFormatter()\n",
        "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "axs.xaxis.set_major_formatter(lonfmt)\n",
        "axs.yaxis.set_major_formatter(latfmt)\n",
        "axs.set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "\n",
        "cs = axs.contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "axs.set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "\n",
        "frac = float(len(idxs_model[1,:]))/float(len(dates2[:]))/float(emax)*100.0\n",
        "ddays = len(idxs_model[1,:])\n",
        "\n",
        "plt.title(\"%.1f%%, Total %i days\" % (frac, ddays), fontsize=20)\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.05, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly_model\" % (datatype)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJI37STy6HDl"
      },
      "source": [
        "## Time Series of observed anomalous days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:29:01.492380Z",
          "start_time": "2024-07-17T13:29:01.344474Z"
        },
        "id": "K1IXzzS36HDl"
      },
      "outputs": [],
      "source": [
        "# Time series for anomalous days\n",
        "adates = dates[idxs]\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# derive (years,frequency of anomaly)\n",
        "alldata = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "alldata.to_json(\"%s/timeseries.json\" % (datatype))\n",
        "\n",
        "# Plot\n",
        "years = alldata.index\n",
        "alldata = alldata.to_numpy()\n",
        "plt.plot(years, alldata, \"o-\", color=\"r\")\n",
        "\n",
        "h, pval = mk_test(alldata, alpha=0.05)\n",
        "print (h,pval)\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,alldata)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days (Pvalue=%.2f)\" % (pval))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries.png\" % (datatype))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJR23NHtq-l1"
      },
      "outputs": [],
      "source": [
        "# Time series for anomalous days\n",
        "adates = dates[idxs]\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# derive (years,frequency of anomaly)\n",
        "alldata = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "alldata.to_json(\"%s/timeseries.json\" % (datatype))\n",
        "\n",
        "# Plot\n",
        "years = alldata.index\n",
        "alldata = alldata.to_numpy()\n",
        "plt.plot(years, alldata, \"o-\", color=\"r\")\n",
        "\n",
        "h, pval = mk_test(alldata, alpha=0.05)\n",
        "print (h,pval)\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,alldata)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days (Pvalue=%.2f)\" % (pval))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries.png\" % (datatype))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OlbeGgpq-l1"
      },
      "outputs": [],
      "source": [
        "obs =np.array(aa)\n",
        "obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:29:01.789124Z",
          "start_time": "2024-07-17T13:29:01.493802Z"
        },
        "id": "Sp_2U3uCq-l1"
      },
      "outputs": [],
      "source": [
        "# Observed Monthly plot\n",
        "\n",
        "adates = dates[idxs]\n",
        "\n",
        "#print (adates.dt.year.value_counts())\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "ax1 = plt.subplot(2,1,1)\n",
        "\n",
        "# derive (years,frequency of anomaly)\n",
        "alldata = adates.dt.month.value_counts().reindex(dates.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "years_ = sorted(dates.dt.year.unique())\n",
        "\n",
        "# Plot\n",
        "months_ = alldata.index\n",
        "alldata = alldata.to_numpy()/float(years_[-1]-years_[0]+1)\n",
        "plt.plot(months_, alldata, \"o-\", color=\"r\")\n",
        "\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Trend\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "\n",
        "counts_ = adates.groupby([adates.dt.year, adates.dt.month]).agg({'count'})\n",
        "dates_ = counts_.index\n",
        "vals_ = counts_.values.squeeze()\n",
        "\n",
        "data_ = np.ma.zeros((len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros(len(months_))\n",
        "pval_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for date_, count_ in zip(dates_,vals_):\n",
        "    year_ = date_[0]\n",
        "    month_ = date_[1]\n",
        "    tt = year_ - years_[0]\n",
        "    im = month_ - months_[0]\n",
        "    data_[tt,im]=count_\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,im]\n",
        "    xx_,yy_,trend_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "plt.plot(months_, trend_, \"-\", color=\"red\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_[im] <= 0.05:\n",
        "        plt.plot(months_[im], trend_[im], \"o\", color=\"red\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months_[im], trend_[im], \"o\", color=\"black\", ms=12)\n",
        "plt.axhline(y=0, ls=\"--\", color=\"black\")\n",
        "\n",
        "plt.title(\"Trend in Abnormal Days for Each Month [days per year]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Y1ZNXeq-l1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.471797Z",
          "start_time": "2024-07-17T13:21:32.471789Z"
        },
        "id": "EsoFF5Zbq-l1"
      },
      "outputs": [],
      "source": [
        "# Model Time Series\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "#for ee in range(30):\n",
        "#for ee in range(30,61,1):\n",
        "  idx9 = np.where(idxs_model[0,:]==ee)\n",
        "  idx9 = np.array(idx9).flatten()\n",
        "  adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "  temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "  years = temp.index\n",
        "\n",
        "  temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], temps[:,:-1].min(axis=0), temps[:,:-1].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:-1], temps[:,:-1].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "else:\n",
        "    plt.fill_between(years[:], temps[:,:].min(axis=0), temps[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:], temps[:,:].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "# Save timeseries\n",
        "df_ = pd.DataFrame(data=np.swapaxes(temps,0,1), index=years, columns=np.arange(1,emax+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_model.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "h, pval = mk_test(temps[:,:].mean(axis=0), alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,temps[:,:].mean(axis=0))\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PxK2WSyq-l1"
      },
      "outputs": [],
      "source": [
        "# Model Time Series\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "#for ee in range(30):\n",
        "#for ee in range(30,61,1):\n",
        "  idx9 = np.where(idxs_model[0,:]==ee)\n",
        "  idx9 = np.array(idx9).flatten()\n",
        "  adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "  temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "  years = temp.index\n",
        "\n",
        "  temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], temps[:,:-1].min(axis=0), temps[:,:-1].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:-1], temps[:,:-1].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "else:\n",
        "    plt.fill_between(years[:], temps[:,:].min(axis=0), temps[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "    plt.plot(years[:], temps[:,:].mean(axis=0), \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "# Save timeseries\n",
        "df_ = pd.DataFrame(data=np.swapaxes(temps,0,1), index=years, columns=np.arange(1,emax+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_model.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "h, pval = mk_test(temps[:,:].mean(axis=0), alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,temps[:,:].mean(axis=0))\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ott_hgkDq-l1"
      },
      "outputs": [],
      "source": [
        "trends_model = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,temps[e])\n",
        "    trends_model.append(aa)\n",
        "trends_model =np.array(trends_model)\n",
        "trends_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWUKQKNdq-l1"
      },
      "outputs": [],
      "source": [
        "trends_model.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.472443Z",
          "start_time": "2024-07-17T13:21:32.472436Z"
        },
        "id": "UrFLHI1Wq-l1"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax):\n",
        "    idx9 = np.where(idxs_model[0,:]==ee)\n",
        "    idx9 = np.array(idx9).flatten()\n",
        "    adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "    temp = adates.dt.month.value_counts().reindex(dates2.dt.month.unique(),fill_value=0).sort_index()\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "\n",
        "years_ = sorted(dates2.dt.year.unique())\n",
        "months_ = sorted(dates2.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax,len(months_)))\n",
        "pval_ = np.ma.zeros((emax,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax):\n",
        "    idx9 = np.where(idxs_model[0,:]==ee)\n",
        "    idx9 = np.array(idx9).flatten()\n",
        "    adates = dates2[idxs_model[1,idx9]]\n",
        "\n",
        "    counts_ = adates.groupby([adates.dt.year, adates.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = year_ - years_[0]\n",
        "        im = month_ - months_[0]\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[im], trend_ensmean_[im], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[im], trend_ensmean_[im], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_model.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5CkJJW6HDm",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Regional Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:42:37.830931Z",
          "start_time": "2024-07-17T13:42:37.713698Z"
        },
        "id": "9gxUlqgCq-l1"
      },
      "outputs": [],
      "source": [
        "max_regs = np.zeros(len(idxs))\n",
        "\n",
        "for ii,i in enumerate(idxs):\n",
        "\n",
        "    original = temp_img_array_list[i]*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "   #max_regs[ii] = regional_stats(jreg_uids, jreg, original2, kind=\"mean\")\n",
        "    max_regs[ii] = regional_stats(jreg_uids, jreg, original2, kind=\"max\")\n",
        "   #print (ii,i,max_reg1[ii])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9te0br9Nq-l2"
      },
      "source": [
        "### Composite (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHLZvNtMq-l2"
      },
      "outputs": [],
      "source": [
        "nregs = len(jreg_uids)\n",
        "\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "fig, axs = plt.subplots(nrows=math.ceil(nregs/2), ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(20,15))\n",
        "pcol=0\n",
        "prow=0\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    idx = np.where(max_regs==cl)\n",
        "\n",
        "    original = temp_img_array_list[idxs[idx]]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 =temp_img_array_list[idxs[idx]]\n",
        "    decoded = autoencoder.predict(image2)\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].coastlines(resolution=\"50m\")\n",
        "    axs[prow,pcol].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[prow,pcol].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[prow,pcol].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[prow,pcol].contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "\n",
        "\n",
        "    frac = float(len(idxs[idx]))/float(len(idxs[:]))*100.0\n",
        "\n",
        "    ddays = len(idxs[idx])\n",
        "    axs[prow,pcol].set_title(\"%s (%.1f%%, Total %i days)\" % (sregnames[cl], frac, ddays), fontsize=20)\n",
        "\n",
        "    pcol=pcol+1\n",
        "    if pcol==2:\n",
        "        pcol=0\n",
        "        prow=prow+1\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.0, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"%s/anomaly/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_cluster_composite.png\" % (outdir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_wfoNyq-l2"
      },
      "source": [
        "### Time Series (Observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:43:23.979825Z",
          "start_time": "2024-07-17T13:43:22.648692Z"
        },
        "id": "9XdYbaDG6HDn"
      },
      "outputs": [],
      "source": [
        "#print (adates.dt.year.value_counts()) Obs\n",
        "\n",
        "plt.figure(0, figsize=(12,10))\n",
        "\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    plt.subplot(math.ceil(nregs/2), 2, cl)\n",
        "    idx2 = np.where(max_regs==cl)\n",
        "\n",
        "    adates = dates[idxs[idx2]]\n",
        "\n",
        "    temp = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "    data = np.array(temp.tolist())\n",
        "\n",
        "    plt.plot(years, data, \"o-\", color=\"red\", label=\"Cluster%i\" % (cl+1))\n",
        "\n",
        "   # Mann and Kendall singnificant test for trend\n",
        "    h, pval = mk_test(data, alpha=0.05)\n",
        "\n",
        "    if pval <=0.05:\n",
        "        xx, yy, aa = linreg(years,data)\n",
        "        plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "        plt.title(\"%s (Trend=%.5f, Trend P-Val=%.2f)\" % (sregnames[cl],aa, pval))\n",
        "    else:\n",
        "        plt.title(\"%s (Trend P-Val=%.2f)\" % (sregnames[cl], pval))\n",
        "\n",
        "outdir = \"%s/anomaly/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"%s/timeseries_regional%i_each.png\" % (outdir,nregs))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJjJ0NhRq-l2"
      },
      "source": [
        "### Composite (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.474792Z",
          "start_time": "2024-07-17T13:21:32.474785Z"
        },
        "id": "LGIDirjbq-l2"
      },
      "outputs": [],
      "source": [
        "pmax, kmax = np.shape(idxs_model)\n",
        "max_regs_model = np.zeros(kmax)\n",
        "\n",
        "for i in range(kmax):\n",
        "\n",
        "    ee = idxs_model[0,i]\n",
        "    tt = idxs_model[1,i]\n",
        "\n",
        "    original = temp_img_array_list2[ee,tt]*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "   #max_regs_model[i] = regional_stats(jreg_uids, jreg, original2, kind=\"mean\")\n",
        "    max_regs_model[i] = regional_stats(jreg_uids, jreg, original2, kind=\"max\")\n",
        "    if i%1000==0:\n",
        "        print (\"%i/%i max=%i\" % (i+1,kmax,max_regs_model[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.475415Z",
          "start_time": "2024-07-17T13:21:32.475408Z"
        },
        "id": "50lQN4A8q-l2"
      },
      "outputs": [],
      "source": [
        "# Show composite of each cluster pattern (Model)\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "#draw_tc=True\n",
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "fig, axs = plt.subplots(nrows=math.ceil(nregs/2), ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=(20,15))\n",
        "\n",
        "pcol=0\n",
        "prow=0\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    print (\"plotting region\",cl)\n",
        "\n",
        "    idx = np.where(max_regs_model==cl)\n",
        "\n",
        "    idx = np.array(idx).flatten()\n",
        "\n",
        "    if len(idx)==0:\n",
        "        continue\n",
        "\n",
        "    ees = idxs_model[0,idx]\n",
        "    tts = idxs_model[1,idx]\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list2[ees,tts]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "#    temp_img_array_list = np.pad(temp_img_array_list, pad_width=((0,0),(1,2),(1,1)), mode='constant',constant_values=0)\n",
        "\n",
        "    image2 =temp_img_array_list2[ees,tts]\n",
        "    #image2 = image2[0,1:-2,1:-1,0] # pad -> original\n",
        "    decoded = autoencoder2.predict(image2, verbose=0)\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid,multidata=True)\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid,multidata=True)\n",
        "\n",
        "    axs[prow,pcol].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].coastlines(resolution=\"50m\")\n",
        "    axs[prow,pcol].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[prow,pcol].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[prow,pcol].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[prow,pcol].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[prow,pcol].yaxis.set_major_formatter(latfmt)\n",
        "    cs = axs[prow,pcol].contourf(lons[:],lats[:],original2.mean(axis=0), contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[prow,pcol].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "\n",
        "    #m.readshapefile('st99_d00', name='states', drawbounds=True)\n",
        "    frac = float(len(idx))/float(len(max_regs_model[:]))*100.0\n",
        "    ddays = len(idx)\n",
        "\n",
        "    ddays = ddays/float(emax) # normalized by emax\n",
        "    axs[prow,pcol].set_title(\"%s (%.1f%%, Total %i days)\" % (sregnames[cl], frac, ddays), fontsize=20)\n",
        "\n",
        "    pcol=pcol+1\n",
        "    if pcol==2:\n",
        "        pcol=0\n",
        "        prow=prow+1\n",
        "\n",
        "# add legend\n",
        "cax = fig.add_axes([0.2, 0.0, 0.6, 0.01])\n",
        "\n",
        "art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "art.ax.tick_params(labelsize=18)\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "outdir = \"./%s/anomaly_model/regional%2.2i\" % (datatype, nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "fig.savefig(\"%s/fig_regional_composite.png\" % (outdir))\n",
        "print (datatype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ksgyBfIq-l2"
      },
      "source": [
        "### Time Series (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.475949Z",
          "start_time": "2024-07-17T13:21:32.475942Z"
        },
        "id": "5tWRs-uSq-l2"
      },
      "outputs": [],
      "source": [
        "#print (adates.dt.year.value_counts()) Obs\n",
        "nregs = len(jreg_uids)\n",
        "\n",
        "plt.figure(0, figsize=(10,8))\n",
        "\n",
        "for cl in jreg_uids:\n",
        "\n",
        "    plt.subplot(math.ceil(nregs/2), 2, cl)\n",
        "\n",
        "    cl2=cl\n",
        "    idx2a = np.where(max_regs_model==cl)\n",
        "    idx2b = idxs_model[1,idx2a].squeeze()\n",
        "    adates = dates2[idx2b]\n",
        "\n",
        "    temp = adates.dt.year.value_counts().reindex(dates.dt.year.unique(),fill_value=0).sort_index()\n",
        "    if season==\"DJF\":\n",
        "        years = temp.index[:-1]\n",
        "        data = np.array(temp.tolist()[:-1])/float(emax)\n",
        "    else:\n",
        "        years = temp.index\n",
        "        data = np.array(temp.tolist())/float(emax)\n",
        "\n",
        "    allens=[]\n",
        "    for ee in range(emax):\n",
        "        idx2a = np.where((max_regs_model==cl)&(idxs_model[0,:]==ee))\n",
        "        idx2b = idxs_model[1,idx2a].squeeze()\n",
        "        adates = dates2[idx2b]\n",
        "\n",
        "        try:\n",
        "            temp = adates.dt.year.value_counts().reindex(dates2.dt.year.unique(),fill_value=0).sort_index()\n",
        "            years = temp.index\n",
        "            allens.append(temp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    allens=np.array(allens)\n",
        "    allens2_list = []\n",
        "\n",
        "    for ee in range(emax):\n",
        "        temp1_ = pd.DataFrame({'year':years,\"ens\":ee+1,\"cl\":cl, \"freq\":allens[ee,:]})\n",
        "        allens2_list.append(temp1_)\n",
        "    allens2 = pd.concat(allens2_list, ignore_index=True, sort=False)\n",
        "\n",
        "    if season==\"DJF\":\n",
        "        plt.fill_between(years[:-1], allens[:,:-1].min(axis=0), allens[:,:-1].max(axis=0), color=\"red\", alpha=0.4)\n",
        "    else:\n",
        "        plt.fill_between(years[:], allens[:,:].min(axis=0), allens[:,:].max(axis=0), color=\"red\", alpha=0.4)\n",
        "\n",
        "    plt.plot(years, data, \"o-\", color=\"red\", label=\"%s\" % (sregnames[cl]))\n",
        "\n",
        "   # Mann and Kendall singnificant test for trend\n",
        "    h, pval = mk_test(data, alpha=0.05)\n",
        "\n",
        "    if pval <=0.05:\n",
        "        xx, yy, aa = linreg(years,data)\n",
        "        plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "        plt.title(\"%s (Trend=%.5f, Trend P-Val=%.2f)\" % (sregnames[cl],aa, pval))\n",
        "    else:\n",
        "        plt.title(\"%s (Trend P-Val=%.2f)\" % (sregnames[cl], pval))\n",
        "\n",
        "outdir = \"./%s/anomaly_model/regional%2.2i\" % (datatype,nregs)\n",
        "if not os.path.exists(outdir):\n",
        "    os.makedirs(outdir)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"%s/timeseries_regional%i_each.png\" % (outdir,nregs))\n",
        "print (datatype)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Save file\n",
        "#alldata2.to_json(\"%s/timeseries_cluster%2.2i_each.json\" % (outdir,ncluster))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlq-Er8Hq-l3"
      },
      "source": [
        "# Test (Future) data (SSP 5-8.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487480Z",
          "start_time": "2024-07-17T13:21:32.487473Z"
        },
        "scrolled": true,
        "id": "iORqiEDbq-l3"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list3)\n",
        "errors3_ = comp_error(temp_img_array_list3.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_test = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487993Z",
          "start_time": "2024-07-17T13:21:32.487986Z"
        },
        "id": "SBCzAW8jq-l3"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_test/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.488545Z",
          "start_time": "2024-07-17T13:21:32.488537Z"
        },
        "id": "W48HUkiRq-l3"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_test = pickle_load(\"%s/output_test/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489131Z",
          "start_time": "2024-07-17T13:21:32.489123Z"
        },
        "id": "BfQhMUStq-l3"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "#thresh2 = np.quantile(errors_model.flatten(), 0.95)\n",
        "idxs_test = np.where(np.array(errors_test) >= thresh2)\n",
        "idxs_test = np.array(idxs_test)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_test[0,:])))\n",
        "thresh2, np.shape(idxs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489684Z",
          "start_time": "2024-07-17T13:21:32.489677Z"
        },
        "id": "eDzmRuT_q-l3"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list3[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490244Z",
          "start_time": "2024-07-17T13:21:32.490237Z"
        },
        "id": "0yGYYsnRq-l3"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jXR36xLq-l3"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490922Z",
          "start_time": "2024-07-17T13:21:32.490915Z"
        },
        "id": "ymAdih3Rq-l4"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKK9yZDCq-l4"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.491497Z",
          "start_time": "2024-07-17T13:21:32.491490Z"
        },
        "id": "4dMsGyO3q-l4"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fXDtf5Sq-l4"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492055Z",
          "start_time": "2024-07-17T13:21:32.492048Z"
        },
        "id": "ZKKARiJKq-l4"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHZvHCaQq-l4"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SvXTvsEq-l4"
      },
      "outputs": [],
      "source": [
        "trends_future = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,alldata2[e])\n",
        "    trends_future.append(aa)\n",
        "trends_future =np.array(trends_future)\n",
        "trends_future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nQZWX96q-l4"
      },
      "outputs": [],
      "source": [
        "np.shape(alldata2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM18QoPtq-l4"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492630Z",
          "start_time": "2024-07-17T13:21:32.492623Z"
        },
        "id": "dUR4REuQq-l4"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqlvZ6Pgq-l4"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.493210Z",
          "start_time": "2024-07-17T13:21:32.493204Z"
        },
        "id": "uinyugX6q-l4"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "output_nat\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rEhcuvq-l5"
      },
      "source": [
        "# Test (Future) data (SSP 1-1.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487480Z",
          "start_time": "2024-07-17T13:21:32.487473Z"
        },
        "scrolled": true,
        "id": "I2mYxC7Dq-l5"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list119)\n",
        "errors3_ = comp_error(temp_img_array_list119.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_test = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487993Z",
          "start_time": "2024-07-17T13:21:32.487986Z"
        },
        "id": "RtzXc14Iq-l5"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_119/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.488545Z",
          "start_time": "2024-07-17T13:21:32.488537Z"
        },
        "id": "BpIHuzgEq-l5"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_119= pickle_load(\"%s/output_119/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489131Z",
          "start_time": "2024-07-17T13:21:32.489123Z"
        },
        "id": "l6G7KwvTq-l5"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "#thresh2 = np.quantile(errors_model.flatten(), 0.95)\n",
        "idxs_119 = np.where(np.array(errors_119) >= thresh2)\n",
        "idxs_119 = np.array(idxs_119)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_119[0,:])))\n",
        "thresh2, np.shape(idxs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489684Z",
          "start_time": "2024-07-17T13:21:32.489677Z"
        },
        "id": "3EF28vREq-l5"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list119[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueldupPKq-l5"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3 = temp_img_array_list119"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490244Z",
          "start_time": "2024-07-17T13:21:32.490237Z"
        },
        "id": "nO8GjfACq-l5"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "zSRj_j3sq-l5"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490922Z",
          "start_time": "2024-07-17T13:21:32.490915Z"
        },
        "id": "qfIYA9PMq-l5"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    #print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "OcL8hoNnq-l5"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.491497Z",
          "start_time": "2024-07-17T13:21:32.491490Z"
        },
        "id": "NBbj_kbfq-l5"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5atBb1LVq-l5"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492055Z",
          "start_time": "2024-07-17T13:21:32.492048Z"
        },
        "id": "6sDvz0tIq-l5"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_119[0,:]==ee)\n",
        "    idxs_test4 = idxs_119[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "FYEg70-lq-l5"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492630Z",
          "start_time": "2024-07-17T13:21:32.492623Z"
        },
        "id": "TN7FOHVfq-l5"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIoHNaCq-l5"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.493210Z",
          "start_time": "2024-07-17T13:21:32.493204Z"
        },
        "id": "N8tyNa7Xq-l5"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZS7voeq-l6"
      },
      "source": [
        "# Test (Future) data (NatForc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487480Z",
          "start_time": "2024-07-17T13:21:32.487473Z"
        },
        "scrolled": true,
        "id": "m9WECCmXq-l6"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list4)\n",
        "errors3_ = comp_error(temp_img_array_list4.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_test = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487993Z",
          "start_time": "2024-07-17T13:21:32.487986Z"
        },
        "id": "GT6vQC5Iq-l6"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_nat/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.488545Z",
          "start_time": "2024-07-17T13:21:32.488537Z"
        },
        "id": "xMTIM8ePq-l6"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_test = pickle_load(\"%s/output_nat/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489131Z",
          "start_time": "2024-07-17T13:21:32.489123Z"
        },
        "id": "M42-TaXwq-l6"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "#thresh2 = np.quantile(errors_model.flatten(), 0.95)\n",
        "idxs_nat = np.where(np.array(errors_nat) >= thresh2)\n",
        "idxs_nat = np.array(idxs_nat)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_nat[0,:])))\n",
        "thresh2, np.shape(idxs_nat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489684Z",
          "start_time": "2024-07-17T13:21:32.489677Z"
        },
        "id": "TaDe_Bbaq-l6"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list4[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuDLwvW0q-l6"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3 = temp_img_array_list4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490244Z",
          "start_time": "2024-07-17T13:21:32.490237Z"
        },
        "id": "ZtDvMDkHq-l6"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOGiNxYrq-l6"
      },
      "outputs": [],
      "source": [
        "yyyymmdds3=[]\n",
        "for date3 in dates3[:]:\n",
        "    yyyymmdds3.append(int(\"%4.4i%2.2i%2.2i\" % (date3.year,date3.month,date3.day)))\n",
        "\n",
        "ds3 = pd.Series(yyyymmdds3)\n",
        "dates3 = pd.to_datetime(ds3, format='%Y%m%d')\n",
        "print (dates3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "zbY2L0W-q-l6"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490922Z",
          "start_time": "2024-07-17T13:21:32.490915Z"
        },
        "id": "n-Bg4CBFq-l6"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    #print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "si2xRYRZq-l6"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.491497Z",
          "start_time": "2024-07-17T13:21:32.491490Z"
        },
        "id": "MTVJPkffq-l7"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh8Uynltq-l7"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492055Z",
          "start_time": "2024-07-17T13:21:32.492048Z"
        },
        "id": "_jH84_Glq-l7"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_nat[0,:]==ee)\n",
        "    idxs_test4 = idxs_nat[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfAoiJ8lq-l7"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_nat[0,:]==ee)\n",
        "    idxs_test4 = idxs_nat[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaKLRkS1q-l7"
      },
      "outputs": [],
      "source": [
        "trends_nat = []\n",
        "for e in range(0,30,1):\n",
        "    xx,yy,aa=linreg(years,alldata2[e])\n",
        "    trends_nat.append(aa)\n",
        "trends_nat =np.array(trends_nat)\n",
        "trends_nat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrDs7vpcq-l7"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492630Z",
          "start_time": "2024-07-17T13:21:32.492623Z"
        },
        "id": "-43iJgKxq-l7"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uSzG_2Pq-l7"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.493210Z",
          "start_time": "2024-07-17T13:21:32.493204Z"
        },
        "id": "hVFfsljNq-l7"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4MZc3OO5q-l8"
      },
      "source": [
        "# Test (Future) data (SSP 2-4.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487480Z",
          "start_time": "2024-07-17T13:21:32.487473Z"
        },
        "scrolled": true,
        "id": "6OldzirHq-l8"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list245)\n",
        "errors3_ = comp_error(temp_img_array_list245.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_test = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487993Z",
          "start_time": "2024-07-17T13:21:32.487986Z"
        },
        "id": "_CUIcwtkq-l8"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_test, \"%s/output_245/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.488545Z",
          "start_time": "2024-07-17T13:21:32.488537Z"
        },
        "id": "aeXQV6iJq-l8"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_245 = pickle_load(\"%s/output_245/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489131Z",
          "start_time": "2024-07-17T13:21:32.489123Z"
        },
        "id": "Jeda3RNJq-l8"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "#thresh2 = np.quantile(errors_model.flatten(), 0.95)\n",
        "idxs_245 = np.where(np.array(errors_245) >= thresh2)\n",
        "idxs_245= np.array(idxs_245)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_245[0,:])))\n",
        "thresh2, np.shape(idxs_245)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489684Z",
          "start_time": "2024-07-17T13:21:32.489677Z"
        },
        "id": "M2t_1N20q-l8"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list245[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUMvgrASq-l8"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3 = temp_img_array_list245"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490244Z",
          "start_time": "2024-07-17T13:21:32.490237Z"
        },
        "id": "2D2vXi9Kq-l8"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh468QvEq-l8"
      },
      "outputs": [],
      "source": [
        "yyyymmdds3=[]\n",
        "for date3 in dates3[:]:\n",
        "    yyyymmdds3.append(int(\"%4.4i%2.2i%2.2i\" % (date3.year,date3.month,date3.day)))\n",
        "\n",
        "ds3 = pd.Series(yyyymmdds3)\n",
        "dates3 = pd.to_datetime(ds3, format='%Y%m%d')\n",
        "print (dates3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIC3989bq-l8"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490922Z",
          "start_time": "2024-07-17T13:21:32.490915Z"
        },
        "id": "OvkEzh4bq-l8"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P2axPU6q-l8"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.491497Z",
          "start_time": "2024-07-17T13:21:32.491490Z"
        },
        "id": "y4qtuNnZq-l8"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sl7fxVSq-l8"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492055Z",
          "start_time": "2024-07-17T13:21:32.492048Z"
        },
        "id": "X0aC0Owuq-l8"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_245[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOKsIfuq-l8"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492630Z",
          "start_time": "2024-07-17T13:21:32.492623Z"
        },
        "id": "h3yx7gQuq-l8"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0quZIapq-l8"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.493210Z",
          "start_time": "2024-07-17T13:21:32.493204Z"
        },
        "id": "Arar-hUcq-l8"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "VsvlJLFEq-l9"
      },
      "source": [
        "# Test (Future) data (SSP 3-7.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487480Z",
          "start_time": "2024-07-17T13:21:32.487473Z"
        },
        "scrolled": true,
        "id": "MP9ud4eTq-l9"
      },
      "outputs": [],
      "source": [
        "emax13_,tmax13_,jmax13_,imax13_,kmax13_ = np.shape(temp_img_array_list370)\n",
        "errors3_ = comp_error(temp_img_array_list370.reshape((emax13_*tmax13_,jmax13_,imax13_,kmax13_)), autoencoder2)\n",
        "errors_370 = errors3_.reshape((emax13_,tmax13_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.487993Z",
          "start_time": "2024-07-17T13:21:32.487986Z"
        },
        "id": "c9-kXuabq-l9"
      },
      "outputs": [],
      "source": [
        "# saving error to disk (model)\n",
        "print(\"[INFO] saving error data...\")\n",
        "pickle_dump(errors_370, \"%s/output_370/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.488545Z",
          "start_time": "2024-07-17T13:21:32.488537Z"
        },
        "id": "adpl3AqVq-l9"
      },
      "outputs": [],
      "source": [
        "# load error from disk (model)\n",
        "print(\"[INFO] loading error data...\")\n",
        "errors_370 = pickle_load(\"%s/output_370/error.pickle\" % (datatype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489131Z",
          "start_time": "2024-07-17T13:21:32.489123Z"
        },
        "id": "aYNigwQgq-l9"
      },
      "outputs": [],
      "source": [
        "# compute the q-th quantile of the errors which serves as our\n",
        "# threshold to identify anomalies -- any data point that our model\n",
        "# reconstructed with > threshold error will be marked as an outlier\n",
        "#thresh2 = np.quantile(errors_model.flatten(), 0.95)\n",
        "idxs_370= np.where(np.array(errors_370) >= thresh2)\n",
        "idxs_370 = np.array(idxs_370)\n",
        "print(\"[INFO] mse threshold: {}\".format(thresh2))\n",
        "print(\"[INFO] {} outliers found\".format(len(idxs_370[0,:])))\n",
        "thresh2, np.shape(idxs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.489684Z",
          "start_time": "2024-07-17T13:21:32.489677Z"
        },
        "id": "Q2-YLgV7q-l9"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list3_ = temp_img_array_list3[idxs_test[0,:],idxs_test[1,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE5_r_d6q-l9"
      },
      "outputs": [],
      "source": [
        "temp_img_array_list370 = temp_img_array_list370"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490244Z",
          "start_time": "2024-07-17T13:21:32.490237Z"
        },
        "id": "VHtfswxeq-l9"
      },
      "outputs": [],
      "source": [
        "pmax,kmax = np.shape(idxs_test)\n",
        "print (pmax,kmax)\n",
        "dates5 = []\n",
        "for kk in range(kmax):\n",
        "    date = dates3[idxs_test[1,kk]]\n",
        "    #print (date)\n",
        "    dates5.append(date)\n",
        "dates5 = pd.Series(dates5)\n",
        "print (dates5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyEvJkI3q-l9"
      },
      "outputs": [],
      "source": [
        "yyyymmdds3=[]\n",
        "for date3 in dates3[:]:\n",
        "    yyyymmdds3.append(int(\"%4.4i%2.2i%2.2i\" % (date3.year,date3.month,date3.day)))\n",
        "\n",
        "ds3 = pd.Series(yyyymmdds3)\n",
        "dates3 = pd.to_datetime(ds3, format='%Y%m%d')\n",
        "print (dates3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "jRlzBwg4q-l9"
      },
      "source": [
        "### Check autoencoder for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.490922Z",
          "start_time": "2024-07-17T13:21:32.490915Z"
        },
        "id": "FwPpuG7_q-l9"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "idx2a = np.random.randint(0,emax,5)\n",
        "idx2b = np.random.randint(0,len(temp_img_array_list3[0,:]), 3)\n",
        "\n",
        "#for i in idxs:\n",
        "for ee,ii in zip(idx2a,idx2b):\n",
        "\n",
        "\n",
        "    print (dates3[i])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/restored_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "EK8yqd-Cq-l9"
      },
      "source": [
        "### Plot anomalous days for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.491497Z",
          "start_time": "2024-07-17T13:21:32.491490Z"
        },
        "id": "CzNh2Kpnq-l9"
      },
      "outputs": [],
      "source": [
        "ccrs.PlateCarree()\n",
        "\n",
        "print (np.shape(idxs_test))\n",
        "\n",
        "idxs_test2 = np.random.randint(0,len(idxs_test[0,:]), 3)\n",
        "central_lon = np.mean(ddomain[0:2])\n",
        "central_lat = np.mean(ddomain[2:4])\n",
        "\n",
        "\n",
        "ee=1 # specific ensemble member\n",
        "\n",
        "for i in idxs_test2:\n",
        "\n",
        "    ee = idxs_test[0,i]\n",
        "    ii = idxs_test[1,i]\n",
        "\n",
        "    print (dates3[ii])\n",
        "    date = dates3[ii]\n",
        "\n",
        "    #fig = plt.figure(figsize=figsize) # set figure environemnt\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,\n",
        "                             subplot_kw={'projection': ccrs.Miller(central_longitude=central_lon)},\n",
        "    #                         subplot_kw={'projection': ccrs.AlbersEqualArea(central_lon, central_lat)},\n",
        "                             figsize=figsize)\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    # grab the original image and reconstructed image\n",
        "    original = temp_img_array_list3[ee,ii]*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "    image2 = np.expand_dims(temp_img_array_list3[ee,ii], axis=0)\n",
        "    decoded = autoencoder2.predict(image2) # autoencoder\n",
        "\n",
        "    recon = pad_remover(decoded,domain,grid) # reconstructed\n",
        "    recon = recon*(maxvalue-minvalue)+minvalue\n",
        "    original2 = pad_remover(original,domain,grid)\n",
        "\n",
        "#    flon,flat = np.meshgrid(lons,lats) # meshgrid\n",
        "\n",
        "    axs[0].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[0].coastlines(resolution='50m')\n",
        "    axs[0].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[0].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[0].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[0].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[0].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[0].yaxis.set_major_formatter(latfmt)\n",
        "    axs[0].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[0].contourf(lons[:],lats[:],original2[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[0].set_title(\"Original %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "    axs[1].set_extent(ddomain,crs=ccrs.PlateCarree())\n",
        "    axs[1].coastlines(resolution='50m')\n",
        "    axs[1].add_feature(cfeature.STATES, edgecolor='black', zorder=10)\n",
        "    axs[1].add_feature(cfeature.BORDERS, edgecolor='black', zorder=10)\n",
        "    axs[1].set_xticks(xticks0,crs=ccrs.PlateCarree())\n",
        "    axs[1].set_yticks(yticks0,crs=ccrs.PlateCarree())\n",
        "    latfmt=LatitudeFormatter()\n",
        "    lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
        "    axs[1].xaxis.set_major_formatter(lonfmt)\n",
        "    axs[1].yaxis.set_major_formatter(latfmt)\n",
        "    axs[1].set_extent(ddomain, crs=ccrs.PlateCarree())\n",
        "    cs = axs[1].contourf(lons[:],lats[:],recon[:,:], contours,transform=ccrs.PlateCarree(), cmap=cmap, extend=\"both\")\n",
        "    axs[1].set_title(\"Restored %4.4i-%2.2i-%2.2i\" % (date.year,date.month,date.day), fontsize=18)\n",
        "\n",
        "   # add legend\n",
        "    cax = fig.add_axes(legend_posi)\n",
        "    art = plt.colorbar(cs, cax, orientation='horizontal')\n",
        "    art.set_label('%s [%s]' % (elem,vunits), fontsize=20)\n",
        "    art.ax.tick_params(labelsize=18)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"%s/anomaly_test/fig_%4.4i_%2.2i_%2.2i_ee%2.2i.png\" % (datatype,dates3[ii].year,dates3[ii].month,dates3[ii].day,ee+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0q2O0Ftq-l9"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492055Z",
          "start_time": "2024-07-17T13:21:32.492048Z"
        },
        "id": "2nAcckXeq-l-"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_370[0,:]==ee)\n",
        "    idxs_test4 = idxs_370[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADHdUQnUq-l-"
      },
      "source": [
        "### Monthly Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.492630Z",
          "start_time": "2024-07-17T13:21:32.492623Z"
        },
        "id": "cPQEKYqUq-l-"
      },
      "outputs": [],
      "source": [
        "# Model Monthly Plot\n",
        "\n",
        "plt.figure(0, figsize=(8,8))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "temps=[]\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    temp = adates2.dt.month.value_counts().reindex(dates3.dt.month.unique(),fill_value=0).sort_index()\n",
        "\n",
        "    months = temp.index\n",
        "\n",
        "    temps.append(temp)\n",
        "\n",
        "temps = np.array(temps)\n",
        "years_ = sorted(dates3.dt.year.unique())\n",
        "months_ = sorted(dates3.dt.month.unique())\n",
        "tmax = len(years_)\n",
        "\n",
        "\n",
        "plt.fill_between(months[:], temps[:,:].min(axis=0)/tmax, temps[:,:].max(axis=0)/tmax, color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], temps[:,:].mean(axis=0)/tmax, \"o-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "plt.title(\"Abnormal Days for Each Month\")\n",
        "\n",
        "# Mann and Kendall Significant Ttest\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "data_ = np.ma.zeros((emax2,len(years_),len(months_)))\n",
        "trend_ = np.ma.zeros((emax2,len(months_)))\n",
        "pval_ = np.ma.zeros((emax2,len(months_)))\n",
        "\n",
        "trend_ensmean_ = np.ma.zeros(len(months_))\n",
        "pval_ensmean_ = np.ma.zeros(len(months_))\n",
        "\n",
        "for ee in range(emax2):\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    counts_ = adates2.groupby([adates2.dt.year, adates2.dt.month]).agg({'count'})\n",
        "    dates_ = counts_.index\n",
        "    vals_ = counts_.values.squeeze()\n",
        "\n",
        "    for date_, count_ in zip(dates_,vals_):\n",
        "        year_ = date_[0]\n",
        "        month_ = date_[1]\n",
        "        tt = int(year_-years_[0])\n",
        "        im = int(month_-months_[0])\n",
        "        data_[ee,tt,im]=count_\n",
        "\n",
        "    for im in range(len(months_)):\n",
        "        data2_ = data_[ee,:,im]\n",
        "        xx_, yy_, trend_[ee,im] = linreg(np.array(years_),np.array(data2_))\n",
        "        h, pval_[ee,im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#ens mean\n",
        "for im in range(len(months_)):\n",
        "    data2_ = data_[:,:,im].mean(axis=0)\n",
        "    xx_, yy_, trend_ensmean_[im] = linreg(np.array(years_),np.array(data2_))\n",
        "    h, pval_ensmean_[im] = mk_test(data2_, alpha=0.05)\n",
        "\n",
        "#plot\n",
        "plt.fill_between(months[:], trend_[:,:].min(axis=0), trend_[:,:].max(axis=0), color=\"r\", alpha=0.4)\n",
        "plt.plot(months[:], trend_ensmean_[:], \"-\", color=\"r\", label=\"Ensemble Mean\")\n",
        "\n",
        "for im in range(len(months_)):\n",
        "    if pval_ensmean_[im]<=0.05:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"r\", ms=12)\n",
        "    else:\n",
        "        plt.plot(months[:], trend_ensmean_[:], \"o\", color=\"k\", ms=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/month_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhLbmBzq-l-"
      },
      "source": [
        "### Amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-17T13:21:32.493210Z",
          "start_time": "2024-07-17T13:21:32.493204Z"
        },
        "id": "skrpnNhtq-l-"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "undef = -9.99E33\n",
        "\n",
        "years_ = dates3.dt.year.unique()\n",
        "tmax_ = len(years_)\n",
        "data_ = np.ma.zeros((emax2,tmax_))\n",
        "data_[:,:] = undef\n",
        "\n",
        "alldata2=[]\n",
        "for ee in range(emax2):\n",
        "\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    for tt,year in enumerate(years_):\n",
        "        idx_ = adates2[adates2.dt.year==year].index\n",
        "        if len(idx_)>0:\n",
        "            data_[ee,tt] = (temp_img_array_list3[ee,idx_].sum()/len(idx_))*(maxvalue-minvalue)+minvalue\n",
        "\n",
        "data_ = np.ma.masked_where(data_==undef, data_)\n",
        "\n",
        "ensmean = data_.mean(axis=0)\n",
        "ensmax = data_.max(axis=0)\n",
        "ensmin = data_.min(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    plt.fill_between(years_[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "    plt.fill_between(years_, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years_, ensmean, \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean[~ensmean.mask], alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years_[~ensmean.mask],ensmean[~ensmean.mask])\n",
        "    plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    plt.title(\"Amplitude for each event (Trend=%.5f)\" % (aa))\n",
        "else:\n",
        "    plt.title(\"Amplitude for each event\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test_amp.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9De1BtWpq-l_"
      },
      "source": [
        "# SSP Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKqsMFD_q-mA"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(8,4))\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "alldata2=[]\n",
        "alldata_nat=[]\n",
        "alldata_119 =[]\n",
        "alldata_245 = []\n",
        "alldata_370=[]\n",
        "alldata_534=[]\n",
        "for ee in range(emax2):\n",
        "    # SSP 5-8.5---------------------------------\n",
        "    idxs_test3 = np.where(idxs_test[0,:]==ee)\n",
        "    idxs_test4 = idxs_test[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata2.append(temp)\n",
        "    # NATURAL-------------------------------------\n",
        "    idxs_test3 = np.where(idxs_nat[0,:]==ee)\n",
        "    idxs_test4 = idxs_nat[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata_nat.append(temp)\n",
        "\n",
        "    # SSP 119 ----------------------------------------\n",
        "    idxs_test3 = np.where(idxs_119[0,:]==ee)\n",
        "    idxs_test4 = idxs_119[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata_119.append(temp)\n",
        "\n",
        "    # SSP 245 --------------------------------------\n",
        "    idxs_test3 = np.where(idxs_245[0,:]==ee)\n",
        "    idxs_test4 = idxs_245[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata_245.append(temp)\n",
        "\n",
        "    # SSP 370 --------------------------------------\n",
        "    idxs_test3 = np.where(idxs_370[0,:]==ee)\n",
        "    idxs_test4 = idxs_370[1,idxs_test3]\n",
        "    adates2 = dates3[idxs_test4[0,:]]\n",
        "\n",
        "    # derive (years,frequency of anomaly)\n",
        "    temp = adates2.dt.year.value_counts().reindex(dates3.dt.year.unique(),fill_value=0).sort_index()\n",
        "    years = temp.index\n",
        "\n",
        "    temp = temp.to_numpy()\n",
        "    alldata_370.append(temp)\n",
        "\n",
        " # Plot\n",
        "alldata2 = np.array(alldata2)\n",
        "\n",
        "ensmean = alldata2.mean(axis=0)\n",
        "ensmax = alldata2.max(axis=0)\n",
        "ensmin = alldata2.min(axis=0)\n",
        "\n",
        "alldata_nat = np.array(alldata_nat)\n",
        "ensmean_nat = alldata_nat.mean(axis=0)\n",
        "\n",
        "alldata_119 = np.array(alldata_119)\n",
        "ensmean_119 = alldata_119.mean(axis=0)\n",
        "\n",
        "alldata_245 = np.array(alldata_245)\n",
        "ensmean_245 = alldata_245.mean(axis=0)\n",
        "\n",
        "alldata_370 = np.array(alldata_370)\n",
        "ensmean_370 = alldata_370.mean(axis=0)\n",
        "\n",
        "if season==\"DJF\":\n",
        "    #plt.fill_between(years[:-1], ensmin[:-1], ensmax[:-1], color=\"red\", alpha=0.4)\n",
        "    plt.plot(years[:-1], ensmean[:-1], \"o-\", color=\"red\", label=\"Ensmean\")\n",
        "else:\n",
        "   # plt.fill_between(years, ensmin, ensmax, color=\"red\", alpha=0.4)\n",
        "    plt.plot(years, ensmean, \"o-\", color=\"purple\", label=\"SSP5-8.5\")\n",
        "    plt.plot(years, ensmean_nat, \"o-\", color=\"blue\", label=\"NatForc\")\n",
        "    plt.plot(years, ensmean_119, \"o-\", color=\"yellow\", label=\"SSP119\")\n",
        "    plt.plot(years, ensmean_245, \"o-\", color=\"green\", label=\"SSP245\")\n",
        "    plt.plot(years, ensmean_370, \"o-\", color=\"red\", label=\"SSP370\")\n",
        "\n",
        "df_ = pd.DataFrame(data=np.swapaxes(alldata2,0,1), index=years, columns=np.arange(1,emax2+1,1))\n",
        "df_.index.name=\"year\"\n",
        "df_.to_json(\"%s/timeseries_test.json\" % (datatype))\n",
        "\n",
        "plt.legend()\n",
        "# Mann and Kendall singnificant test for trend\n",
        "h, pval = mk_test(ensmean, alpha=0.05)\n",
        "\n",
        "if pval <=0.05:\n",
        "    xx, yy, aa = linreg(years,ensmean)\n",
        "    #plt.plot(xx, yy, \"--\", color=\"black\")\n",
        "    #plt.title(\"Abnormal Days (Trend=%.5f)\" % (aa))\n",
        "plt.title(\"Abnormal Days\")\n",
        "\n",
        "#plt.ylim(0,2)\n",
        "print (ensmean)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"%s/timeseries_test.png\" % (datatype))\n",
        "print (datatype)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5N8JqUjq-mA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.yticks([])\n",
        "plt.xticks([])\n",
        "\n",
        "SMALL_SIZE = 20\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 10\n",
        "\n",
        "labels=[\"Observations(1971-2023)\",\"AllForc(1971-2023)\",\"AllForc(1971-2100)\",\"NatForc(1971-2100)\"]\n",
        "\n",
        "plt.rc('font', size=15)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=15)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=10)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
        " # legend fontsize\n",
        "plt.title('Linear Trend in Anomalous Days')\n",
        "data= [obs,trends_model,trends_future,trends_nat]\n",
        "ax = fig.add_subplot()\n",
        "bp = ax.boxplot(data,vert=False,labels=labels,patch_artist=True, medianprops=dict(color=\"black\", alpha=0.7))\n",
        "ax.axvline(x = 0, color = 'black',linestyle='dashed')\n",
        "ax.set_xlabel('Days per year')\n",
        "\n",
        "\n",
        "colors = [\"mistyrose\",\"mistyrose\",\"mistyrose\",\"lightgray\"]\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "for i in range(1,4,1):\n",
        "    ax.plot(data[i],np.full((30),i+1),'o',markersize=3,color='black')\n",
        "for i in range(4):\n",
        "    ax.plot(np.average(data[i]),np.average(bp['medians'][i].get_ydata()),color='red', marker='s')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5gd8j11q-mA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.yticks([])\n",
        "plt.xticks([])\n",
        "\n",
        "SMALL_SIZE = 20\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 10\n",
        "\n",
        "labels=[\"Observations(1971-2023)\",\"AllForc(1971-2023)\",\"AllForc(1971-2100)\",\"NatForc(1971-2100)\"]\n",
        "\n",
        "plt.rc('font', size=15)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=15)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=10)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
        " # legend fontsize\n",
        "plt.title('Linear Trend in Anomalous Days')\n",
        "data= [obs,trends_model,trends_future,trends_nat]\n",
        "ax = fig.add_subplot()\n",
        "bp = ax.boxplot(data,vert=False,labels=labels,patch_artist=True, medianprops=dict(color=\"black\", alpha=0.7))\n",
        "ax.axvline(x = 0, color = 'black',linestyle='dashed')\n",
        "ax.set_xlabel('Days per year')\n",
        "\n",
        "\n",
        "colors = [\"mistyrose\",\"mistyrose\",\"mistyrose\",\"lightgray\"]\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "for i in range(1,4,1):\n",
        "    ax.plot(data[i],np.full((30),i+1),'o',markersize=3,color='black')\n",
        "for i in range(4):\n",
        "    ax.plot(np.average(data[i]),np.average(bp['medians'][i].get_ydata()),color='red', marker='s')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryfZtpN4q-mA"
      },
      "outputs": [],
      "source": [
        "# Generate some data for this demonstration.\n",
        "\n",
        "\n",
        "# Fit a normal distribution to the data:\n",
        "data= trends_model\n",
        "mu, std = norm.fit(data)\n",
        "\n",
        "# Plot the histogram.\n",
        "plt.hist(data, bins=10, density=True, alpha=0.3, color='red')\n",
        "\n",
        "# Plot the PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mu, std)\n",
        "plt.plot(x, p, 'k', linewidth=2,color='r',label = \"AllForc\")\n",
        "\n",
        "data= trends_nat\n",
        "mu, std = norm.fit(data)\n",
        "\n",
        "# Plot the histogram.\n",
        "plt.hist(data, bins=10, density=True, alpha=0.3, color='blue')\n",
        "\n",
        "# Plot the PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mu, std)\n",
        "plt.plot(x, p, 'k', linewidth=2,color='b',label = \"NatForc\")\n",
        "\n",
        "\n",
        "plt.axvline(x = obs, color = 'black', label = 'Observed')\n",
        "plt.title(\"Linear Trend in Anomalous Days\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kUNaHVmu6HDY",
        "QYJ9En9Q6HDZ",
        "fB-TzNda6HDa"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "313.188px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}